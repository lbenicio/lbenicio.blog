<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Mlops on Leonardo Benicio</title><link>https://lbenicio.dev/tags/mlops/</link><description>Recent content in Mlops on Leonardo Benicio</description><generator>Hugo</generator><language>en-us</language><lastBuildDate>Sat, 05 Apr 2025 13:25:00 +0000</lastBuildDate><atom:link href="https://lbenicio.dev/tags/mlops/index.xml" rel="self" type="application/rss+xml"/><item><title>Auditing the Algorithm: Building a Responsible AI Pipeline That Scales</title><link>https://lbenicio.dev/blog/auditing-the-algorithm-building-a-responsible-ai-pipeline-that-scales/</link><pubDate>Sat, 05 Apr 2025 13:25:00 +0000</pubDate><guid>https://lbenicio.dev/blog/auditing-the-algorithm-building-a-responsible-ai-pipeline-that-scales/</guid><description>&lt;p&gt;When regulators asked for evidence that our AI systems behave responsibly, we realized spreadsheets and ad hoc reviews wouldn&amp;rsquo;t suffice. Our models influenced credit decisions, hiring suggestions, and medical triage. Stakeholders demanded more than accuracyâ€”they wanted fairness, explainability, and accountability. We responded by building a responsible AI audit pipeline woven into development, deployment, and operations. This article shares how we did it: the processes, tooling, and cultural shifts that turned compliance into continuous curiosity.&lt;/p&gt;</description></item><item><title>Seeing in the Dark: Observability for Edge AI Fleets</title><link>https://lbenicio.dev/blog/seeing-in-the-dark-observability-for-edge-ai-fleets/</link><pubDate>Fri, 16 Aug 2024 10:55:00 +0000</pubDate><guid>https://lbenicio.dev/blog/seeing-in-the-dark-observability-for-edge-ai-fleets/</guid><description>&lt;p&gt;Our edge AI deployment began with a handful of pilot devices in retail stores. Within months, thousands of cameras, sensors, and point-of-sale terminals joined the fleet. They detected shelves running low, predicted queue lengths, and flagged suspicious transactions. But when a customer called asking why a device misclassified bananas as tennis balls, we realized our observability blurred at the edge. Logs vanished into the ether, metrics arrived sporadically, and models drifted silently. This article shares how we built observability robust enough for flaky networks, sensitive data, and autonomous updates.&lt;/p&gt;</description></item><item><title>Keeping the Model Awake: Building a Self-Healing ML Inference Platform</title><link>https://lbenicio.dev/blog/keeping-the-model-awake-building-a-self-healing-ml-inference-platform/</link><pubDate>Tue, 14 Feb 2023 07:20:00 +0000</pubDate><guid>https://lbenicio.dev/blog/keeping-the-model-awake-building-a-self-healing-ml-inference-platform/</guid><description>&lt;p&gt;During a winter holiday freeze, our recommendation API refused to scale. GPUs idled waiting for models to load, autoscalers fought each other, and on-call engineers reheated leftovers at 3 a.m. while spike traffic slammed into origin. We promised leadership that this would never happen again. The solution wasn&amp;rsquo;t magic; it was a self-healing inference platform blending old-school reliability, modern ML tooling, and relentless experimentation.&lt;/p&gt;
&lt;p&gt;This post documents the rebuild. We&amp;rsquo;ll explore model packaging, warm-up rituals, adaptive scheduling, observability, chaos drills, and the social contract between ML researchers and production engineers. The goal: keep models awake, snappy, and trustworthy even when the world throws curveballs.&lt;/p&gt;</description></item></channel></rss>