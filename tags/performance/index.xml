<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Performance on Leonardo Benicio</title><link>https://lbenicio.dev/tags/performance/</link><description>Recent content in Performance on Leonardo Benicio</description><generator>Hugo</generator><language>en-us</language><lastBuildDate>Sat, 04 Oct 2025 10:00:00 +0000</lastBuildDate><atom:link href="https://lbenicio.dev/tags/performance/index.xml" rel="self" type="application/rss+xml"/><item><title>Learned Indexes: When Models Replace B‑Trees</title><link>https://lbenicio.dev/blog/learned-indexes-when-models-replace-btrees/</link><pubDate>Sat, 04 Oct 2025 10:00:00 +0000</pubDate><guid>https://lbenicio.dev/blog/learned-indexes-when-models-replace-btrees/</guid><description>&lt;p&gt;If you’ve spent a career trusting B‑trees and hash tables, the idea of using a machine‑learned model as an index can feel like swapping a torque wrench for a Ouija board. But learned indexes aren’t a gimmick. They exploit a simple observation: real data isn’t uniformly random. It has shape—monotonic keys, skewed distributions, natural clusters—and a model can learn that shape to predict where a key lives in a sorted array. The payoff is smaller indexes, fewer cache misses, and—sometimes—dramatically faster lookups.&lt;/p&gt;</description></item><item><title>The 100‑Microsecond Rule: Why Tail Latency Eats Your Throughput (and How to Fight Back)</title><link>https://lbenicio.dev/blog/the-100microsecond-rule-why-tail-latency-eats-your-throughput-and-how-to-fight-back/</link><pubDate>Sat, 04 Oct 2025 10:00:00 +0000</pubDate><guid>https://lbenicio.dev/blog/the-100microsecond-rule-why-tail-latency-eats-your-throughput-and-how-to-fight-back/</guid><description>&lt;p&gt;If you stare at a performance dashboard long enough, you’ll eventually see a ghost—an outlier that refuses to go away. It’s the P99 spike that surfaces at the worst time; the one you “fix” three times and then rediscover during a product launch or a perfectly normal Tuesday.&lt;/p&gt;
&lt;p&gt;Here’s the hard truth: tail latency doesn’t just ruin your service levels; it compounds into lost throughput and broken guarantees. In systems with fan‑out, retries, and microservices, the slowest 1% isn’t “rare”—it’s the norm you ship to users most of the time. This is the 100‑microsecond rule in practice: small latencies multiply brutally at scale, and the invisible cost often starts below a single millisecond.&lt;/p&gt;</description></item><item><title>Tuning CUDA with the GPU Memory Hierarchy</title><link>https://lbenicio.dev/blog/tuning-cuda-with-the-gpu-memory-hierarchy/</link><pubDate>Wed, 27 Nov 2024 00:00:00 +0000</pubDate><guid>https://lbenicio.dev/blog/tuning-cuda-with-the-gpu-memory-hierarchy/</guid><description>&lt;p&gt;CUDA performance hinges on moving data efficiently through a &lt;em&gt;hierarchy&lt;/em&gt; of memories that differ by latency, bandwidth, scope (visibility), and capacity. Raw FLOP throughput is rarely the first limiter—memory behavior, access ordering, and reuse patterns almost always dominate performance envelopes.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id="1-the-hierarchy-at-a-glance"&gt;1. The Hierarchy at a Glance&lt;/h2&gt;
&lt;table&gt;
 &lt;thead&gt;
 &lt;tr&gt;
 &lt;th&gt;Level&lt;/th&gt;
 &lt;th&gt;Scope / Visibility&lt;/th&gt;
 &lt;th&gt;Approx Latency (cycles)*&lt;/th&gt;
 &lt;th&gt;Bandwidth&lt;/th&gt;
 &lt;th&gt;Capacity (per SM / device)&lt;/th&gt;
 &lt;th&gt;Notes&lt;/th&gt;
 &lt;/tr&gt;
 &lt;/thead&gt;
 &lt;tbody&gt;
 &lt;tr&gt;
 &lt;td&gt;Registers&lt;/td&gt;
 &lt;td&gt;Thread private&lt;/td&gt;
 &lt;td&gt;~1&lt;/td&gt;
 &lt;td&gt;Extreme&lt;/td&gt;
 &lt;td&gt;Tens of k per SM (allocated per thread)&lt;/td&gt;
 &lt;td&gt;Allocation affects occupancy; spilling -&amp;gt; local memory&lt;/td&gt;
 &lt;/tr&gt;
 &lt;tr&gt;
 &lt;td&gt;Shared Memory (SMEM)&lt;/td&gt;
 &lt;td&gt;Block (CTA)&lt;/td&gt;
 &lt;td&gt;~20–35&lt;/td&gt;
 &lt;td&gt;Very high&lt;/td&gt;
 &lt;td&gt;48–228 KB configurable (arch dependent)&lt;/td&gt;
 &lt;td&gt;Banked; subject to conflicts; optional split w/ L1&lt;/td&gt;
 &lt;/tr&gt;
 &lt;tr&gt;
 &lt;td&gt;L1 / Texture Cache&lt;/td&gt;
 &lt;td&gt;SM&lt;/td&gt;
 &lt;td&gt;~30–60&lt;/td&gt;
 &lt;td&gt;High&lt;/td&gt;
 &lt;td&gt;~128–256 KB (unified)&lt;/td&gt;
 &lt;td&gt;Serves global loads; spatial locality &amp;amp; coalescing still matter&lt;/td&gt;
 &lt;/tr&gt;
 &lt;tr&gt;
 &lt;td&gt;L2 Cache&lt;/td&gt;
 &lt;td&gt;Device-wide&lt;/td&gt;
 &lt;td&gt;~200–300&lt;/td&gt;
 &lt;td&gt;High&lt;/td&gt;
 &lt;td&gt;Multi-MB&lt;/td&gt;
 &lt;td&gt;Coherent across SMs; crucial for global data reuse&lt;/td&gt;
 &lt;/tr&gt;
 &lt;tr&gt;
 &lt;td&gt;Global DRAM&lt;/td&gt;
 &lt;td&gt;Device-wide&lt;/td&gt;
 &lt;td&gt;~400–800&lt;/td&gt;
 &lt;td&gt;High (GB/s)&lt;/td&gt;
 &lt;td&gt;Many GB&lt;/td&gt;
 &lt;td&gt;Long latency—hide with parallelism &amp;amp; coalescing&lt;/td&gt;
 &lt;/tr&gt;
 &lt;tr&gt;
 &lt;td&gt;Constant Cache&lt;/td&gt;
 &lt;td&gt;Device-wide (read-only)&lt;/td&gt;
 &lt;td&gt;~ L1 hit if cached&lt;/td&gt;
 &lt;td&gt;High (broadcast)&lt;/td&gt;
 &lt;td&gt;64 KB&lt;/td&gt;
 &lt;td&gt;Broadcast to warp if all threads read same address&lt;/td&gt;
 &lt;/tr&gt;
 &lt;tr&gt;
 &lt;td&gt;Texture / Read-Only Cache&lt;/td&gt;
 &lt;td&gt;Device-wide (cached)&lt;/td&gt;
 &lt;td&gt;Similar to L1&lt;/td&gt;
 &lt;td&gt;High&lt;/td&gt;
 &lt;td&gt;N/A&lt;/td&gt;
 &lt;td&gt;Provides specialized spatial filtering &amp;amp; relaxed coalescing&lt;/td&gt;
 &lt;/tr&gt;
 &lt;tr&gt;
 &lt;td&gt;Local Memory&lt;/td&gt;
 &lt;td&gt;Thread (spill/backing)&lt;/td&gt;
 &lt;td&gt;DRAM latency&lt;/td&gt;
 &lt;td&gt;DRAM&lt;/td&gt;
 &lt;td&gt;Per-thread virtual&lt;/td&gt;
 &lt;td&gt;“Local” is misnomer if spilled—same as global latency&lt;/td&gt;
 &lt;/tr&gt;
 &lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;&lt;em&gt;Indicative ranges; varies by architecture generation (e.g., Turing, Ampere, Hopper). Absolute numbers less important than ratio gaps.&lt;/em&gt;&lt;/p&gt;</description></item><item><title>Teaching GraphQL to Cache at the Edge</title><link>https://lbenicio.dev/blog/teaching-graphql-to-cache-at-the-edge/</link><pubDate>Sat, 03 Sep 2022 12:15:00 +0000</pubDate><guid>https://lbenicio.dev/blog/teaching-graphql-to-cache-at-the-edge/</guid><description>&lt;p&gt;GraphQL promises tailor-made responses, but tailor-made payloads resist caching. For years, we treated GraphQL responses as ephemeral: generated on demand, personalized, too unique to reuse. Then mobile latency complaints reached a boiling point. Edge locations sat underutilized while origin clusters sweated. We set out to teach GraphQL how to cache—respecting declarative queries, personalization boundaries, and real-time freshness. This is the story of building an edge caching layer that felt invisible to developers yet shaved hundreds of milliseconds off user interactions.&lt;/p&gt;</description></item><item><title>Cache‑Friendly Data Layouts: AoS vs. SoA (and the Hybrid In‑Between)</title><link>https://lbenicio.dev/blog/cachefriendly-data-layouts-aos-vs.-soa-and-the-hybrid-inbetween/</link><pubDate>Thu, 18 Mar 2021 10:00:00 +0000</pubDate><guid>https://lbenicio.dev/blog/cachefriendly-data-layouts-aos-vs.-soa-and-the-hybrid-inbetween/</guid><description>&lt;p&gt;The fastest code you’ll ever write usually isn’t a brand‑new algorithm—it’s the same algorithm organized in memory so the CPU (or GPU) can consume it efficiently. That journey often starts with a deceptively small choice: Array‑of‑Structs (AoS) or Struct‑of‑Arrays (SoA). The layout you choose determines which cachelines move, which prefetchers trigger, how branch predictors behave, and whether SIMD units stay busy or starve.&lt;/p&gt;
&lt;p&gt;In this post we’ll build an intuitive mental model for cachelines, TLBs, prefetchers, and vector units; examine AoS and SoA under realistic access patterns; then derive a hybrid (AoSoA) that quietly powers many high‑performance systems—from physics engines to databases to ML dataloaders. We’ll also walk through migration strategies, benchmarking pitfalls, and checklists you can apply this week.&lt;/p&gt;</description></item><item><title>Speculative Prefetchers: Designing Memory Systems That Read the Future</title><link>https://lbenicio.dev/blog/speculative-prefetchers-designing-memory-systems-that-read-the-future/</link><pubDate>Thu, 14 Feb 2019 10:00:00 +0000</pubDate><guid>https://lbenicio.dev/blog/speculative-prefetchers-designing-memory-systems-that-read-the-future/</guid><description>&lt;p&gt;At 2:17 a.m., the on-call performance engineer watches another alert crawl across the dashboard. The new machine image promised higher throughput for a latency-sensitive analytics service, yet caches still thrash whenever end-of-day reconciliation jobs arrive. Each job walks a sparsely linked graph of customer transactions, and the CPU spends more time waiting on memory than executing instructions. &amp;ldquo;If only the hardware could guess where the program was going next,&amp;rdquo; she sighs. That daydream is the seed of speculative prefetching—the art of reading tomorrow’s memory today.&lt;/p&gt;</description></item><item><title>Computer Architecture: A Quantitative Approach (6th ed.)</title><link>https://lbenicio.dev/reading/computer-architecture-a-quantitative-approach-6th-ed./</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://lbenicio.dev/reading/computer-architecture-a-quantitative-approach-6th-ed./</guid><description>&lt;p&gt;Definitive textbook on modern computer architecture and performance analysis.&lt;/p&gt;</description></item><item><title>Computer Architecture: A Quantitative Approach (6th ed.)</title><link>https://lbenicio.dev/reading/computer-architecture-a-quantitative-approach-6th-ed./</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://lbenicio.dev/reading/computer-architecture-a-quantitative-approach-6th-ed./</guid><description>&lt;p&gt;An in-depth treatment of modern processor and system design with quantitative evaluation.&lt;/p&gt;</description></item><item><title>Computer Systems: A Programmer's Perspective (3rd ed.)</title><link>https://lbenicio.dev/reading/computer-systems-a-programmers-perspective-3rd-ed./</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://lbenicio.dev/reading/computer-systems-a-programmers-perspective-3rd-ed./</guid><description>&lt;p&gt;Bridges programming and computer architecture with a focus on performance and systems.&lt;/p&gt;</description></item><item><title>Computer Systems: A Programmer's Perspective (3rd ed.)</title><link>https://lbenicio.dev/reading/computer-systems-a-programmers-perspective-3rd-ed./</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://lbenicio.dev/reading/computer-systems-a-programmers-perspective-3rd-ed./</guid><description>&lt;p&gt;Bridges the gap between programming and computer architecture with a focus on performance and systems.&lt;/p&gt;</description></item><item><title>Improving the Scalability and Performance of a Rails Application: A Case Study with Consul</title><link>https://lbenicio.dev/publications/improving-the-scalability-and-performance-of-a-rails-application-a-case-study-with-consul/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://lbenicio.dev/publications/improving-the-scalability-and-performance-of-a-rails-application-a-case-study-with-consul/</guid><description>&lt;p&gt;Case study evaluating scalability and performance improvements of a Ruby on Rails application using Consul.&lt;/p&gt;</description></item></channel></rss>