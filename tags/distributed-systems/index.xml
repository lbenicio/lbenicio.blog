<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Distributed-Systems on Leonardo Benicio</title><link>https://lbenicio.dev/tags/distributed-systems/</link><description>Recent content in Distributed-Systems on Leonardo Benicio</description><generator>Hugo</generator><language>en-us</language><lastBuildDate>Sat, 04 Oct 2025 10:00:00 +0000</lastBuildDate><atom:link href="https://lbenicio.dev/tags/distributed-systems/index.xml" rel="self" type="application/rss+xml"/><item><title>The 100‑Microsecond Rule: Why Tail Latency Eats Your Throughput (and How to Fight Back)</title><link>https://lbenicio.dev/blog/the-100microsecond-rule-why-tail-latency-eats-your-throughput-and-how-to-fight-back/</link><pubDate>Sat, 04 Oct 2025 10:00:00 +0000</pubDate><guid>https://lbenicio.dev/blog/the-100microsecond-rule-why-tail-latency-eats-your-throughput-and-how-to-fight-back/</guid><description>&lt;p&gt;If you stare at a performance dashboard long enough, you’ll eventually see a ghost—an outlier that refuses to go away. It’s the P99 spike that surfaces at the worst time; the one you “fix” three times and then rediscover during a product launch or a perfectly normal Tuesday.&lt;/p&gt;
&lt;p&gt;Here’s the hard truth: tail latency doesn’t just ruin your service levels; it compounds into lost throughput and broken guarantees. In systems with fan‑out, retries, and microservices, the slowest 1% isn’t “rare”—it’s the norm you ship to users most of the time. This is the 100‑microsecond rule in practice: small latencies multiply brutally at scale, and the invisible cost often starts below a single millisecond.&lt;/p&gt;</description></item><item><title>The Quiet Calculus of Probabilistic Commutativity</title><link>https://lbenicio.dev/blog/the-quiet-calculus-of-probabilistic-commutativity/</link><pubDate>Sat, 27 Sep 2025 00:00:00 +0000</pubDate><guid>https://lbenicio.dev/blog/the-quiet-calculus-of-probabilistic-commutativity/</guid><description>&lt;h2 id="abstract"&gt;Abstract&lt;/h2&gt;
&lt;p&gt;Eventual consistency dominates many internet-scale systems, but reasoning about concurrency under minimal coordination remains ad hoc. This post introduces &amp;ldquo;probabilistic commutativity&amp;rdquo; — a lightweight calculus for reasoning about whether concurrent operations, under reasonable stochastic assumptions about ordering and visibility delays, are likely to commute in practice. Probabilistic commutativity offers an intermediate lens between strict algebraic commutativity and empirical test-driven guarantees, enabling low-overhead coordination strategies and probabilistic correctness arguments for producing practically consistent distributed services.&lt;/p&gt;</description></item><item><title>The Hidden Backbone of Parallelism: How Prefix Sums Power Distributed Computation</title><link>https://lbenicio.dev/blog/the-hidden-backbone-of-parallelism-how-prefix-sums-power-distributed-computation/</link><pubDate>Sun, 21 Sep 2025 00:00:00 +0000</pubDate><guid>https://lbenicio.dev/blog/the-hidden-backbone-of-parallelism-how-prefix-sums-power-distributed-computation/</guid><description>&lt;h2 id="introduction"&gt;Introduction&lt;/h2&gt;
&lt;p&gt;When most people think about parallel computing, they imagine splitting a massive task into smaller chunks and running them simultaneously. That’s the Hollywood version: thousands of processors blazing through data, crunching numbers in parallel, and finishing jobs in seconds.&lt;/p&gt;
&lt;p&gt;The reality is both more fascinating and more subtle. Beneath the surface of supercomputers and distributed systems lies a set of seemingly modest mathematical operations—building blocks that make large-scale parallelism possible. Among these, one stands out for its simplicity and profound impact: the &lt;strong&gt;parallel prefix sum&lt;/strong&gt;, also called a &lt;em&gt;scan&lt;/em&gt;.&lt;/p&gt;</description></item><item><title>Timeouts, Retries, and Idempotency Keys: A Practical Guide</title><link>https://lbenicio.dev/blog/timeouts-retries-and-idempotency-keys-a-practical-guide/</link><pubDate>Thu, 08 Sep 2022 10:00:00 +0000</pubDate><guid>https://lbenicio.dev/blog/timeouts-retries-and-idempotency-keys-a-practical-guide/</guid><description>&lt;p&gt;Every distributed call needs three decisions: how long to wait, how to retry, and how to avoid duplicate effects. Done right, these three give you calm dashboards during partial failures; done wrong, they turn blips into incidents. This is a practical guide you can wire into client libraries and services without heroics.&lt;/p&gt;
&lt;h2 id="budget-your-time-not-perhop-timeouts"&gt;Budget your time, not per‑hop timeouts&lt;/h2&gt;
&lt;p&gt;Propagate a request deadline. Each hop consumes a slice and passes the remainder. Treat it as a budget: if it’s nearly spent, fail fast. Per‑hop fixed timeouts accumulate and blow your SLOs because each layer waits its entire local timeout.&lt;/p&gt;</description></item><item><title>Raft Fast‑Commit and PreVote in Practice</title><link>https://lbenicio.dev/blog/raft-fastcommit-and-prevote-in-practice/</link><pubDate>Mon, 09 Nov 2020 10:00:00 +0000</pubDate><guid>https://lbenicio.dev/blog/raft-fastcommit-and-prevote-in-practice/</guid><description>&lt;p&gt;Raft’s original paper optimizes for understandability. Production clusters optimize for availability and mean‑time‑to‑recovery. Two common extensions—PreVote and fast‑commit—reduce needless disruptions and trim the time it takes to make progress. Let’s unpack them without hand‑waving.&lt;/p&gt;
&lt;h2 id="the-pain-disruptive-elections-and-long-commit-paths"&gt;The pain: disruptive elections and long commit paths&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Without PreVote, a partitioned follower may increment term and trigger elections on rejoin, ousting a healthy leader.&lt;/li&gt;
&lt;li&gt;Without fast‑commit, clients wait for the full round‑trip alignment even when a quorum already holds the entry durably.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="prevote"&gt;PreVote&lt;/h2&gt;
&lt;p&gt;PreVote adds a “dry‑run” phase: before incrementing term, a candidate asks peers if they’d vote. Peers reject if their logs are more up‑to‑date.&lt;/p&gt;</description></item><item><title>Tuning the Dial: Adaptive Consistency at Planet Scale</title><link>https://lbenicio.dev/blog/tuning-the-dial-adaptive-consistency-at-planet-scale/</link><pubDate>Wed, 11 Mar 2020 14:05:00 +0000</pubDate><guid>https://lbenicio.dev/blog/tuning-the-dial-adaptive-consistency-at-planet-scale/</guid><description>&lt;p&gt;At 2:17 a.m. UTC, a partner bank in Singapore called our incident bridge. A fund transfer appeared twice in their ledger. The culprit: a replication lag spike between Singapore and Frankfurt had stretched past our standard safety buffers. Historically we would have halted writes across the fleet, cutting availability to protect consistency. Instead, our adaptive consistency layer dialed a region-specific policy: Singapore moved from &amp;ldquo;read-after-write&amp;rdquo; to &amp;ldquo;read-your-writes&amp;rdquo; guarantees, while Frankfurt raised its commit quorum. The double posting self-corrected before social media noticed. No downtime, no irreversible loss—just a story about how we learned to treat consistency as a spectrum rather than a binary.&lt;/p&gt;</description></item><item><title>When Data Centers Learned to Sleep: Energy-Aware Scheduling in Practice</title><link>https://lbenicio.dev/blog/when-data-centers-learned-to-sleep-energy-aware-scheduling-in-practice/</link><pubDate>Fri, 19 Jul 2019 09:30:00 +0000</pubDate><guid>https://lbenicio.dev/blog/when-data-centers-learned-to-sleep-energy-aware-scheduling-in-practice/</guid><description>&lt;p&gt;The first time we let a data center &amp;ldquo;sleep&amp;rdquo; during daylight hours felt reckless. Customers trusted us with near-infinite elasticity. Flipping servers into deep power states to save energy sounded like penny-pinching, not engineering. Yet the math was undeniable: idling a hyperscale fleet burned as much electricity as a mid-size city. This post tells the story of how we evolved from skepticism to confidence, building energy-aware scheduling systems that keep promises while honoring planetary limits.&lt;/p&gt;</description></item></channel></rss>