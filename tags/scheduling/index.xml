<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Scheduling on Leonardo Benicio</title><link>https://lbenicio.dev/tags/scheduling/</link><description>Recent content in Scheduling on Leonardo Benicio</description><generator>Hugo</generator><language>en-us</language><lastBuildDate>Sat, 04 Oct 2025 10:00:00 +0000</lastBuildDate><atom:link href="https://lbenicio.dev/tags/scheduling/index.xml" rel="self" type="application/rss+xml"/><item><title>The 100‑Microsecond Rule: Why Tail Latency Eats Your Throughput (and How to Fight Back)</title><link>https://lbenicio.dev/blog/the-100microsecond-rule-why-tail-latency-eats-your-throughput-and-how-to-fight-back/</link><pubDate>Sat, 04 Oct 2025 10:00:00 +0000</pubDate><guid>https://lbenicio.dev/blog/the-100microsecond-rule-why-tail-latency-eats-your-throughput-and-how-to-fight-back/</guid><description>&lt;p&gt;If you stare at a performance dashboard long enough, you’ll eventually see a ghost—an outlier that refuses to go away. It’s the P99 spike that surfaces at the worst time; the one you “fix” three times and then rediscover during a product launch or a perfectly normal Tuesday.&lt;/p&gt;
&lt;p&gt;Here’s the hard truth: tail latency doesn’t just ruin your service levels; it compounds into lost throughput and broken guarantees. In systems with fan‑out, retries, and microservices, the slowest 1% isn’t “rare”—it’s the norm you ship to users most of the time. This is the 100‑microsecond rule in practice: small latencies multiply brutally at scale, and the invisible cost often starts below a single millisecond.&lt;/p&gt;</description></item><item><title>Scheduling: Trading Latency for Throughput (and Back Again)</title><link>https://lbenicio.dev/blog/scheduling-trading-latency-for-throughput-and-back-again/</link><pubDate>Wed, 12 Feb 2025 10:00:00 +0000</pubDate><guid>https://lbenicio.dev/blog/scheduling-trading-latency-for-throughput-and-back-again/</guid><description>&lt;p&gt;Schedulers encode policy: who runs next, on which core, and for how long. Those choices shuffle latency and throughput. Let’s make the trade‑offs explicit.&lt;/p&gt;
&lt;h2 id="fifo-vs-priority-vs-fair"&gt;FIFO vs. priority vs. fair&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;FIFO: simple, minimal overhead; tail prone under bursty arrivals.&lt;/li&gt;
&lt;li&gt;Priority: protects critical work; risks starvation without aging.&lt;/li&gt;
&lt;li&gt;Fair (CFQ‑like): shares CPU evenly; may underutilize when work is imbalanced.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="work-stealing"&gt;Work stealing&lt;/h2&gt;
&lt;p&gt;Great for irregular parallel workloads. Each worker has a deque; thieves steal from the tail. Pros: high utilization; Cons: cache locality losses and noisy tails under high contention.&lt;/p&gt;</description></item><item><title>When Data Centers Learned to Sleep: Energy-Aware Scheduling in Practice</title><link>https://lbenicio.dev/blog/when-data-centers-learned-to-sleep-energy-aware-scheduling-in-practice/</link><pubDate>Fri, 19 Jul 2019 09:30:00 +0000</pubDate><guid>https://lbenicio.dev/blog/when-data-centers-learned-to-sleep-energy-aware-scheduling-in-practice/</guid><description>&lt;p&gt;The first time we let a data center &amp;ldquo;sleep&amp;rdquo; during daylight hours felt reckless. Customers trusted us with near-infinite elasticity. Flipping servers into deep power states to save energy sounded like penny-pinching, not engineering. Yet the math was undeniable: idling a hyperscale fleet burned as much electricity as a mid-size city. This post tells the story of how we evolved from skepticism to confidence, building energy-aware scheduling systems that keep promises while honoring planetary limits.&lt;/p&gt;</description></item></channel></rss>