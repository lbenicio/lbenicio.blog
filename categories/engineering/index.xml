<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Engineering on Leonardo Benicio</title><link>https://lbenicio.dev/categories/engineering/</link><description>Recent content in Engineering on Leonardo Benicio</description><generator>Hugo</generator><language>en-us</language><lastBuildDate>Sat, 04 Oct 2025 10:00:00 +0000</lastBuildDate><atom:link href="https://lbenicio.dev/categories/engineering/index.xml" rel="self" type="application/rss+xml"/><item><title>Learned Indexes: When Models Replace B‑Trees</title><link>https://lbenicio.dev/blog/learned-indexes-when-models-replace-btrees/</link><pubDate>Sat, 04 Oct 2025 10:00:00 +0000</pubDate><guid>https://lbenicio.dev/blog/learned-indexes-when-models-replace-btrees/</guid><description>&lt;p&gt;If you’ve spent a career trusting B‑trees and hash tables, the idea of using a machine‑learned model as an index can feel like swapping a torque wrench for a Ouija board. But learned indexes aren’t a gimmick. They exploit a simple observation: real data isn’t uniformly random. It has shape—monotonic keys, skewed distributions, natural clusters—and a model can learn that shape to predict where a key lives in a sorted array. The payoff is smaller indexes, fewer cache misses, and—sometimes—dramatically faster lookups.&lt;/p&gt;</description></item><item><title>The 100‑Microsecond Rule: Why Tail Latency Eats Your Throughput (and How to Fight Back)</title><link>https://lbenicio.dev/blog/the-100microsecond-rule-why-tail-latency-eats-your-throughput-and-how-to-fight-back/</link><pubDate>Sat, 04 Oct 2025 10:00:00 +0000</pubDate><guid>https://lbenicio.dev/blog/the-100microsecond-rule-why-tail-latency-eats-your-throughput-and-how-to-fight-back/</guid><description>&lt;p&gt;If you stare at a performance dashboard long enough, you’ll eventually see a ghost—an outlier that refuses to go away. It’s the P99 spike that surfaces at the worst time; the one you “fix” three times and then rediscover during a product launch or a perfectly normal Tuesday.&lt;/p&gt;
&lt;p&gt;Here’s the hard truth: tail latency doesn’t just ruin your service levels; it compounds into lost throughput and broken guarantees. In systems with fan‑out, retries, and microservices, the slowest 1% isn’t “rare”—it’s the norm you ship to users most of the time. This is the 100‑microsecond rule in practice: small latencies multiply brutally at scale, and the invisible cost often starts below a single millisecond.&lt;/p&gt;</description></item><item><title>Auditing the Algorithm: Building a Responsible AI Pipeline That Scales</title><link>https://lbenicio.dev/blog/auditing-the-algorithm-building-a-responsible-ai-pipeline-that-scales/</link><pubDate>Sat, 05 Apr 2025 13:25:00 +0000</pubDate><guid>https://lbenicio.dev/blog/auditing-the-algorithm-building-a-responsible-ai-pipeline-that-scales/</guid><description>&lt;p&gt;When regulators asked for evidence that our AI systems behave responsibly, we realized spreadsheets and ad hoc reviews wouldn&amp;rsquo;t suffice. Our models influenced credit decisions, hiring suggestions, and medical triage. Stakeholders demanded more than accuracy—they wanted fairness, explainability, and accountability. We responded by building a responsible AI audit pipeline woven into development, deployment, and operations. This article shares how we did it: the processes, tooling, and cultural shifts that turned compliance into continuous curiosity.&lt;/p&gt;</description></item><item><title>Scheduling: Trading Latency for Throughput (and Back Again)</title><link>https://lbenicio.dev/blog/scheduling-trading-latency-for-throughput-and-back-again/</link><pubDate>Wed, 12 Feb 2025 10:00:00 +0000</pubDate><guid>https://lbenicio.dev/blog/scheduling-trading-latency-for-throughput-and-back-again/</guid><description>&lt;p&gt;Schedulers encode policy: who runs next, on which core, and for how long. Those choices shuffle latency and throughput. Let’s make the trade‑offs explicit.&lt;/p&gt;
&lt;h2 id="fifo-vs-priority-vs-fair"&gt;FIFO vs. priority vs. fair&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;FIFO: simple, minimal overhead; tail prone under bursty arrivals.&lt;/li&gt;
&lt;li&gt;Priority: protects critical work; risks starvation without aging.&lt;/li&gt;
&lt;li&gt;Fair (CFQ‑like): shares CPU evenly; may underutilize when work is imbalanced.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="work-stealing"&gt;Work stealing&lt;/h2&gt;
&lt;p&gt;Great for irregular parallel workloads. Each worker has a deque; thieves steal from the tail. Pros: high utilization; Cons: cache locality losses and noisy tails under high contention.&lt;/p&gt;</description></item><item><title>Seeing in the Dark: Observability for Edge AI Fleets</title><link>https://lbenicio.dev/blog/seeing-in-the-dark-observability-for-edge-ai-fleets/</link><pubDate>Fri, 16 Aug 2024 10:55:00 +0000</pubDate><guid>https://lbenicio.dev/blog/seeing-in-the-dark-observability-for-edge-ai-fleets/</guid><description>&lt;p&gt;Our edge AI deployment began with a handful of pilot devices in retail stores. Within months, thousands of cameras, sensors, and point-of-sale terminals joined the fleet. They detected shelves running low, predicted queue lengths, and flagged suspicious transactions. But when a customer called asking why a device misclassified bananas as tennis balls, we realized our observability blurred at the edge. Logs vanished into the ether, metrics arrived sporadically, and models drifted silently. This article shares how we built observability robust enough for flaky networks, sensitive data, and autonomous updates.&lt;/p&gt;</description></item><item><title>Countdown to Quantum: Migrating an Enterprise to Post-Quantum Cryptography</title><link>https://lbenicio.dev/blog/countdown-to-quantum-migrating-an-enterprise-to-post-quantum-cryptography/</link><pubDate>Mon, 29 Jan 2024 16:40:00 +0000</pubDate><guid>https://lbenicio.dev/blog/countdown-to-quantum-migrating-an-enterprise-to-post-quantum-cryptography/</guid><description>&lt;p&gt;When the first credible quantum threat report reached our CISO&amp;rsquo;s desk, the reaction was cautious curiosity. Quantum computers capable of breaking RSA-2048 remain years away, but data harvested today could be decrypted decades later. We handle sensitive customer contracts, healthcare records, and government workloads. Waiting for the quantum threat to manifest felt irresponsible. So we embarked on a multi-year migration to post-quantum cryptography (PQC). This post documents the journey—strategy, tooling, experiments, and the human moments that kept the program afloat.&lt;/p&gt;</description></item><item><title>Sealing the Supply Chain: Zero-Trust Build Pipelines That Scale</title><link>https://lbenicio.dev/blog/sealing-the-supply-chain-zero-trust-build-pipelines-that-scale/</link><pubDate>Sun, 08 Oct 2023 21:10:00 +0000</pubDate><guid>https://lbenicio.dev/blog/sealing-the-supply-chain-zero-trust-build-pipelines-that-scale/</guid><description>&lt;p&gt;The day we discovered a compromised dependency in our build pipeline felt like waking up to find the locks on our headquarters replaced. The package arrived through a trusted mirror, signed with a familiar key, yet embedded a subtle time bomb. We escaped without production impact thanks to layered defenses, but the scare pushed us to redesign our build system around zero trust. This article chronicles that transformation—technical and human—from ad hoc pipelines to hermetic, verifiable, auditable delivery.&lt;/p&gt;</description></item><item><title>Reverse Indexing and Inverted Files: How Search Engines Fly</title><link>https://lbenicio.dev/blog/reverse-indexing-and-inverted-files-how-search-engines-fly/</link><pubDate>Wed, 19 Jul 2023 10:00:00 +0000</pubDate><guid>https://lbenicio.dev/blog/reverse-indexing-and-inverted-files-how-search-engines-fly/</guid><description>&lt;p&gt;Full‑text search is a masterclass in practical data structures. The inverted index—also called a reverse index—maps terms to the list of documents in which they occur. Everything else in a production search engine is optimization: reducing bytes, minimizing random I/O, and avoiding work you don’t have to do.&lt;/p&gt;
&lt;p&gt;In this deep dive we’ll build a complete mental model of inverted files and the techniques that make them fast:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Parsing pipeline: tokenization, normalization, stemming/lemmatization, and multilingual realities&lt;/li&gt;
&lt;li&gt;Index structure: vocabulary, postings (doc IDs), term frequencies, and positions for phrase queries&lt;/li&gt;
&lt;li&gt;Compression: delta encoding, Variable‑Byte (VB), Simple‑8b, PForDelta, SIMD‑BP128, QMX, and how they trade space for CPU&lt;/li&gt;
&lt;li&gt;Skipping and acceleration: skip lists, block max indexes, WAND/BMW dynamic pruning&lt;/li&gt;
&lt;li&gt;Scoring: BM25, term and document statistics, field boosts, and normalization&lt;/li&gt;
&lt;li&gt;Updates and merges: segment architecture (Lucene‑style), in‑place deletes, and background compaction&lt;/li&gt;
&lt;li&gt;Caching and tiering: hot vs cold shards, result caching, and Bloom‑like structures&lt;/li&gt;
&lt;li&gt;Distributed search: sharding, replication, and query fan‑out under tail latency pressure&lt;/li&gt;
&lt;li&gt;Measuring and tuning: from recall/precision to p95 query time, heap usage, and GC pauses&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;By the end you’ll be able to reason about why each knob exists and which ones matter for your workload.&lt;/p&gt;</description></item><item><title>Keeping the Model Awake: Building a Self-Healing ML Inference Platform</title><link>https://lbenicio.dev/blog/keeping-the-model-awake-building-a-self-healing-ml-inference-platform/</link><pubDate>Tue, 14 Feb 2023 07:20:00 +0000</pubDate><guid>https://lbenicio.dev/blog/keeping-the-model-awake-building-a-self-healing-ml-inference-platform/</guid><description>&lt;p&gt;During a winter holiday freeze, our recommendation API refused to scale. GPUs idled waiting for models to load, autoscalers fought each other, and on-call engineers reheated leftovers at 3 a.m. while spike traffic slammed into origin. We promised leadership that this would never happen again. The solution wasn&amp;rsquo;t magic; it was a self-healing inference platform blending old-school reliability, modern ML tooling, and relentless experimentation.&lt;/p&gt;
&lt;p&gt;This post documents the rebuild. We&amp;rsquo;ll explore model packaging, warm-up rituals, adaptive scheduling, observability, chaos drills, and the social contract between ML researchers and production engineers. The goal: keep models awake, snappy, and trustworthy even when the world throws curveballs.&lt;/p&gt;</description></item><item><title>Timeouts, Retries, and Idempotency Keys: A Practical Guide</title><link>https://lbenicio.dev/blog/timeouts-retries-and-idempotency-keys-a-practical-guide/</link><pubDate>Thu, 08 Sep 2022 10:00:00 +0000</pubDate><guid>https://lbenicio.dev/blog/timeouts-retries-and-idempotency-keys-a-practical-guide/</guid><description>&lt;p&gt;Every distributed call needs three decisions: how long to wait, how to retry, and how to avoid duplicate effects. Done right, these three give you calm dashboards during partial failures; done wrong, they turn blips into incidents. This is a practical guide you can wire into client libraries and services without heroics.&lt;/p&gt;
&lt;h2 id="budget-your-time-not-perhop-timeouts"&gt;Budget your time, not per‑hop timeouts&lt;/h2&gt;
&lt;p&gt;Propagate a request deadline. Each hop consumes a slice and passes the remainder. Treat it as a budget: if it’s nearly spent, fail fast. Per‑hop fixed timeouts accumulate and blow your SLOs because each layer waits its entire local timeout.&lt;/p&gt;</description></item><item><title>Teaching GraphQL to Cache at the Edge</title><link>https://lbenicio.dev/blog/teaching-graphql-to-cache-at-the-edge/</link><pubDate>Sat, 03 Sep 2022 12:15:00 +0000</pubDate><guid>https://lbenicio.dev/blog/teaching-graphql-to-cache-at-the-edge/</guid><description>&lt;p&gt;GraphQL promises tailor-made responses, but tailor-made payloads resist caching. For years, we treated GraphQL responses as ephemeral: generated on demand, personalized, too unique to reuse. Then mobile latency complaints reached a boiling point. Edge locations sat underutilized while origin clusters sweated. We set out to teach GraphQL how to cache—respecting declarative queries, personalization boundaries, and real-time freshness. This is the story of building an edge caching layer that felt invisible to developers yet shaved hundreds of milliseconds off user interactions.&lt;/p&gt;</description></item><item><title>Instrumenting Without Spying: Privacy-Preserving Telemetry at Scale</title><link>https://lbenicio.dev/blog/instrumenting-without-spying-privacy-preserving-telemetry-at-scale/</link><pubDate>Thu, 27 May 2021 18:45:00 +0000</pubDate><guid>https://lbenicio.dev/blog/instrumenting-without-spying-privacy-preserving-telemetry-at-scale/</guid><description>&lt;p&gt;Three years ago, our observability dashboards flickered ominously: ingestion lag, missing metrics, red error bars. We had paused telemetry from millions of devices after discovering that our pipeline collected more than we were comfortable storing. The pause kept our promise to users—but left engineers blind. We needed a way to reinstate observability without betraying trust. The result was a radical redesign: a privacy-preserving telemetry system that treats data minimization as a feature, not a compliance chore.&lt;/p&gt;</description></item><item><title>Cache‑Friendly Data Layouts: AoS vs. SoA (and the Hybrid In‑Between)</title><link>https://lbenicio.dev/blog/cachefriendly-data-layouts-aos-vs.-soa-and-the-hybrid-inbetween/</link><pubDate>Thu, 18 Mar 2021 10:00:00 +0000</pubDate><guid>https://lbenicio.dev/blog/cachefriendly-data-layouts-aos-vs.-soa-and-the-hybrid-inbetween/</guid><description>&lt;p&gt;The fastest code you’ll ever write usually isn’t a brand‑new algorithm—it’s the same algorithm organized in memory so the CPU (or GPU) can consume it efficiently. That journey often starts with a deceptively small choice: Array‑of‑Structs (AoS) or Struct‑of‑Arrays (SoA). The layout you choose determines which cachelines move, which prefetchers trigger, how branch predictors behave, and whether SIMD units stay busy or starve.&lt;/p&gt;
&lt;p&gt;In this post we’ll build an intuitive mental model for cachelines, TLBs, prefetchers, and vector units; examine AoS and SoA under realistic access patterns; then derive a hybrid (AoSoA) that quietly powers many high‑performance systems—from physics engines to databases to ML dataloaders. We’ll also walk through migration strategies, benchmarking pitfalls, and checklists you can apply this week.&lt;/p&gt;</description></item><item><title>Raft Fast‑Commit and PreVote in Practice</title><link>https://lbenicio.dev/blog/raft-fastcommit-and-prevote-in-practice/</link><pubDate>Mon, 09 Nov 2020 10:00:00 +0000</pubDate><guid>https://lbenicio.dev/blog/raft-fastcommit-and-prevote-in-practice/</guid><description>&lt;p&gt;Raft’s original paper optimizes for understandability. Production clusters optimize for availability and mean‑time‑to‑recovery. Two common extensions—PreVote and fast‑commit—reduce needless disruptions and trim the time it takes to make progress. Let’s unpack them without hand‑waving.&lt;/p&gt;
&lt;h2 id="the-pain-disruptive-elections-and-long-commit-paths"&gt;The pain: disruptive elections and long commit paths&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Without PreVote, a partitioned follower may increment term and trigger elections on rejoin, ousting a healthy leader.&lt;/li&gt;
&lt;li&gt;Without fast‑commit, clients wait for the full round‑trip alignment even when a quorum already holds the entry durably.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="prevote"&gt;PreVote&lt;/h2&gt;
&lt;p&gt;PreVote adds a “dry‑run” phase: before incrementing term, a candidate asks peers if they’d vote. Peers reject if their logs are more up‑to‑date.&lt;/p&gt;</description></item><item><title>Merkle Trees and Content‑Addressable Storage</title><link>https://lbenicio.dev/blog/merkle-trees-and-contentaddressable-storage/</link><pubDate>Mon, 17 Aug 2020 10:00:00 +0000</pubDate><guid>https://lbenicio.dev/blog/merkle-trees-and-contentaddressable-storage/</guid><description>&lt;p&gt;Hash the content, not the location—that’s the core of content‑addressable storage (CAS). Combine it with Merkle trees (or DAGs) and you get efficient verification, deduplication, and synchronization. This post connects the dots from Git to large‑scale object stores.&lt;/p&gt;
&lt;h2 id="why-merkle"&gt;Why Merkle?&lt;/h2&gt;
&lt;p&gt;Parent hashes commit to child hashes; any change percolates up. You can verify integrity by checking a root hash and a short proof path.&lt;/p&gt;
&lt;h2 id="uses"&gt;Uses&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Git commits and trees; shallow clones via missing subtrees.&lt;/li&gt;
&lt;li&gt;Package managers with integrity checks.&lt;/li&gt;
&lt;li&gt;Deduplicated backups with chunking and rolling hashes.&lt;/li&gt;
&lt;li&gt;Object stores that replicate by exchanging missing subgraphs.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="practical-notes"&gt;Practical notes&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Hash choice (SHA‑256 vs BLAKE3) affects speed and hardware support.&lt;/li&gt;
&lt;li&gt;Chunking strategy controls dedupe granularity; rolling fingerprints (Rabin) find natural boundaries.&lt;/li&gt;
&lt;li&gt;Store metadata alongside blobs to avoid rehashing for trivial changes.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="1-from-trees-to-dags-modeling-versions-that-share-content"&gt;1) From trees to DAGs: modeling versions that share content&lt;/h2&gt;
&lt;p&gt;Classic Merkle trees have a fixed arity and two kinds of nodes: leaves containing hashes of fixed‑size blocks, and internal nodes containing hashes of their children. In real systems, multiple versions of content share substructure: two versions of a directory share most files, two backups share most chunks, two container images share many layers. That sharing naturally forms a directed acyclic graph (DAG) where a node can be referenced from multiple parents. The root is still a commitment to the whole, but now many roots can reference common subgraphs without duplication.&lt;/p&gt;</description></item><item><title>Tuning the Dial: Adaptive Consistency at Planet Scale</title><link>https://lbenicio.dev/blog/tuning-the-dial-adaptive-consistency-at-planet-scale/</link><pubDate>Wed, 11 Mar 2020 14:05:00 +0000</pubDate><guid>https://lbenicio.dev/blog/tuning-the-dial-adaptive-consistency-at-planet-scale/</guid><description>&lt;p&gt;At 2:17 a.m. UTC, a partner bank in Singapore called our incident bridge. A fund transfer appeared twice in their ledger. The culprit: a replication lag spike between Singapore and Frankfurt had stretched past our standard safety buffers. Historically we would have halted writes across the fleet, cutting availability to protect consistency. Instead, our adaptive consistency layer dialed a region-specific policy: Singapore moved from &amp;ldquo;read-after-write&amp;rdquo; to &amp;ldquo;read-your-writes&amp;rdquo; guarantees, while Frankfurt raised its commit quorum. The double posting self-corrected before social media noticed. No downtime, no irreversible loss—just a story about how we learned to treat consistency as a spectrum rather than a binary.&lt;/p&gt;</description></item><item><title>When Data Centers Learned to Sleep: Energy-Aware Scheduling in Practice</title><link>https://lbenicio.dev/blog/when-data-centers-learned-to-sleep-energy-aware-scheduling-in-practice/</link><pubDate>Fri, 19 Jul 2019 09:30:00 +0000</pubDate><guid>https://lbenicio.dev/blog/when-data-centers-learned-to-sleep-energy-aware-scheduling-in-practice/</guid><description>&lt;p&gt;The first time we let a data center &amp;ldquo;sleep&amp;rdquo; during daylight hours felt reckless. Customers trusted us with near-infinite elasticity. Flipping servers into deep power states to save energy sounded like penny-pinching, not engineering. Yet the math was undeniable: idling a hyperscale fleet burned as much electricity as a mid-size city. This post tells the story of how we evolved from skepticism to confidence, building energy-aware scheduling systems that keep promises while honoring planetary limits.&lt;/p&gt;</description></item><item><title>Speculative Prefetchers: Designing Memory Systems That Read the Future</title><link>https://lbenicio.dev/blog/speculative-prefetchers-designing-memory-systems-that-read-the-future/</link><pubDate>Thu, 14 Feb 2019 10:00:00 +0000</pubDate><guid>https://lbenicio.dev/blog/speculative-prefetchers-designing-memory-systems-that-read-the-future/</guid><description>&lt;p&gt;At 2:17 a.m., the on-call performance engineer watches another alert crawl across the dashboard. The new machine image promised higher throughput for a latency-sensitive analytics service, yet caches still thrash whenever end-of-day reconciliation jobs arrive. Each job walks a sparsely linked graph of customer transactions, and the CPU spends more time waiting on memory than executing instructions. &amp;ldquo;If only the hardware could guess where the program was going next,&amp;rdquo; she sighs. That daydream is the seed of speculative prefetching—the art of reading tomorrow’s memory today.&lt;/p&gt;</description></item></channel></rss>