<!doctype html><html lang=en><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,maximum-scale=1,minimum-scale=1,user-scalable=no"><title>GPUDirect Storage in 2025: Optimizing the End-to-End Data Path · Leonardo Benicio</title><meta name=description content="How modern systems move data from NVMe and object storage into GPU kernels with minimal CPU overhead and maximal throughput."><link rel=alternate type=application/rss+xml title=RSS href=https://lbenicio.dev/index.xml><link rel=canonical href=https://blog.lbenicio.dev/blog/gpudirect-storage-in-2025-optimizing-the-end-to-end-data-path/><link rel=preload href=/static/fonts/OpenSans-Regular.ttf as=font type=font/ttf crossorigin><link rel="stylesheet" href="/assets/css/fonts.min.40e2054b739ac45a0f9c940f4b44ec00c3b372356ebf61440a413c0337c5512e.css" crossorigin="anonymous" integrity="sha256-QOIFS3OaxFoPnJQPS0TsAMOzcjVuv2FECkE8AzfFUS4="><link rel="shortcut icon" href=/static/assets/favicon/favicon.ico><link rel=icon type=image/x-icon href=/static/assets/favicon/favicon.ico><link rel=icon href=/static/assets/favicon/favicon.svg type=image/svg+xml><link rel=icon href=/static/assets/favicon/favicon-32x32.png sizes=32x32 type=image/png><link rel=icon href=/static/assets/favicon/favicon-16x16.png sizes=16x16 type=image/png><link rel=apple-touch-icon href=/static/assets/favicon/apple-touch-icon.png><link rel=manifest href=/static/assets/favicon/site.webmanifest><link rel=mask-icon href=/static/assets/favicon/safari-pinned-tab.svg color=#209cee><meta name=msapplication-TileColor content="#209cee"><meta name=msapplication-config content="/static/assets/favicon/browserconfig.xml"><meta name=theme-color content="#d2e9f8"><meta property="og:title" content="GPUDirect Storage in 2025: Optimizing the End-to-End Data Path · Leonardo Benicio"><meta property="og:description" content="How modern systems move data from NVMe and object storage into GPU kernels with minimal CPU overhead and maximal throughput."><meta property="og:url" content="https://blog.lbenicio.dev/blog/gpudirect-storage-in-2025-optimizing-the-end-to-end-data-path/"><meta property="og:type" content="article"><meta property="og:image" content="https://blog.lbenicio.dev/static/assets/images/blog/gpudirect-storage.png"><meta name=twitter:card content="summary_large_image"><meta name=twitter:title content="GPUDirect Storage in 2025: Optimizing the End-to-End Data Path · Leonardo Benicio"><meta name=twitter:description content="How modern systems move data from NVMe and object storage into GPU kernels with minimal CPU overhead and maximal throughput."><meta name=twitter:site content="@lbenicio_"><script type=application/ld+json>"{\"@context\":\"https://schema.org\",\"@type\":\"WebSite\",\"name\":\"About Leonardo Benicio\",\"url\":\"https://blog.lbenicio.dev\"}"</script><script type=application/ld+json>"{\"@context\":\"https://schema.org\",\"@type\":\"Person\",\"name\":\"Leonardo Benicio\",\"sameAs\":[\"https://github.com/lbenicio\",\"https://www.linkedin.com/in/leonardo-benicio\",\"https://twitter.com/lbenicio_\"],\"url\":\"https://blog.lbenicio.dev\"}"</script><script type=application/ld+json>"{\"@context\":\"https://schema.org\",\"@type\":\"BreadcrumbList\",\"itemListElement\":[{\"@type\":\"ListItem\",\"item\":\"https://blog.lbenicio.dev/\",\"name\":\"Home\",\"position\":1},{\"@type\":\"ListItem\",\"item\":\"https://blog.lbenicio.dev/\",\"name\":\"Blog\",\"position\":2},{\"@type\":\"ListItem\",\"item\":\"https://blog.lbenicio.dev/blog/gpudirect-storage-in-2025-optimizing-the-end-to-end-data-path/\",\"name\":\"Gpudirect Storage in 2025 Optimizing the End to End Data Path\",\"position\":3}]}"</script><link rel="stylesheet" href="/assets/css/main.min.23cb77fd3186d94b425cf879bfff3195d7648b23b860d880dbb47fe2e115b884.css" crossorigin="anonymous" integrity="sha256-owHVkwE1+9dguAma85DLJbKG8+7vYa137CVrUeaaaxk="></head><body class="c6942b3 c03620d cf3bd2e"><script>(function(){try{document.addEventListener("gesturestart",function(e){e.preventDefault()}),document.addEventListener("touchstart",function(e){e.touches&&e.touches.length>1&&e.preventDefault()},{passive:!1});var e=0;document.addEventListener("touchend",function(t){var n=Date.now();n-e<=300&&t.preventDefault(),e=n},{passive:!1})}catch{}})()</script><a href=#content class="cba5854 c21e770 caffa6e cc5f604 cf2c31d cdd44dd c10dda9 c43876e c787e9b cddc2d2 cf55a7b c6dfb1e c9391e2">Skip to content</a>
<script>(function(){try{const e=localStorage.getItem("theme");e==="dark"&&document.documentElement.classList.add("dark");const t=document.querySelector('button[aria-label="Toggle theme"]');t&&t.setAttribute("aria-pressed",String(e==="dark"))}catch{}})();function toggleTheme(e){const s=document.documentElement,t=s.classList.toggle("dark");try{localStorage.setItem("theme",t?"dark":"light")}catch{}try{var n=e&&e.nodeType===1?e:document.querySelector('button[aria-label="Toggle theme"]');n&&n.setAttribute("aria-pressed",String(!!t))}catch{}}</script><header class="cd019ba c98dfae cdd44dd cfdda01 c9ee25d ce2dc7a cd72dd7 cc0dc37" role=banner><div class="cfdda01 c6942b3 ccf47f4 c7c11d8"><a href=/ class="c87e2b0 c6942b3 c7c11d8 c1838fa cb594e4" aria-label=Home><img src=/static/assets/favicon/favicon.svg alt=Logo width=32 height=32 class="c3de71a c4d5191">
<span class="cf8f011 c4d1253 cbd72bc cd7e69e">Leonardo Benicio</span></a><div class="c6942b3 c85cbd4 c7c11d8 ca798da c1838fa c7a0580"><nav class="cc1689c cd9b445 c75065d c04bab1" aria-label=Main><a href=/ class="c4d1253 c9e4539 cbbda39 c01f421 c19ee42 c3ecea6">Home</a>
<a href=https://lbenicio.dev/about target=_blank rel="noopener noreferrer" class="c4d1253 c9e4539 cbbda39 c01f421 c19ee42 c3ecea6">About</a>
<a href=https://lbenicio.dev/timeline target=_blank rel="noopener noreferrer" class="c4d1253 c9e4539 cbbda39 c01f421 c19ee42 c3ecea6">Timeline</a>
<a href=https://lbenicio.dev/reading target=_blank rel="noopener noreferrer" class="c4d1253 c9e4539 cbbda39 c01f421 c19ee42 c3ecea6">Reading</a>
<a href=https://lbenicio.dev/publications target=_blank rel="noopener noreferrer" class="c4d1253 c9e4539 cbbda39 c01f421 c19ee42 c3ecea6">Publications</a>
<a href=https://lbenicio.dev/contact target=_blank rel="noopener noreferrer" class="c4d1253 c9e4539 cbbda39 c01f421 c19ee42 c3ecea6">Contact</a></nav><button id="i1d73d4" type=button class="c1d6c20 c81ac7c c6a899b c7c11d8 c1d0018 c10dda9 c8e184d c514027 c88daee c7a66a6 c097fa1 cfc01c7 c286dd7 c2bd687 cfdce1d cfef18f" onclick=toggleTheme(this) aria-label="Toggle theme" aria-pressed=false title="Toggle theme">
<svg class="cb26e41 c50ceea cb69a5c c4f45c8 c8c2c40" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" aria-hidden="true"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg>
<svg class="cb26e41 c8fca2b cb69a5c c4f45c8 cc1689c c9c27ff" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" aria-hidden="true"><circle cx="12" cy="12" r="4"/><path d="M12 2v4"/><path d="M12 18v4"/><path d="M2 12h4"/><path d="M18 12h4"/><path d="M4.93 4.93l2.83 2.83"/><path d="M16.24 16.24l2.83 2.83"/><path d="M6.34 17.66l2.83-2.83"/><path d="M14.83 9.17l2.83-2.83"/></svg>
<span class="cba5854">Toggle theme</span></button><div class="c658bcf c097fa1"><details class="ccd45bf"><summary class="cc7a258 c1d6c20 c7c11d8 c1d0018 c10dda9 c000b66 cf55a7b"><svg class="c20e4eb cb58471" fill="none" viewBox="0 0 24 24" stroke="currentColor"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M4 6h16M4 12h16M4 18h16"/></svg>
<span class="cba5854">Open menu</span></summary><div class="ce49c1e c437fa9 c1b4412 c8c0110 c887979 c43876e c10dda9 c60a4cc c401fa1 cb2c551 cf514a5 cadfe0b ce3dbb2 c72ad85 cbd4710 c6988b4"><a href=/ class="c62aaf0 c364589 c6942b3 c7c11d8 c1838fa" aria-label=Home><img src=/static/assets/favicon/favicon.svg alt=Logo width=24 height=24 class="c20e4eb cb58471">
<span class="cf8f011 c7c1b66 cbd72bc cbac0b8">Leonardo Benicio</span></a><nav class="c6942b3 c03620d cd69733"><a href=/ class="c4d1253 cbbda39 c3ecea6 c19ee42">Home</a>
<a href=https://lbenicio.dev/about target=_blank rel="noopener noreferrer" class="c4d1253 cbbda39 c3ecea6 c19ee42">About</a>
<a href=https://lbenicio.dev/timeline target=_blank rel="noopener noreferrer" class="c4d1253 cbbda39 c3ecea6 c19ee42">Timeline</a>
<a href=https://lbenicio.dev/reading target=_blank rel="noopener noreferrer" class="c4d1253 cbbda39 c3ecea6 c19ee42">Reading</a>
<a href=https://lbenicio.dev/publications target=_blank rel="noopener noreferrer" class="c4d1253 cbbda39 c3ecea6 c19ee42">Publications</a>
<a href=https://lbenicio.dev/contact target=_blank rel="noopener noreferrer" class="c4d1253 cbbda39 c3ecea6 c19ee42">Contact</a></nav></div></details></div></div></div></header><div class="caffa6e c437fa9 ce9aced c97bba6 c15da2a c975cba" role=complementary aria-label="GitHub repository"><div class="c9d056d c252f85 ca22532 ca88a1a c876315"><div class="c6942b3 c7c11d8 c1d0018 cd1fd22 c6066e4 c43876e ce3d5b6 caa20d2 c3ecea6 c0cd2e2 cddc2d2 c3ed5c9 cd4074c c876315"><a href=https://github.com/lbenicio/aboutme target=_blank rel="noopener noreferrer" class="c6942b3 c7c11d8 cd1fd22 c71bae8 cfac1ac c19ee42 c25dc7c cb40739 cbbda39 cf55a7b" aria-label="View source on GitHub"><svg width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="cb26e41 c41bcd4 cf17690 cfa4e34 c78d562" aria-hidden="true"><path d="M15 22v-4a4.8 4.8.0 00-1-3.5c3 0 6-2 6-5.5.08-1.25-.27-2.48-1-3.5.28-1.15.28-2.35.0-3.5.0.0-1 0-3 1.5-2.64-.5-5.36-.5-8 0C6 2 5 2 5 2c-.3 1.15-.3 2.35.0 3.5A5.403 5.403.0 004 9c0 3.5 3 5.5 6 5.5-.39.49-.68 1.05-.85 1.65-.17.6-.22 1.23-.15 1.85v4"/><path d="M9 18c-4.51 2-5-2-7-2"/></svg>
<span class="cb5c327 cd7e69e">Fork me</span></a></div></div></div><main id="i7eccc0" class="cfdda01 c5df473 c0eecc8 c85cbd4" role=main aria-label=Content><nav class="cb545ce c8d8ae4 c277478" aria-label=Breadcrumb><ol class="c6942b3 c3adaf2 c7c11d8 cd365ee c3ecea6"><li><a href=/ class="c19ee42 c71bae8 cfac1ac">Home</a></li><li class="c6942b3 c7c11d8 cd365ee"><span class="cb82ddd">/</span><a href=/ class="c19ee42 c71bae8 cfac1ac">Blog</a></li><li class="c6942b3 c7c11d8 cd365ee"><span class="cb82ddd">/</span><span class="c88daee">Gpudirect Storage in 2025 Optimizing the End to End Data Path</span></li></ol></nav><article class="c461ba0 c1c203f cfb6084 c995404 c6ca165"><nav class="cb545ce c8d8ae4 c277478" aria-label=Breadcrumb><ol class="c6942b3 c3adaf2 c7c11d8 cd365ee c3ecea6"><li><a href=/ class="c19ee42 c71bae8 cfac1ac">Home</a></li><li class="c6942b3 c7c11d8 cd365ee"><span class="cb82ddd">/</span><a href=/ class="c19ee42 c71bae8 cfac1ac">Blog</a></li><li class="c6942b3 c7c11d8 cd365ee"><span class="cb82ddd">/</span><span class="c88daee">Gpudirect Storage in 2025 Optimizing the End to End Data Path</span></li></ol></nav><header class="c8aedc7"><h1 class="cf304bc c6fb0fe cf8f011 cc484e1">GPUDirect Storage in 2025: Optimizing the End-to-End Data Path</h1><div class="c277478 c3ecea6 c8fb24a">2025-09-16
· Leonardo Benicio</div><div class="c1a1a3f c8124f2"><img src=/static/assets/images/blog/gpudirect-storage.png alt class="cfdda01 c524300 c677556"></div><p class="lead c3ecea6">How modern systems move data from NVMe and object storage into GPU kernels with minimal CPU overhead and maximal throughput.</p></header><div class="content"><p>High-performance analytics and training pipelines increasingly hinge on how <em>fast</em> and <em>efficiently</em> data reaches GPU memory. Compute has outpaced I/O: a single multi-GPU node can sustain tens of TFLOPs while starved by a few misconfigured storage or copy stages. GPUDirect Storage (GDS) extends the GPUDirect family (peer-to-peer, RDMA) to allow DMA engines (NVMe, NIC) to move bytes directly between storage and GPU memory—bypassing redundant copies through host DRAM and reducing CPU intervention.</p><p>This article provides a deep, engineering-focused exploration of the end-to-end path: filesystems, NVMe queues, PCIe / CXL fabrics, GPU memory hierarchies, kernel launch overlap, compression, and telemetry. It mirrors the style of prior posts: structured sections, performance modeling, tuning checklist, pitfalls, future directions, and references.</p><hr><h2 id="1-motivation--problem-statement">1. Motivation & Problem Statement</h2><h3 id="why-the-data-path-matters">Why the Data Path Matters</h3><ol><li><strong>GPU Utilization Sensitivity</strong>: Idle SMs due to I/O stalls waste expensive accelerator time.</li><li><strong>Energy Efficiency</strong>: Extra memory copies burn power (DDR→PCIe→GPU) with no useful work.</li><li><strong>CPU Contention</strong>: Data loader threads compete with orchestration, scheduling, and networking tasks.</li><li><strong>Scalability Breakpoints</strong>: Adding GPUs does not scale linearly if the I/O subsystem saturates at earlier tiers.</li></ol><h3 id="traditional-path">Traditional Path</h3><p><code>Storage (NVMe / Object)</code> → <code>Kernel / FUSE / Filesystem</code> → <code>Page Cache</code> → <code>User-space read()</code> → <code>Pinned Buffer (cudaHostAlloc)</code> → <code>cudaMemcpy</code> → <code>GPU global memory</code> → <code>Kernel</code></p><p>Each arrow can introduce latency, CPU cycles, cache pollution, and memory bandwidth consumption.</p><h3 id="gds-enhanced-path-ideal">GDS-Enhanced Path (Ideal)</h3><p><code>Storage</code> → <code>DMA (NVMe controller or RDMA NIC)</code> → <code>GPU BAR / Memory</code> → <code>Kernel</code></p><p>Host CPU involvement shrinks to submission/completion queue management and control-plane scheduling.</p><hr><h2 id="2-architectural-components">2. Architectural Components</h2><h3 id="21-storage-device-layer">2.1 Storage Device Layer</h3><ul><li><strong>NVMe SSDs</strong>: Provide parallel submission/completion queues; PCIe Gen5 x4 lanes approach ~14 GB/s. Multiple drives can be striped (RAID0 / software striping) for higher aggregate throughput.</li><li><strong>Zoned Namespace (ZNS)</strong>: Reduces FTL overhead; sequential zone append patterns align with large batched reads.</li><li><strong>NVDIMMs / PMem (legacy)</strong>: Still present in some tiered designs as intermediate caches.</li></ul><h3 id="22-interconnect--fabric">2.2 Interconnect & Fabric</h3><ul><li><strong>PCIe Gen5 / Gen6</strong>: Latency ~150ns fabric hops; lane bifurcation and switch topology shape contention. Avoid oversubscribing upstream link for combined NIC + NVMe + GPUs.</li><li><strong>CXL.io / CXL.mem</strong>: Emerging for memory expansion; future direct GPU access to pooled memory may blur staging distinctions.</li><li><strong>NVLink / NVSwitch</strong>: Enables peer GPU memory forwarding; in multi-GPU pipelines, one GPU may prefetch for others.</li></ul><h3 id="23-gpu-memory--hierarchy">2.3 GPU Memory & Hierarchy</h3><ul><li>Global memory (HBM) is destination for DMA. L2 acts as a large cache for streaming kernels; proper alignment and large transfer granularities (≥128 KB) improve efficiency.</li><li>Page faulting (on-demand memory) adds unpredictable latency; prefer explicit prefetch or pinned allocations.</li></ul><h3 id="24-software-stack">2.4 Software Stack</h3><table><thead><tr><th>Layer</th><th>Role</th><th>Latency Sensitivity</th><th>Key Tuning</th></tr></thead><tbody><tr><td>Filesystem (ext4/xfs/beeGFS/Lustre)</td><td>Namespace & metadata</td><td>Moderate</td><td>Mount opts, stripe size</td></tr><tr><td>Block Layer / NVMe Driver</td><td>Queueing & DMA submission</td><td>High</td><td>IO depth, IRQ affinity</td></tr><tr><td>GDS Library (cuFile)</td><td>Direct path management</td><td>High</td><td>Batch size, alignment</td></tr><tr><td>CUDA Runtime</td><td>Stream & event scheduling</td><td>High</td><td>Concurrency, priority streams</td></tr><tr><td>Application Loader</td><td>Batching, decode, augment</td><td>High</td><td>Async pipelines, thread pinning</td></tr></tbody></table><hr><h2 id="3-data-flow-variants">3. Data Flow Variants</h2><h3 id="31-synchronous-cpu-mediated-copy">3.1 Synchronous CPU-Mediated Copy</h3><ol><li>CPU issues <code>read()</code> into page cache.</li><li>Copies into pinned buffer.</li><li>Launches <code>cudaMemcpyAsync</code> to device.</li><li>Kernel waits on stream event.</li></ol><p>Issues: double-copy overhead, CPU involvement on hot path, page cache thrash for streaming-only data.</p><h3 id="32-direct-storage-to-gpu-gds">3.2 Direct Storage to GPU (GDS)</h3><ol><li>Register file descriptor with cuFile (establish mapping & capabilities).</li><li>Issue <code>cuFileRead</code> into device pointer.</li><li>Use CUDA events to chain kernel execution after DMA completes.</li></ol><p>Reduces memory traffic and CPU load; requires large contiguous reads for best efficiency.</p><h3 id="33-nic-to-gpu-object-storage--rdma">3.3 NIC to GPU (Object Storage + RDMA)</h3><p>When remote object storage supports RDMA (or via gateway node), data can flow: <code>Remote NVMe</code> → <code>RDMA NIC</code> → <code>GPU</code>. Latency dominated by network round-trip; parallel outstanding reads mitigate.</p><h3 id="34-multi-gpu-prefetch--relay">3.4 Multi-GPU Prefetch & Relay</h3><p>GPU0 prefetches data slice and uses NVLink P2P (<code>cudaMemcpyPeerAsync</code>) to distribute subsets to GPU1..N while its own kernel processes earlier batch—overlapping ingest and compute.</p><hr><h2 id="4-api--code-examples">4. API & Code Examples</h2><h3 id="41-basic-cufile-read">4.1 Basic cuFile Read</h3><div class="highlight"><pre tabindex=0 style=color:#e6edf3;background-color:#0d1117;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class="language-c" data-lang=c><span style=display:flex><span><span style=color:#8b949e;font-weight:700;font-style:italic>#include</span> <span style=color:#8b949e;font-weight:700;font-style:italic>&lt;cufile.h&gt;</span><span style=color:#8b949e;font-weight:700;font-style:italic>
</span></span></span><span style=display:flex><span><span style=color:#8b949e;font-weight:700;font-style:italic>#include</span> <span style=color:#8b949e;font-weight:700;font-style:italic>&lt;cuda_runtime.h&gt;</span><span style=color:#8b949e;font-weight:700;font-style:italic>
</span></span></span><span style=display:flex><span><span style=color:#8b949e;font-weight:700;font-style:italic>#include</span> <span style=color:#8b949e;font-weight:700;font-style:italic>&lt;fcntl.h&gt;</span><span style=color:#8b949e;font-weight:700;font-style:italic>
</span></span></span><span style=display:flex><span><span style=color:#8b949e;font-weight:700;font-style:italic>#include</span> <span style=color:#8b949e;font-weight:700;font-style:italic>&lt;unistd.h&gt;</span><span style=color:#8b949e;font-weight:700;font-style:italic>
</span></span></span><span style=display:flex><span><span style=color:#8b949e;font-weight:700;font-style:italic></span>
</span></span><span style=display:flex><span><span style=color:#ff7b72>int</span> <span style=color:#d2a8ff;font-weight:700>main</span>(){
</span></span><span style=display:flex><span>    <span style=color:#ff7b72>const</span> <span style=color:#ff7b72>char</span><span style=color:#ff7b72;font-weight:700>*</span> path <span style=color:#ff7b72;font-weight:700>=</span> <span style=color:#a5d6ff>&#34;/mnt/datasets/segment.bin&#34;</span>;
</span></span><span style=display:flex><span>    <span style=color:#ff7b72>int</span> fd <span style=color:#ff7b72;font-weight:700>=</span> <span style=color:#d2a8ff;font-weight:700>open</span>(path, O_RDONLY <span style=color:#ff7b72;font-weight:700>|</span> O_DIRECT);
</span></span><span style=display:flex><span>    <span style=color:#d2a8ff;font-weight:700>cuFileDriverOpen</span>();
</span></span><span style=display:flex><span>    CUfileDescr_t desc <span style=color:#ff7b72;font-weight:700>=</span> {};
</span></span><span style=display:flex><span>    desc.handle.fd <span style=color:#ff7b72;font-weight:700>=</span> fd;
</span></span><span style=display:flex><span>    desc.type <span style=color:#ff7b72;font-weight:700>=</span> CU_FILE_HANDLE_TYPE_OPAQUE_FD;
</span></span><span style=display:flex><span>    CUfileHandle_t handle;
</span></span><span style=display:flex><span>    <span style=color:#d2a8ff;font-weight:700>cuFileHandleRegister</span>(<span style=color:#ff7b72;font-weight:700>&amp;</span>handle, <span style=color:#ff7b72;font-weight:700>&amp;</span>desc);
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#ff7b72>size_t</span> bytes <span style=color:#ff7b72;font-weight:700>=</span> <span style=color:#a5d6ff>128UL</span> <span style=color:#ff7b72;font-weight:700>*</span> <span style=color:#a5d6ff>1024</span> <span style=color:#ff7b72;font-weight:700>*</span> <span style=color:#a5d6ff>1024</span>; <span style=color:#8b949e;font-style:italic>// 128 MB
</span></span></span><span style=display:flex><span><span style=color:#8b949e;font-style:italic></span>    <span style=color:#ff7b72>void</span><span style=color:#ff7b72;font-weight:700>*</span> d_ptr;
</span></span><span style=display:flex><span>    <span style=color:#d2a8ff;font-weight:700>cudaMalloc</span>(<span style=color:#ff7b72;font-weight:700>&amp;</span>d_ptr, bytes);
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#ff7b72>ssize_t</span> ret <span style=color:#ff7b72;font-weight:700>=</span> <span style=color:#d2a8ff;font-weight:700>cuFileRead</span>(handle, d_ptr, bytes, <span style=color:#a5d6ff>0</span>, <span style=color:#a5d6ff>0</span>);
</span></span><span style=display:flex><span>    <span style=color:#ff7b72>if</span> (ret <span style=color:#ff7b72;font-weight:700>&lt;</span> <span style=color:#a5d6ff>0</span>) { <span style=color:#8b949e;font-style:italic>/* handle error */</span> }
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#8b949e;font-style:italic>// Launch kernel using data
</span></span></span><span style=display:flex><span><span style=color:#8b949e;font-style:italic></span>    <span style=color:#8b949e;font-style:italic>// myKernel&lt;&lt;&lt;grid, block, 0, stream&gt;&gt;&gt;(d_ptr, ...);
</span></span></span><span style=display:flex><span><span style=color:#8b949e;font-style:italic></span>
</span></span><span style=display:flex><span>    <span style=color:#d2a8ff;font-weight:700>cuFileHandleDeregister</span>(handle);
</span></span><span style=display:flex><span>    <span style=color:#d2a8ff;font-weight:700>close</span>(fd);
</span></span><span style=display:flex><span>    <span style=color:#d2a8ff;font-weight:700>cuFileDriverClose</span>();
</span></span><span style=display:flex><span>    <span style=color:#d2a8ff;font-weight:700>cudaFree</span>(d_ptr);
</span></span><span style=display:flex><span>}
</span></span></code></pre></div><h3 id="42-overlapped-batch-pipeline-pseudo-python">4.2 Overlapped Batch Pipeline (Pseudo-Python)</h3><div class="highlight"><pre tabindex=0 style=color:#e6edf3;background-color:#0d1117;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class="language-python" data-lang=python><span style=display:flex><span><span style=color:#8b949e;font-style:italic># Pseudo: overlap I/O and compute on batches</span>
</span></span><span style=display:flex><span>streams <span style=color:#ff7b72;font-weight:700>=</span> [cuda<span style=color:#ff7b72;font-weight:700>.</span>Stream() <span style=color:#ff7b72>for</span> _ <span style=color:#ff7b72;font-weight:700>in</span> range(P)]
</span></span><span style=display:flex><span>buffers <span style=color:#ff7b72;font-weight:700>=</span> [cuda<span style=color:#ff7b72;font-weight:700>.</span>device_array((BATCH, SHAPE), dtype<span style=color:#ff7b72;font-weight:700>=</span>np<span style=color:#ff7b72;font-weight:700>.</span>float32) <span style=color:#ff7b72>for</span> _ <span style=color:#ff7b72;font-weight:700>in</span> range(P)]
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#ff7b72>for</span> batch_idx, offset <span style=color:#ff7b72;font-weight:700>in</span> enumerate(range(<span style=color:#a5d6ff>0</span>, total_bytes, chunk)):
</span></span><span style=display:flex><span>    s <span style=color:#ff7b72;font-weight:700>=</span> streams[batch_idx <span style=color:#ff7b72;font-weight:700>%</span> P]
</span></span><span style=display:flex><span>    buf <span style=color:#ff7b72;font-weight:700>=</span> buffers[batch_idx <span style=color:#ff7b72;font-weight:700>%</span> P]
</span></span><span style=display:flex><span>    cufile<span style=color:#ff7b72;font-weight:700>.</span>read(fd, buf, size<span style=color:#ff7b72;font-weight:700>=</span>chunk, file_offset<span style=color:#ff7b72;font-weight:700>=</span>offset, stream<span style=color:#ff7b72;font-weight:700>=</span>s)
</span></span><span style=display:flex><span>    model<span style=color:#ff7b72;font-weight:700>.</span>forward_async(buf, stream<span style=color:#ff7b72;font-weight:700>=</span>s)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#8b949e;font-style:italic># synchronize at end</span>
</span></span><span style=display:flex><span><span style=color:#ff7b72>for</span> s <span style=color:#ff7b72;font-weight:700>in</span> streams:
</span></span><span style=display:flex><span>    s<span style=color:#ff7b72;font-weight:700>.</span>synchronize()
</span></span></code></pre></div><h3 id="43-hybrid-path-compression--direct-read">4.3 Hybrid Path: Compression + Direct Read</h3><p>Read compressed blocks directly to GPU, then launch GPU decompression (e.g., nvCOMP) before model ingestion.</p><div class="highlight"><pre tabindex=0 style=color:#e6edf3;background-color:#0d1117;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class="language-c++" data-lang=c++><span style=display:flex><span><span style=color:#8b949e;font-style:italic>// Pseudocode structure
</span></span></span><span style=display:flex><span><span style=color:#8b949e;font-style:italic></span><span style=color:#ff7b72>for</span> (<span style=color:#79c0ff;font-weight:700>block</span> : blocks) {
</span></span><span style=display:flex><span>    cuFileRead(handle, d_comp[slot], comp_bytes, file_off, <span style=color:#a5d6ff>0</span>);
</span></span><span style=display:flex><span>    launch_nvcomp_decompress<span style=color:#ff7b72;font-weight:700>&lt;&lt;&lt;</span>... , stream<span style=color:#ff7b72;font-weight:700>&gt;&gt;&gt;</span>(d_comp[slot], d_decomp[slot]);
</span></span><span style=display:flex><span>    user_kernel<span style=color:#ff7b72;font-weight:700>&lt;&lt;&lt;</span>... , stream<span style=color:#ff7b72;font-weight:700>&gt;&gt;&gt;</span>(d_decomp[slot]);
</span></span><span style=display:flex><span>}
</span></span></code></pre></div><p>Constraint: Ensure decompression kernel consumes only after read completion—use stream ordering or explicit events.</p><hr><h2 id="5-performance-modeling">5. Performance Modeling</h2><h3 id="51-throughput-model">5.1 Throughput Model</h3><p>Let:</p><ul><li>B_nvme = Aggregate NVMe bandwidth (GB/s)</li><li>B_pcie = Sustained PCIe bandwidth to GPU (GB/s)</li><li>B_mem = Effective GPU memory write bandwidth (GB/s) for large transfers</li><li>B_kernel = Data consumption rate of downstream kernels (GB/s)</li></ul><p>Steady-state ingest rate R_ingest ≈ min(B_nvme, B_pcie, B_mem, B_kernel).</p><h3 id="52-queue-depth--outstanding-io">5.2 Queue Depth & Outstanding I/O</h3><p>Throughput saturates as outstanding requests (QD) approaches device parallelism. Latency-sensitive small reads degrade aggregate. Use large aligned reads (≥1 MB) and maintain QD≥N_lanes utilization target.</p><h3 id="53-overlap-efficiency">5.3 Overlap Efficiency</h3><p>Define overlap factor O = (Compute_Time + IO_Time - Makespan) / min(Compute_Time, IO_Time). Aim for O → 1. Diagnose with timeline correlation (Nsight Systems). Underlap indicates serialization or insufficient parallel I/O.</p><h3 id="54-roofline-extension-io--compute">5.4 Roofline Extension (I/O + Compute)</h3><p>Effective performance limited by min(Compute FLOP/s, R_ingest * Operational_Intensity). Increase Operational Intensity by fusing lightweight transformations (e.g., normalization) into decompression or loading kernel.</p><h3 id="55-cpu-offload-savings">5.5 CPU Offload Savings</h3><p>CPU_Copy_Cycles ≈ (Bytes / Mem_BW_host) * cycles_per_byte. GDS removes one host copy: savings scale linearly with dataset size. For multi-GPU nodes, cumulative CPU cycles reclaimed can be reassigned to coordination or pre-processing tasks.</p><hr><h2 id="6-filesystems--object-storage-considerations">6. Filesystems & Object Storage Considerations</h2><h3 id="61-local-posix-ext4xfs">6.1 Local POSIX (ext4/xfs)</h3><ul><li>Align file extents with large sequential reads.</li><li>Disable atime updates (<code>noatime</code>) to cut metadata writes.</li><li>Consider direct I/O (<code>O_DIRECT</code>) to bypass page cache for purely streaming workloads.</li></ul><h3 id="62-parallel-fs-lustre--beegfs--spectrum-scale">6.2 Parallel FS (Lustre / BeeGFS / Spectrum Scale)</h3><table><thead><tr><th>Aspect</th><th>Importance</th><th>Tuning Knob</th><th>Note</th></tr></thead><tbody><tr><td>Stripe Count</td><td>High</td><td>lfs setstripe</td><td>Match stripes to NVMe count</td></tr><tr><td>Stripe Size</td><td>High</td><td>lfs setstripe -S</td><td>Large (≥4MB) for throughput</td></tr><tr><td>Metadata RPCs</td><td>Medium</td><td>MDT config</td><td>Cache directory entries</td></tr><tr><td>Locking</td><td>Medium</td><td>Lock ahead</td><td>Reduce contention for sequential scans</td></tr></tbody></table><h3 id="63-object-storage-s3-like">6.3 Object Storage (S3-like)</h3><ul><li>Latent; parallel range GET requests necessary.</li><li>Use persistent HTTP connections, HTTP/2, or QUIC (where available) to reduce handshake overhead.</li><li>Batch small objects or aggregate into larger shards.</li></ul><h3 id="64-cache-layers">6.4 Cache Layers</h3><p>Local NVMe tier as read cache: promote hot shards; ensure eviction aligns with working set predictions.</p><hr><h2 id="7-compression-encoding--format-choices">7. Compression, Encoding & Format Choices</h2><h3 id="71-columnar-formats-parquet-orc">7.1 Columnar Formats (Parquet, ORC)</h3><p>Pros: Predicate pushdown, selective decoding reduces bytes moved. Con: Nested encodings may fragment reads (seek storms) if columns interleaved physically.</p><h3 id="72-row-major-binary-blocks">7.2 Row-Major Binary Blocks</h3><p>Favorable for GPU kernels expecting AoS→SoA transforms performed once on ingest; simpler prefetch logic.</p><h3 id="73-gpu-friendly-compression">7.3 GPU-Friendly Compression</h3><table><thead><tr><th>Codec</th><th>GPU Decode Availability</th><th>Typical Ratio</th><th>Notes</th></tr></thead><tbody><tr><td>LZ4</td><td>Yes (nvCOMP)</td><td>1.5–2.0×</td><td>Fast, lower ratio</td></tr><tr><td>ZSTD</td><td>Emerging</td><td>2–4×</td><td>Higher CPU fallback cost</td></tr><tr><td>GDeflate</td><td>Yes</td><td>2–3×</td><td>Balanced speed/ratio</td></tr><tr><td>Snappy</td><td>Partial</td><td>1.5–2.0×</td><td>Legacy analytic stacks</td></tr></tbody></table><h3 id="74-trade-off-analysis">7.4 Trade-Off Analysis</h3><p>Net ingest gain when (Compressed_Size / Raw_Size) &lt; (Decode_Time / Copy_Time) threshold. GPU decode amortizes better at large batch sizes due to kernel launch overhead.</p><hr><h2 id="8-telemetry--observability">8. Telemetry & Observability</h2><h3 id="81-metrics-to-capture">8.1 Metrics to Capture</h3><ol><li>NVMe queue depth distribution.</li><li>Average and p95 I/O latency.</li><li>DMA throughput (GB/s) per device.</li><li>GPU memory write throughput vs. theoretical.</li><li>SM occupancy during ingest phases.</li><li>Kernel wait time on data availability (timeline gaps).</li><li>CPU utilization (sys vs. user) for loader threads.</li><li>Dropped or retried I/O operations.</li></ol><h3 id="82-tools">8.2 Tools</h3><table><thead><tr><th>Tool</th><th>Layer</th><th>Use</th></tr></thead><tbody><tr><td>iostat / blktrace</td><td>Block</td><td>Latency & queue depth</td></tr><tr><td>nvidia-smi dmon</td><td>GPU</td><td>PCIe Rx bytes / utilization</td></tr><tr><td>Nsight Systems</td><td>GPU + IO</td><td>Correlate streams & I/O events</td></tr><tr><td>cuFile logs</td><td>GDS</td><td>API timing, errors</td></tr><tr><td>perf / eBPF probes</td><td>Kernel</td><td>Syscall & IRQ attribution</td></tr></tbody></table><h3 id="83-anomaly-diagnosis">8.3 Anomaly Diagnosis</h3><ul><li>Rising I/O latency + flat queue depth → device throttling or thermal limits.</li><li>High CPU sys% + low ingest throughput → excessive context switches or page cache churn (remove buffering layers).</li><li>Low GPU utilization + high PCIe Rx → compute under-saturated; kernel fusion opportunities.</li></ul><hr><h2 id="9-tuning-techniques">9. Tuning Techniques</h2><h3 id="91-alignment--granularity">9.1 Alignment & Granularity</h3><ul><li>Align reads to 4KB (filesystem block) and ideally 128KB (device optimal). Misaligned offsets cause read-modify cycles.</li><li>Batch small logical records into large extent-aligned I/O.</li></ul><h3 id="92-queue-depth-management">9.2 Queue Depth Management</h3><p>Maintain sufficient concurrent <code>cuFileRead</code> requests. Empirically find knee where more parallelism adds latency (tail amplification) without throughput increase.</p><h3 id="93-irq--core-affinity">9.3 IRQ & Core Affinity</h3><p>Pin NVMe and NIC interrupts to isolated cores; separate from CUDA driver management threads. Avoid sharing with application orchestration.</p><h3 id="94-stream--stage-concurrency">9.4 Stream & Stage Concurrency</h3><p>Use multiple CUDA streams: one for read DMA, one for decompression, one for compute. Use events to build a dependency chain rather than host synchronizations.</p><h3 id="95-zero-copy-pitfalls">9.5 Zero-Copy Pitfalls</h3><p>Some control metadata may still transit host; measure with PCIe counters. Validate actual host copy elimination (profilers) instead of assuming.</p><h3 id="96-filesystem-mount-options">9.6 Filesystem Mount Options</h3><p><code>noatime</code>, large journal commit intervals for ext4, disabling barriers only if power-loss risks acceptable (rarely recommended in production training clusters).</p><h3 id="97-numa-considerations">9.7 NUMA Considerations</h3><p>If staging buffers unavoidable (mixed paths), allocate on NUMA node attached to target PCIe root complex. Use <code>numactl --hardware</code> mapping.</p><h3 id="98-adaptive-batch-sizing">9.8 Adaptive Batch Sizing</h3><p>Adjust batch size at runtime based on observed ingest latency: keep pipeline depth so that compute rarely stalls; shrink when latency spikes to relieve memory pressure.</p><hr><h2 id="10-pitfalls--anti-patterns">10. Pitfalls & Anti-Patterns</h2><ol><li><strong>Tiny Random Reads</strong>: Fragment throughput; consolidate or reorder.</li><li><strong>Overzealous Page Cache Bypass</strong>: Some reuse patterns benefit from caching; measure before forcing <code>O_DIRECT</code>.</li><li><strong>Ignoring NUMA for Control Threads</strong>: Results in cross-node waking of IRQ handlers.</li><li><strong>Underutilized NVMe Queues</strong>: Single-threaded submission leaving bandwidth idle.</li><li><strong>Oversized Queue Depth</strong>: Inflates latency tail; hurts responsiveness for mixed workloads.</li><li><strong>Synchronous Decompression</strong>: Blocks potential overlap; move decode to GPU streams.</li><li><strong>Single Monolithic Kernel</strong>: Hides I/O-induced stalls; break into stages with explicit dependencies.</li><li><strong>Assuming Compression Always Helps</strong>: High-entropy data wastes decode cycles.</li><li><strong>Neglecting Thermal Throttling</strong>: SSD temperature >70°C reduces performance; ensure airflow.</li><li><strong>Opaque Error Handling</strong>: Silent short reads or partial DMA failures propagate corrupt tensors.</li></ol><hr><h2 id="11-case-study-synthetic-benchmark">11. Case Study (Synthetic Benchmark)</h2><h3 id="setup">Setup</h3><ul><li>4× NVMe Gen5 drives (striped) delivering ~50 GB/s aggregate theoretical.</li><li>4× GPUs (H100), PCIe Gen5, each with ~2.0 TB/s HBM peak.</li><li>Dataset: 4 TB of compressed binary blocks (average 2.2× compression via GDeflate).</li><li>Workload: Read → decompress → normalize → feed into compute kernel simulating ML preprocessing.</li></ul><h3 id="scenarios">Scenarios</h3><table><thead><tr><th>Scenario</th><th>Path</th><th>Avg Ingest (GB/s)</th><th>GPU Utilization</th><th>CPU Core Usage</th><th>Notes</th></tr></thead><tbody><tr><td>A</td><td>CPU read + memcpy</td><td>18</td><td>55%</td><td>24 cores busy</td><td>Double copy bound</td></tr><tr><td>B</td><td>CPU read + GPU decompress</td><td>21</td><td>61%</td><td>20 cores</td><td>CPU still copy bottleneck</td></tr><tr><td>C</td><td>GDS direct + GPU decompress</td><td>34</td><td>82%</td><td>8 cores</td><td>Host copy removed</td></tr><tr><td>D</td><td>GDS + NVLink relay</td><td>36</td><td>85%</td><td>8 cores</td><td>P2P distribution overlapped</td></tr><tr><td>E</td><td>GDS + dynamic batch</td><td>38</td><td>88%</td><td>8 cores</td><td>Adaptive pipeline tuning</td></tr></tbody></table><h3 id="observations">Observations</h3><ul><li>Transition A→C shows ~1.9× ingest improvement; GPU utilization correlates with ingest.</li><li>Relay plus dynamic batch provided marginal but real gains; diminishing returns approaching NVMe ceiling.</li><li>CPU core usage drops freeing resources for auxiliary services (logging, scheduling).</li></ul><hr><h2 id="12-future-directions-2025">12. Future Directions (2025+)</h2><ol><li><strong>CXL Memory Pooling</strong>: Direct GPU reads from pooled CXL-attached memory could collapse staging layers.</li><li><strong>In-NIC Decompression</strong>: Offloading lightweight codecs to programmable DPUs to free GPU cycles.</li><li><strong>Intelligent Prefetch Graphs</strong>: ML-driven prediction of next dataset shard to pre-stage.</li><li><strong>End-to-End QoS</strong>: Coordinated throttling across storage, PCIe switches, and GPU to maintain latency SLOs.</li><li><strong>Standardized Telemetry Schema</strong>: Cross-vendor metrics for ingest phases enabling portable auto-tuners.</li><li><strong>Security Hardening</strong>: Direct DMA paths expand attack surface; IOMMU + attestation enforcement.</li><li><strong>Data Path Virtualization</strong>: Multi-tenant isolation of GDS resources without large overhead.</li></ol><hr><h2 id="13-tuning-checklist">13. Tuning Checklist</h2><table><thead><tr><th>Category</th><th>Question</th><th>Tool / Metric</th><th>Action</th></tr></thead><tbody><tr><td>Bandwidth</td><td>Are NVMe lanes saturated?</td><td>iostat, perf PCIe counters</td><td>Increase queue depth, stripe more drives</td></tr><tr><td>Alignment</td><td>Are reads aligned?</td><td>blktrace, strace</td><td>Pad / repackage shards</td></tr><tr><td>GPU Wait</td><td>Kernels waiting for data?</td><td>Nsight timeline gaps</td><td>Increase parallel reads / batch size</td></tr><tr><td>Decompression</td><td>GPU underutilized during decode?</td><td>SM occupancy</td><td>Fuse decode + normalize</td></tr><tr><td>CPU Utilization</td><td>High sys%?</td><td>top, perf</td><td>Reduce copies, enable GDS</td></tr><tr><td>Queue Depth</td><td>Flat throughput before peak?</td><td>iostat, custom logs</td><td>Tune outstanding I/O count</td></tr><tr><td>Thermal</td><td>SSD temps high?</td><td>smartctl</td><td>Improve cooling / spacing</td></tr><tr><td>Compression Ratio</td><td>Low ratio (&lt;1.2×)?</td><td>ingest logs</td><td>Disable compression for that shard class</td></tr><tr><td>NUMA Locality</td><td>Remote memory accesses?</td><td>numastat</td><td>Rebind threads, allocate locally</td></tr><tr><td>Error Handling</td><td>Silent short reads?</td><td>cuFile logs</td><td>Add verification / retries</td></tr></tbody></table><hr><h2 id="14-references">14. References</h2><ol><li>NVIDIA GPUDirect Storage Documentation (2025).</li><li>NVMe 2.0 Base Specification.</li><li>PCI Express Base Specification 6.0.</li><li>NVIDIA Nsight Systems User Guide.</li><li>nvCOMP Compression Library Docs.</li><li>BeeGFS & Lustre tuning guides (2024–2025).</li><li>CXL Consortium: CXL 3.0 Spec Overview.</li><li>Research: <em>In-storage compute for data reduction</em> (various 2023–2025 papers).</li><li>Linux Block Layer & IO_uring design docs.</li><li>SmartNIC/DPU architecture whitepapers (NVIDIA BlueField, AMD Pensando).</li></ol><hr><h2 id="15-summary">15. Summary</h2><p>GPUDirect Storage shifts the bottleneck conversation from host memory copies to genuine device-level throughput constraints. Engineering high-throughput ingest requires coordinated tuning: filesystem striping, queue depth, alignment, compression strategy, stream concurrency, and telemetry-driven adaptation. As CXL, in-NIC processing, and standardized metrics mature, the ideal path trends toward fully pipelined, low-copy, and self-optimizing ingestion layers feeding ever-faster GPU compute pipelines.</p></div><footer class="ce1a612 c6dfb1e c3ecea6"><div class="c364589">Categories:
<a href=/categories/distributed%20systems/>distributed systems</a>, <a href=/categories/performance/>performance</a></div><div>Tags:
<a href=/tags/gpu/>#gpu</a>, <a href=/tags/storage/>#storage</a>, <a href=/tags/rdma/>#rdma</a>, <a href=/tags/data-path/>#data-path</a>, <a href=/tags/hpc/>#hpc</a></div></footer></article></main><footer class="ccdf0e8" role=contentinfo aria-label=Footer><div class="cfdda01 c133889 c5df473 c0eecc8 c69618a c6942b3 c03620d c2a9f27 c7c11d8 c82c52d c14527b"><div class="c6dfb1e c3ecea6 c39ef11 c88ae6f">&copy; 2025 Leonardo Benicio. All rights
reserved.</div><div class="c6942b3 c7c11d8 cd1fd22"><a href=https://github.com/lbenicio target=_blank rel="noopener noreferrer" aria-label=GitHub class="c1d6c20 c7c11d8 c1d0018 cd1fd22 cb5c327 c10dda9 c6dfb1e cbbda39 cfc01c7 c01f421 c286dd7 c2bd687 cfdce1d cfef18f c000b66 cf55a7b c514027"><svg width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M15 22v-4a4.8 4.8.0 00-1-3.5c3 0 6-2 6-5.5.08-1.25-.27-2.48-1-3.5.28-1.15.28-2.35.0-3.5.0.0-1 0-3 1.5-2.64-.5-5.36-.5-8 0C6 2 5 2 5 2c-.3 1.15-.3 2.35.0 3.5A5.403 5.403.0 004 9c0 3.5 3 5.5 6 5.5-.39.5-.67 1.08-.82 1.7s-.2 1.27-.18 1.9V22"/></svg>
<span class="cba5854">GitHub</span>
</a><a href=https://www.linkedin.com/in/leonardo-benicio target=_blank rel="noopener noreferrer" aria-label=LinkedIn class="c1d6c20 c7c11d8 c1d0018 cd1fd22 cb5c327 c10dda9 c6dfb1e cbbda39 cfc01c7 c01f421 c286dd7 c2bd687 cfdce1d cfef18f c000b66 cf55a7b c514027"><svg width="20" height="20" viewBox="0 0 24 24" fill="currentColor"><path d="M20.447 20.452H17.21V14.86c0-1.333-.027-3.046-1.858-3.046-1.86.0-2.145 1.45-2.145 2.948v5.69H9.069V9h3.112v1.561h.044c.434-.82 1.494-1.686 3.074-1.686 3.29.0 3.897 2.165 3.897 4.983v6.594zM5.337 7.433a1.805 1.805.0 11-.002-3.61 1.805 1.805.0 01.002 3.61zM6.763 20.452H3.911V9h2.852v11.452z"/></svg>
<span class="cba5854">LinkedIn</span>
</a><a href=https://twitter.com/lbenicio_ target=_blank rel="noopener noreferrer" aria-label=Twitter class="c1d6c20 c7c11d8 c1d0018 cd1fd22 cb5c327 c10dda9 c6dfb1e cbbda39 cfc01c7 c01f421 c286dd7 c2bd687 cfdce1d cfef18f c000b66 cf55a7b c514027"><svg width="20" height="20" viewBox="0 0 24 24" fill="currentColor"><path d="M19.633 7.997c.013.177.013.354.013.53.0 5.386-4.099 11.599-11.6 11.599-2.31.0-4.457-.676-6.265-1.842.324.038.636.05.972.05 1.91.0 3.67-.65 5.07-1.755a4.099 4.099.0 01-3.827-2.84c.25.039.5.064.763.064.363.0.726-.051 1.065-.139A4.091 4.091.0 012.542 9.649v-.051c.538.3 1.162.482 1.824.507A4.082 4.082.0 012.54 6.7c0-.751.2-1.435.551-2.034a11.63 11.63.0 008.44 4.281 4.615 4.615.0 01-.101-.938 4.091 4.091.0 017.078-2.799 8.1 8.1.0 002.595-.988 4.112 4.112.0 01-1.8 2.261 8.2 8.2.0 002.357-.638A8.824 8.824.0 0119.613 7.96z"/></svg>
<span class="cba5854">Twitter</span></a></div></div></footer></body></html>