<!doctype html><html lang=en><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,maximum-scale=1,minimum-scale=1,user-scalable=no"><title>From MapReduce to Spark: The Arc of Data-Parallel Systems · Leonardo Benicio</title><meta name=description content="MapReduce taught fault-tolerant batch at scale; Spark generalized it with resilient distributed datasets (RDDs) and DAG scheduling."><link rel=alternate type=application/rss+xml title=RSS href=https://lbenicio.dev/index.xml><link rel=canonical href=https://blog.lbenicio.dev/blog/from-mapreduce-to-spark-the-arc-of-data-parallel-systems/><link rel=preload href=/static/fonts/OpenSans-Regular.ttf as=font type=font/ttf crossorigin><link rel="stylesheet" href="/assets/css/fonts.min.40e2054b739ac45a0f9c940f4b44ec00c3b372356ebf61440a413c0337c5512e.css" crossorigin="anonymous" integrity="sha256-QOIFS3OaxFoPnJQPS0TsAMOzcjVuv2FECkE8AzfFUS4="><link rel="shortcut icon" href=/static/assets/favicon/favicon.ico><link rel=icon type=image/x-icon href=/static/assets/favicon/favicon.ico><link rel=icon href=/static/assets/favicon/favicon.svg type=image/svg+xml><link rel=icon href=/static/assets/favicon/favicon-32x32.png sizes=32x32 type=image/png><link rel=icon href=/static/assets/favicon/favicon-16x16.png sizes=16x16 type=image/png><link rel=apple-touch-icon href=/static/assets/favicon/apple-touch-icon.png><link rel=manifest href=/static/assets/favicon/site.webmanifest><link rel=mask-icon href=/static/assets/favicon/safari-pinned-tab.svg color=#209cee><meta name=msapplication-TileColor content="#209cee"><meta name=msapplication-config content="/static/assets/favicon/browserconfig.xml"><meta name=theme-color content="#d2e9f8"><meta property="og:title" content="From MapReduce to Spark: The Arc of Data-Parallel Systems · Leonardo Benicio"><meta property="og:description" content="MapReduce taught fault-tolerant batch at scale; Spark generalized it with resilient distributed datasets (RDDs) and DAG scheduling."><meta property="og:url" content="https://blog.lbenicio.dev/blog/from-mapreduce-to-spark-the-arc-of-data-parallel-systems/"><meta property="og:type" content="article"><meta property="og:image" content="https://blog.lbenicio.dev/static/assets/images/blog/mapreduce-spark.png"><meta name=twitter:card content="summary_large_image"><meta name=twitter:title content="From MapReduce to Spark: The Arc of Data-Parallel Systems · Leonardo Benicio"><meta name=twitter:description content="MapReduce taught fault-tolerant batch at scale; Spark generalized it with resilient distributed datasets (RDDs) and DAG scheduling."><meta name=twitter:site content="@lbenicio_"><script type=application/ld+json>"{\"@context\":\"https://schema.org\",\"@type\":\"WebSite\",\"name\":\"About Leonardo Benicio\",\"url\":\"https://blog.lbenicio.dev\"}"</script><script type=application/ld+json>"{\"@context\":\"https://schema.org\",\"@type\":\"Person\",\"name\":\"Leonardo Benicio\",\"sameAs\":[\"https://github.com/lbenicio\",\"https://www.linkedin.com/in/leonardo-benicio\",\"https://twitter.com/lbenicio_\"],\"url\":\"https://blog.lbenicio.dev\"}"</script><script type=application/ld+json>"{\"@context\":\"https://schema.org\",\"@type\":\"BreadcrumbList\",\"itemListElement\":[{\"@type\":\"ListItem\",\"item\":\"https://blog.lbenicio.dev/\",\"name\":\"Home\",\"position\":1},{\"@type\":\"ListItem\",\"item\":\"https://blog.lbenicio.dev/\",\"name\":\"Blog\",\"position\":2},{\"@type\":\"ListItem\",\"item\":\"https://blog.lbenicio.dev/blog/from-mapreduce-to-spark-the-arc-of-data-parallel-systems/\",\"name\":\"From Mapreduce to Spark the Arc of Data Parallel Systems\",\"position\":3}]}"</script><link rel="stylesheet" href="/assets/css/main.min.23cb77fd3186d94b425cf879bfff3195d7648b23b860d880dbb47fe2e115b884.css" crossorigin="anonymous" integrity="sha256-owHVkwE1+9dguAma85DLJbKG8+7vYa137CVrUeaaaxk="></head><body class="c6942b3 c03620d cf3bd2e"><script>(function(){try{document.addEventListener("gesturestart",function(e){e.preventDefault()}),document.addEventListener("touchstart",function(e){e.touches&&e.touches.length>1&&e.preventDefault()},{passive:!1});var e=0;document.addEventListener("touchend",function(t){var n=Date.now();n-e<=300&&t.preventDefault(),e=n},{passive:!1})}catch{}})()</script><a href=#content class="cba5854 c21e770 caffa6e cc5f604 cf2c31d cdd44dd c10dda9 c43876e c787e9b cddc2d2 cf55a7b c6dfb1e c9391e2">Skip to content</a>
<script>(function(){try{const e=localStorage.getItem("theme");e==="dark"&&document.documentElement.classList.add("dark");const t=document.querySelector('button[aria-label="Toggle theme"]');t&&t.setAttribute("aria-pressed",String(e==="dark"))}catch{}})();function toggleTheme(e){const s=document.documentElement,t=s.classList.toggle("dark");try{localStorage.setItem("theme",t?"dark":"light")}catch{}try{var n=e&&e.nodeType===1?e:document.querySelector('button[aria-label="Toggle theme"]');n&&n.setAttribute("aria-pressed",String(!!t))}catch{}}</script><header class="cd019ba c98dfae cdd44dd cfdda01 c9ee25d ce2dc7a cd72dd7 cc0dc37" role=banner><div class="cfdda01 c6942b3 ccf47f4 c7c11d8"><a href=/ class="c87e2b0 c6942b3 c7c11d8 c1838fa cb594e4" aria-label=Home><img src=/static/assets/favicon/favicon.svg alt=Logo width=32 height=32 class="c3de71a c4d5191">
<span class="cf8f011 c4d1253 cbd72bc cd7e69e">Leonardo Benicio</span></a><div class="c6942b3 c85cbd4 c7c11d8 ca798da c1838fa c7a0580"><nav class="cc1689c cd9b445 c75065d c04bab1" aria-label=Main><a href=/ class="c4d1253 c9e4539 cbbda39 c01f421 c19ee42 c3ecea6">Home</a>
<a href=https://lbenicio.dev/about target=_blank rel="noopener noreferrer" class="c4d1253 c9e4539 cbbda39 c01f421 c19ee42 c3ecea6">About</a>
<a href=https://lbenicio.dev/timeline target=_blank rel="noopener noreferrer" class="c4d1253 c9e4539 cbbda39 c01f421 c19ee42 c3ecea6">Timeline</a>
<a href=https://lbenicio.dev/reading target=_blank rel="noopener noreferrer" class="c4d1253 c9e4539 cbbda39 c01f421 c19ee42 c3ecea6">Reading</a>
<a href=https://lbenicio.dev/publications target=_blank rel="noopener noreferrer" class="c4d1253 c9e4539 cbbda39 c01f421 c19ee42 c3ecea6">Publications</a>
<a href=https://lbenicio.dev/contact target=_blank rel="noopener noreferrer" class="c4d1253 c9e4539 cbbda39 c01f421 c19ee42 c3ecea6">Contact</a></nav><button id="i1d73d4" type=button class="c1d6c20 c81ac7c c6a899b c7c11d8 c1d0018 c10dda9 c8e184d c514027 c88daee c7a66a6 c097fa1 cfc01c7 c286dd7 c2bd687 cfdce1d cfef18f" onclick=toggleTheme(this) aria-label="Toggle theme" aria-pressed=false title="Toggle theme">
<svg class="cb26e41 c50ceea cb69a5c c4f45c8 c8c2c40" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" aria-hidden="true"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg>
<svg class="cb26e41 c8fca2b cb69a5c c4f45c8 cc1689c c9c27ff" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" aria-hidden="true"><circle cx="12" cy="12" r="4"/><path d="M12 2v4"/><path d="M12 18v4"/><path d="M2 12h4"/><path d="M18 12h4"/><path d="M4.93 4.93l2.83 2.83"/><path d="M16.24 16.24l2.83 2.83"/><path d="M6.34 17.66l2.83-2.83"/><path d="M14.83 9.17l2.83-2.83"/></svg>
<span class="cba5854">Toggle theme</span></button><div class="c658bcf c097fa1"><details class="ccd45bf"><summary class="cc7a258 c1d6c20 c7c11d8 c1d0018 c10dda9 c000b66 cf55a7b"><svg class="c20e4eb cb58471" fill="none" viewBox="0 0 24 24" stroke="currentColor"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M4 6h16M4 12h16M4 18h16"/></svg>
<span class="cba5854">Open menu</span></summary><div class="ce49c1e c437fa9 c1b4412 c8c0110 c887979 c43876e c10dda9 c60a4cc c401fa1 cb2c551 cf514a5 cadfe0b ce3dbb2 c72ad85 cbd4710 c6988b4"><a href=/ class="c62aaf0 c364589 c6942b3 c7c11d8 c1838fa" aria-label=Home><img src=/static/assets/favicon/favicon.svg alt=Logo width=24 height=24 class="c20e4eb cb58471">
<span class="cf8f011 c7c1b66 cbd72bc cbac0b8">Leonardo Benicio</span></a><nav class="c6942b3 c03620d cd69733"><a href=/ class="c4d1253 cbbda39 c3ecea6 c19ee42">Home</a>
<a href=https://lbenicio.dev/about target=_blank rel="noopener noreferrer" class="c4d1253 cbbda39 c3ecea6 c19ee42">About</a>
<a href=https://lbenicio.dev/timeline target=_blank rel="noopener noreferrer" class="c4d1253 cbbda39 c3ecea6 c19ee42">Timeline</a>
<a href=https://lbenicio.dev/reading target=_blank rel="noopener noreferrer" class="c4d1253 cbbda39 c3ecea6 c19ee42">Reading</a>
<a href=https://lbenicio.dev/publications target=_blank rel="noopener noreferrer" class="c4d1253 cbbda39 c3ecea6 c19ee42">Publications</a>
<a href=https://lbenicio.dev/contact target=_blank rel="noopener noreferrer" class="c4d1253 cbbda39 c3ecea6 c19ee42">Contact</a></nav></div></details></div></div></div></header><div class="caffa6e c437fa9 ce9aced c97bba6 c15da2a c975cba" role=complementary aria-label="GitHub repository"><div class="c9d056d c252f85 ca22532 ca88a1a c876315"><div class="c6942b3 c7c11d8 c1d0018 cd1fd22 c6066e4 c43876e ce3d5b6 caa20d2 c3ecea6 c0cd2e2 cddc2d2 c3ed5c9 cd4074c c876315"><a href=https://github.com/lbenicio/aboutme target=_blank rel="noopener noreferrer" class="c6942b3 c7c11d8 cd1fd22 c71bae8 cfac1ac c19ee42 c25dc7c cb40739 cbbda39 cf55a7b" aria-label="View source on GitHub"><svg width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="cb26e41 c41bcd4 cf17690 cfa4e34 c78d562" aria-hidden="true"><path d="M15 22v-4a4.8 4.8.0 00-1-3.5c3 0 6-2 6-5.5.08-1.25-.27-2.48-1-3.5.28-1.15.28-2.35.0-3.5.0.0-1 0-3 1.5-2.64-.5-5.36-.5-8 0C6 2 5 2 5 2c-.3 1.15-.3 2.35.0 3.5A5.403 5.403.0 004 9c0 3.5 3 5.5 6 5.5-.39.49-.68 1.05-.85 1.65-.17.6-.22 1.23-.15 1.85v4"/><path d="M9 18c-4.51 2-5-2-7-2"/></svg>
<span class="cb5c327 cd7e69e">Fork me</span></a></div></div></div><main id="i7eccc0" class="cfdda01 c5df473 c0eecc8 c85cbd4" role=main aria-label=Content><nav class="cb545ce c8d8ae4 c277478" aria-label=Breadcrumb><ol class="c6942b3 c3adaf2 c7c11d8 cd365ee c3ecea6"><li><a href=/ class="c19ee42 c71bae8 cfac1ac">Home</a></li><li class="c6942b3 c7c11d8 cd365ee"><span class="cb82ddd">/</span><a href=/ class="c19ee42 c71bae8 cfac1ac">Blog</a></li><li class="c6942b3 c7c11d8 cd365ee"><span class="cb82ddd">/</span><span class="c88daee">From Mapreduce to Spark the Arc of Data Parallel Systems</span></li></ol></nav><article class="c461ba0 c1c203f cfb6084 c995404 c6ca165"><nav class="cb545ce c8d8ae4 c277478" aria-label=Breadcrumb><ol class="c6942b3 c3adaf2 c7c11d8 cd365ee c3ecea6"><li><a href=/ class="c19ee42 c71bae8 cfac1ac">Home</a></li><li class="c6942b3 c7c11d8 cd365ee"><span class="cb82ddd">/</span><a href=/ class="c19ee42 c71bae8 cfac1ac">Blog</a></li><li class="c6942b3 c7c11d8 cd365ee"><span class="cb82ddd">/</span><span class="c88daee">From Mapreduce to Spark the Arc of Data Parallel Systems</span></li></ol></nav><header class="c8aedc7"><h1 class="cf304bc c6fb0fe cf8f011 cc484e1">From MapReduce to Spark: The Arc of Data-Parallel Systems</h1><div class="c277478 c3ecea6 c8fb24a">2025-05-19
· Leonardo Benicio</div><div class="c1a1a3f c8124f2"><img src=/static/assets/images/blog/mapreduce-spark.png alt class="cfdda01 c524300 c677556"></div><p class="lead c3ecea6">MapReduce taught fault-tolerant batch at scale; Spark generalized it with resilient distributed datasets (RDDs) and DAG scheduling.</p></header><div class="content"><p>MapReduce popularized large-scale batch processing with a simple model (map, shuffle, reduce) and immutable intermediate state on HDFS. It optimized for throughput and fault tolerance via re-execution.</p><p>Spark expanded the model:</p><ul><li>RDDs: immutable, partitioned datasets with lineage, enabling recomputation on failure.</li><li>DAG scheduler: plans multi-stage jobs, pipelining narrow transformations and materializing wide ones.</li><li>In-memory caching: keeps hot datasets in RAM to accelerate iterative workloads.</li><li>Higher-level APIs: DataFrames/Datasets and SQL, plus MLlib and Structured Streaming.</li></ul><h3 id="checkpointing-and-lineage">Checkpointing and lineage</h3><p>RDD lineage can grow large; Spark checkpoints to cut recomputation cost. For streaming, write-ahead logs plus checkpoints enable recovery.</p><h3 id="skew-and-shuffle">Skew and shuffle</h3><p>Stragglers often come from data skew. Remedies: salting keys, custom partitioners, or adaptive query execution (AQE) which can coalesce partitions and optimize joins at runtime.</p><h3 id="code-sketch-word-count-with-dataframes">Code sketch: word count with DataFrames</h3><div class="highlight"><pre tabindex=0 style=color:#e6edf3;background-color:#0d1117;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class="language-python" data-lang=python><span style=display:flex><span><span style=color:#ff7b72>from</span> <span style=color:#ff7b72>pyspark.sql</span> <span style=color:#ff7b72>import</span> SparkSession
</span></span><span style=display:flex><span>spark <span style=color:#ff7b72;font-weight:700>=</span> SparkSession<span style=color:#ff7b72;font-weight:700>.</span>builder<span style=color:#ff7b72;font-weight:700>.</span>getOrCreate()
</span></span><span style=display:flex><span>text <span style=color:#ff7b72;font-weight:700>=</span> spark<span style=color:#ff7b72;font-weight:700>.</span>read<span style=color:#ff7b72;font-weight:700>.</span>text(<span style=color:#a5d6ff>&#34;s3://bucket/corpus/&#34;</span>)
</span></span><span style=display:flex><span>words <span style=color:#ff7b72;font-weight:700>=</span> text<span style=color:#ff7b72;font-weight:700>.</span>selectExpr(<span style=color:#a5d6ff>&#34;explode(split(lower(value), &#39;</span><span style=color:#79c0ff>\\</span><span style=color:#a5d6ff>W+&#39;)) as word&#34;</span>)
</span></span><span style=display:flex><span>counts <span style=color:#ff7b72;font-weight:700>=</span> words<span style=color:#ff7b72;font-weight:700>.</span>groupBy(<span style=color:#a5d6ff>&#34;word&#34;</span>)<span style=color:#ff7b72;font-weight:700>.</span>count()<span style=color:#ff7b72;font-weight:700>.</span>orderBy(<span style=color:#a5d6ff>&#34;count&#34;</span>, ascending<span style=color:#ff7b72;font-weight:700>=</span><span style=color:#79c0ff>False</span>)
</span></span><span style=display:flex><span>counts<span style=color:#ff7b72;font-weight:700>.</span>write<span style=color:#ff7b72;font-weight:700>.</span>mode(<span style=color:#a5d6ff>&#34;overwrite&#34;</span>)<span style=color:#ff7b72;font-weight:700>.</span>parquet(<span style=color:#a5d6ff>&#34;s3://bucket/out/&#34;</span>)
</span></span></code></pre></div><p>The evolution continues: adaptive engines (AQE), vectorized execution, and lakehouse formats (Parquet, Delta) make modern data-parallel systems far more expressive than plain MapReduce.</p><hr><h2 id="1-mapreduce-strengths-and-structural-limits">1. MapReduce: Strengths and Structural Limits</h2><h3 id="strengths">Strengths</h3><ol><li><strong>Deterministic fault tolerance</strong>: Map outputs materialized to disk (local or HDFS) and can be re-fetched by failed reducers.</li><li><strong>Simplicity</strong>: Two-phase API (map → shuffle → reduce) with a clear execution timeline.</li><li><strong>Data locality exploitation</strong>: Schedulers place map tasks where HDFS blocks reside.</li></ol><h3 id="limitations">Limitations</h3><ol><li><strong>Rigid two-stage boundary</strong>: Complex pipelines become chains of MapReduce jobs, each incurring full materialization.</li><li><strong>Inefficient iterative workloads</strong> (ML, graph): Re-reading data from disk each iteration.</li><li><strong>Limited optimization surface</strong>: Without a global DAG, cross-job optimization is impossible.</li></ol><p>Result: High reliability & scalability, but at cost of latency and iterative performance.</p><h2 id="2-rdd-abstraction--lineage">2. RDD Abstraction & Lineage</h2><p>An RDD is a <em>logical</em> dataset split into partitions; each partition knows how to recompute itself from parent partitions via a lineage graph of transformations (map, filter, union, join&mldr;). Fault tolerance arises from replaying only lost partitions instead of checkpointing every intermediate result.</p><h3 id="narrow-vs-wide-dependencies">Narrow vs. Wide Dependencies</h3><table><thead><tr><th>Dependency Type</th><th>Parent → Child Mapping</th><th>Shuffle Required</th><th>Example</th></tr></thead><tbody><tr><td>Narrow</td><td>Each child partition reads a subset of specific parent partitions</td><td>No</td><td>map, filter, mapPartitions</td></tr><tr><td>Wide</td><td>Child partition depends on many parent partitions</td><td>Yes</td><td>groupByKey, reduceByKey, join</td></tr></tbody></table><p>The DAG scheduler groups narrow dependencies into <em>stages</em>, inserting shuffle boundaries for wide dependencies. Pipelines inside a stage avoid unnecessary materializations.</p><h3 id="lineage-truncation">Lineage Truncation</h3><p>Long lineage chains increase recomputation cost after failure. Spark supports <em>checkpointing</em> (persist to reliable storage) to truncate lineage for very deep or iterative graphs (e.g., PageRank after N iterations). Trade-off: extra I/O cost vs. faster recovery / bounded recompute time.</p><h2 id="3-caching--persistence">3. Caching & Persistence</h2><p>RDD/DataFrame caching strategies:</p><ol><li><strong>MEMORY_ONLY</strong>: Fast but may evict partitions (recompute cost on eviction).</li><li><strong>MEMORY_AND_DISK</strong>: Spills non-fit partitions to disk (prevents recompute storms).</li><li><strong>OFF_HEAP / Tachyon-era</strong>: (Historical) external memory layers for sharing across applications.</li><li><strong>Serialized vs. deserialized</strong>: Serialized saves memory, deserialized accelerates CPU-bound loops.</li></ol><p>Guideline: Cache only if reused; measure the <em>reuse ratio</em>. Over-caching increases GC pressure, hurting performance.</p><h2 id="4-from-rdd-to-dataframes--catalyst-optimizer">4. From RDD to DataFrames & Catalyst Optimizer</h2><h3 id="motivation">Motivation</h3><p>RDD API is functionally rich but opaque to the optimizer (user functions are black boxes). Catalyst introduces a logical plan algebra enabling rule-based and cost-based transforms.</p><h3 id="catalyst-phases-conceptual">Catalyst Phases (Conceptual)</h3><ol><li><strong>Parsing</strong>: Convert SQL / DSL to unresolved logical plan.</li><li><strong>Analysis</strong>: Resolve attributes using catalog (tables, columns, types).</li><li><strong>Logical Optimization</strong>: Apply rules (predicate pushdown, constant folding, projection pruning, null propagation).</li><li><strong>Physical Planning</strong>: Enumerate candidates (broadcast hash join, sort-merge join, shuffle hash join) with cost estimation.</li><li><strong>Code Generation</strong>: Whole-stage codegen merges operators into single Java functions, reducing virtual function / iterator overhead.</li></ol><h3 id="example-transformation">Example Transformation</h3><p>SQL: <code>SELECT user, SUM(bytes) FROM logs WHERE day='2025-09-12' GROUP BY user ORDER BY SUM(bytes) DESC LIMIT 10</code></p><ol><li>Filter pushdown partitions only day=&lsquo;2025-09-12&rsquo;.</li><li>Projection prunes unused columns.</li><li>Aggregation planned as hash aggregate (if fits) or sort aggregate.</li><li>ORDER BY + LIMIT may trigger partial top-K then global merge.</li></ol><h2 id="5-tungsten--whole-stage-code-generation">5. Tungsten & Whole-Stage Code Generation</h2><p>Tungsten project delivered memory & CPU efficiency improvements:</p><ol><li><strong>Off-heap binary row format</strong>: Minimizes Java object overhead & GC.</li><li><strong>Cache-conscious layout</strong>: Sequential memory access improves CPU cache utilization.</li><li><strong>Whole-stage codegen</strong>: Fuses operators (filter → project → aggregate) into tight loops; reduces virtual calls & improves branch prediction.</li><li><strong>Vectorized readers</strong>: Batch decode of Parquet/ORC into columnar batches lowers per-tuple overhead; SIMD-friendly.</li></ol><p>Performance benefit: Significant reduction in CPU time for analytic queries, making Spark competitive with MPP databases for many workloads.</p><h2 id="6-shuffle-evolution">6. Shuffle Evolution</h2><p>Early Spark shuffle wrote map outputs as many small files per reducer—scaling poorly. External shuffle service & consolidated files improved scalability. AQE adds <em>dynamic partition coalescing</em> and <em>skew join handling</em> at runtime:</p><ol><li>Detect skewed reduce partitions (data size above threshold).</li><li>Split skewed partition & replicate the smaller side of join for better balance.</li><li>Coalesce many tiny post-shuffle partitions to reduce scheduling overhead.</li></ol><p>Result: Lower straggler tail latency and improved cluster utilization.</p><h2 id="7-adaptive-query-execution-aqe">7. Adaptive Query Execution (AQE)</h2><p>AQE defers some physical plan decisions until runtime statistics (shuffle file sizes, row counts) are known. Adjustments:</p><ol><li>Dynamic join strategy selection (switch to broadcast on small dimension table discovered at runtime).</li><li>Skew partition splitting (as above).</li><li>Coalesce shuffle partitions (reduce scheduler/coordination overhead).</li></ol><p>AQE is particularly impactful for SQL workloads with data skew or unpredictable filters.</p><h2 id="8-structured-streaming-internals">8. Structured Streaming Internals</h2><p>Structured Streaming treats a streaming query as an incremental execution of a <em>static</em> logical plan plus stateful updates. Two primary modes:</p><ol><li><strong>Micro-batch</strong>: Triggers every N ms; each batch is a mini DataFrame job. Provides natural batch semantics (checkpoint per batch).</li><li><strong>Continuous (experimental/limited)</strong>: Low-latency processing with continuous operator execution.</li></ol><h3 id="state-store">State Store</h3><p>Holds aggregates / joins keyed by grouping keys. Backed by local RocksDB or in-memory hash maps; supports checkpointed commit logs. Watermarks prune old state (event-time based) reclaiming memory.</p><h3 id="exactly-once-sink-semantics">Exactly-Once Sink Semantics</h3><p>Achieved via <em>idempotent sink writing</em> (e.g., file sink with atomic commits per batch) or transactional logs (Delta). Offsets + batch IDs recorded in checkpoint dir, ensuring retry safety.</p><h2 id="9-lakehouse-integration-delta--iceberg--hudi">9. Lakehouse Integration (Delta / Iceberg / Hudi)</h2><p>Modern “lakehouse” formats add ACID transactions, schema evolution, and time travel to object stores:</p><ol><li><strong>Delta Lake</strong>: Transaction log JSON + Parquet data files; checkpoint compaction of log for fast listing.</li><li><strong>Iceberg</strong>: Manifest & snapshot metadata tree; hidden partitioning & equality deletes.</li><li><strong>Hudi</strong>: Copy-on-write & merge-on-read tables; delta commit timeline; indexing for upserts.</li></ol><p>Spark leverages these to unify batch & streaming: Structured Streaming writes incremental Parquet & atomic metadata updates, enabling exactly-once ingestion semantics.</p><h2 id="10-performance-tuning-playbook">10. Performance Tuning Playbook</h2><table><thead><tr><th>Area</th><th>Symptom</th><th>Diagnostic</th><th>Action</th></tr></thead><tbody><tr><td>Shuffle</td><td>Long tail tasks</td><td>Spark UI stage detail (bytes/task skew)</td><td>Salting keys, AQE skew split</td></tr><tr><td>Join Strategy</td><td>Memory pressure / spills</td><td>Task metrics: spill bytes</td><td>Broadcast small side, adjust autoBroadcastJoinThreshold</td></tr><tr><td>GC Overhead</td><td>High executor time in GC</td><td>GC logs, Spark UI</td><td>Increase executor memory, tune memoryFraction, off-heap</td></tr><tr><td>Serialization</td><td>High CPU in serialization</td><td>Profiler / flame graph</td><td>Use Kryo, custom serializers, avoid nested small objects</td></tr><tr><td>Caching</td><td>Recompute of reused DF</td><td>UI shows repeated jobs</td><td><code>persist()</code> appropriate storage level</td></tr><tr><td>File Listing</td><td>Slow job start</td><td>Driver thread dumps</td><td>Enable metadata cache, use partition pruning</td></tr><tr><td>Small Files</td><td>Many tiny output files</td><td>Object store listing time</td><td>Coalesce/repartition before write, optimize table</td></tr><tr><td>Skewed Aggregation</td><td>Single hot reducer</td><td>Stage bytes skew metric</td><td>Pre-aggregate, map-side combine, partial aggregation</td></tr></tbody></table><h2 id="11-code-generation--udf-considerations">11. Code Generation & UDF Considerations</h2><p>User Defined Functions (UDFs) can <em>block optimization</em> because Catalyst treats them as black boxes (except for simple Python/Pandas UDF vectorization cases). Alternatives:</p><ol><li>Express logic in Spark SQL functions (built-ins benefit from codegen).</li><li>Use SQL expressions with CASE / WHEN for branching.</li><li>For performance-critical custom code, consider Scala typed Dataset operations enabling some optimization retention.</li></ol><p>Pandas UDF (vectorized) reduces serialization overhead but may still underperform pure SQL when scalar operations dominate.</p><h2 id="12-resource-management--scheduling">12. Resource Management & Scheduling</h2><p>Cluster managers (YARN, Kubernetes, Standalone) allocate executors; dynamic allocation scales executors based on backlog. Considerations:</p><ol><li><strong>Executor sizing</strong>: Too large → long GC pauses; too small → excessive shuffle spill (per-executor memory fragmentation).</li><li><strong>Task parallelism</strong>: <code>spark.default.parallelism</code> and source partition counts drive initial stage partitioning; tune to balance overhead vs. parallelism.</li><li><strong>Locality wait</strong>: Adjust <code>spark.locality.wait</code> if tasks spend time waiting for node-local data.</li><li><strong>Fair vs. FIFO scheduling</strong>: Multi-tenant clusters may use pools to isolate latency-sensitive jobs.</li></ol><h2 id="13-monitoring--observability">13. Monitoring & Observability</h2><p>Key metrics:</p><ol><li>Input rows/sec & processing time (streaming).</li><li>Shuffle read/write sizes & spill metrics.</li><li>Executor CPU utilization, JVM heap usage, GC time ratio.</li><li>Stage failure counts & retried tasks.</li><li>Metadata ops (table refresh time, catalog latency) for lakehouse heavy workloads.</li></ol><p>Tools: Spark UI, History Server, Structured Streaming progress logs (JSON), external APM (OpenTelemetry exporters emerging).</p><h2 id="14-case-study-mini">14. Case Study (Mini)</h2><p>Workload: Sessionization + user feature aggregation + join with product dimension + write to Delta nightly + continuous incremental updates hourly.</p><p>Problems observed:</p><ol><li>Long tail reducers during dimension join.</li><li>Many tiny files (hourly micro-batches) hurting query planning time.</li><li>High GC in large executors.</li></ol><p>Interventions:</p><ol><li>Enabled AQE skew join splitting; tail 95th percentile task time dropped 40%.</li><li>Added <code>OPTIMIZE</code> (Delta file compaction) daily; planning time -60%.</li><li>Reduced executor heap size, increased executor count; GC time ratio from 18% → 6%.</li><li>Migrated Python UDF to SQL built-in expression; stage runtime -25%.</li></ol><p>Outcome: SLA latency met (p95 &lt; 8 min), compute cost reduced ~20%.</p><h2 id="15-future-directions">15. Future Directions</h2><p>Emerging themes:</p><ol><li><strong>Query acceleration via GPU</strong>: RAPIDS Accelerator for Spark offloads SQL/DataFrame ops to GPUs (columnar batches + cudf). Bottlenecks shift to shuffle & CPU↔GPU transfers.</li><li><strong>Incremental materialized views</strong>: Maintaining pre-computed aggregates with minimal recomputation (Delta Live Tables, Iceberg rewrite plans).</li><li><strong>Unified batch + streaming semantic layers</strong>: Continuous tables, streaming joins with snapshot isolation.</li><li><strong>Distributed cost-based optimizers</strong>: Sharing runtime stats across stages/jobs for better initial planning.</li><li><strong>Data-aware scheduling</strong>: Co-optimizing placement based on column subset usage patterns.</li></ol><h2 id="16-further-reading-titles">16. Further Reading (Titles)</h2><ul><li>&ldquo;Learning Spark: Lightning-Fast Data Analytics&rdquo; by Holden Karau, Andy Konwinski, Patrick Wendell, Matei Zaharia</li><li>&ldquo;High Performance Spark&rdquo; by Holden Karau, Rachel Warren</li><li>&ldquo;Spark: The Definitive Guide&rdquo; by Bill Chambers, Matei Zaharia</li><li>&ldquo;Designing Data-Intensive Applications&rdquo; by Martin Kleppmann (for broader data architecture & consistency patterns)</li><li>&ldquo;The Art of Scalability&rdquo; by Martin L. Abbott, Michael T. Fisher (for distributed systems organizational & scaling principles)</li></ul><hr><h2 id="17-summary">17. Summary</h2><p>Spark generalized MapReduce’s batch reliability model into a DAG-based, memory-conscious analytics engine supporting iterative, interactive, and streaming workloads. RDD lineage enabled fine-grained recomputation, while Catalyst + Tungsten closed performance gaps with MPP databases. Modern extensions (AQE, lakehouse formats, structured streaming) continue to blur boundaries between batch and real-time. The strategic shift: treat data processing as an evolving graph with adaptive runtime feedback rather than a fixed two-phase pipeline—unlocking richer optimization and lower latency.</p><h2 id="17-summary-1">17. Summary</h2><p>Spark generalized MapReduce’s batch reliability model into a DAG-based, memory-conscious analytics engine supporting iterative, interactive, and streaming workloads. RDD lineage enabled fine-grained recomputation, while Catalyst + Tungsten closed performance gaps with MPP databases. Modern extensions (AQE, lakehouse formats, structured streaming) continue to blur boundaries between batch and real-time. The strategic shift: treat data processing as an evolving graph with adaptive runtime feedback rather than a fixed two-phase pipeline—unlocking richer optimization and lower latency.</p></div><footer class="ce1a612 c6dfb1e c3ecea6"><div class="c364589">Categories:
<a href=/categories/distributed%20systems/>distributed systems</a>, <a href=/categories/data%20engineering/>data engineering</a></div><div>Tags:
<a href=/tags/spark/>#spark</a>, <a href=/tags/mapreduce/>#mapreduce</a>, <a href=/tags/dataproc/>#dataproc</a>, <a href=/tags/dag/>#dag</a></div></footer></article></main><footer class="ccdf0e8" role=contentinfo aria-label=Footer><div class="cfdda01 c133889 c5df473 c0eecc8 c69618a c6942b3 c03620d c2a9f27 c7c11d8 c82c52d c14527b"><div class="c6dfb1e c3ecea6 c39ef11 c88ae6f">&copy; 2025 Leonardo Benicio. All rights
reserved.</div><div class="c6942b3 c7c11d8 cd1fd22"><a href=https://github.com/lbenicio target=_blank rel="noopener noreferrer" aria-label=GitHub class="c1d6c20 c7c11d8 c1d0018 cd1fd22 cb5c327 c10dda9 c6dfb1e cbbda39 cfc01c7 c01f421 c286dd7 c2bd687 cfdce1d cfef18f c000b66 cf55a7b c514027"><svg width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M15 22v-4a4.8 4.8.0 00-1-3.5c3 0 6-2 6-5.5.08-1.25-.27-2.48-1-3.5.28-1.15.28-2.35.0-3.5.0.0-1 0-3 1.5-2.64-.5-5.36-.5-8 0C6 2 5 2 5 2c-.3 1.15-.3 2.35.0 3.5A5.403 5.403.0 004 9c0 3.5 3 5.5 6 5.5-.39.5-.67 1.08-.82 1.7s-.2 1.27-.18 1.9V22"/></svg>
<span class="cba5854">GitHub</span>
</a><a href=https://www.linkedin.com/in/leonardo-benicio target=_blank rel="noopener noreferrer" aria-label=LinkedIn class="c1d6c20 c7c11d8 c1d0018 cd1fd22 cb5c327 c10dda9 c6dfb1e cbbda39 cfc01c7 c01f421 c286dd7 c2bd687 cfdce1d cfef18f c000b66 cf55a7b c514027"><svg width="20" height="20" viewBox="0 0 24 24" fill="currentColor"><path d="M20.447 20.452H17.21V14.86c0-1.333-.027-3.046-1.858-3.046-1.86.0-2.145 1.45-2.145 2.948v5.69H9.069V9h3.112v1.561h.044c.434-.82 1.494-1.686 3.074-1.686 3.29.0 3.897 2.165 3.897 4.983v6.594zM5.337 7.433a1.805 1.805.0 11-.002-3.61 1.805 1.805.0 01.002 3.61zM6.763 20.452H3.911V9h2.852v11.452z"/></svg>
<span class="cba5854">LinkedIn</span>
</a><a href=https://twitter.com/lbenicio_ target=_blank rel="noopener noreferrer" aria-label=Twitter class="c1d6c20 c7c11d8 c1d0018 cd1fd22 cb5c327 c10dda9 c6dfb1e cbbda39 cfc01c7 c01f421 c286dd7 c2bd687 cfdce1d cfef18f c000b66 cf55a7b c514027"><svg width="20" height="20" viewBox="0 0 24 24" fill="currentColor"><path d="M19.633 7.997c.013.177.013.354.013.53.0 5.386-4.099 11.599-11.6 11.599-2.31.0-4.457-.676-6.265-1.842.324.038.636.05.972.05 1.91.0 3.67-.65 5.07-1.755a4.099 4.099.0 01-3.827-2.84c.25.039.5.064.763.064.363.0.726-.051 1.065-.139A4.091 4.091.0 012.542 9.649v-.051c.538.3 1.162.482 1.824.507A4.082 4.082.0 012.54 6.7c0-.751.2-1.435.551-2.034a11.63 11.63.0 008.44 4.281 4.615 4.615.0 01-.101-.938 4.091 4.091.0 017.078-2.799 8.1 8.1.0 002.595-.988 4.112 4.112.0 01-1.8 2.261 8.2 8.2.0 002.357-.638A8.824 8.824.0 0119.613 7.96z"/></svg>
<span class="cba5854">Twitter</span></a></div></div></footer></body></html>