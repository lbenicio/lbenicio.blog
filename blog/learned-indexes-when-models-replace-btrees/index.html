<!doctype html><html lang=en><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,maximum-scale=1,minimum-scale=1,user-scalable=no"><title>Learned Indexes: When Models Replace B‑Trees · Leonardo Benicio</title><meta name=description content="A practitioner's guide to learned indexes: how they work, when they beat classic data structures, and what it takes to ship them without getting paged."><link rel=alternate type=application/rss+xml title=RSS href=https://lbenicio.dev/index.xml><link rel=canonical href=https://blog.lbenicio.dev/blog/learned-indexes-when-models-replace-btrees/><link rel=preload href=/static/fonts/OpenSans-Regular.ttf as=font type=font/ttf crossorigin><link rel="stylesheet" href="/assets/css/fonts.min.40e2054b739ac45a0f9c940f4b44ec00c3b372356ebf61440a413c0337c5512e.css" crossorigin="anonymous" integrity="sha256-QOIFS3OaxFoPnJQPS0TsAMOzcjVuv2FECkE8AzfFUS4="><link rel="shortcut icon" href=/static/assets/favicon/favicon.ico><link rel=icon type=image/x-icon href=/static/assets/favicon/favicon.ico><link rel=icon href=/static/assets/favicon/favicon.svg type=image/svg+xml><link rel=icon href=/static/assets/favicon/favicon-32x32.png sizes=32x32 type=image/png><link rel=icon href=/static/assets/favicon/favicon-16x16.png sizes=16x16 type=image/png><link rel=apple-touch-icon href=/static/assets/favicon/apple-touch-icon.png><link rel=manifest href=/static/assets/favicon/site.webmanifest><link rel=mask-icon href=/static/assets/favicon/safari-pinned-tab.svg color=#209cee><meta name=msapplication-TileColor content="#209cee"><meta name=msapplication-config content="/static/assets/favicon/browserconfig.xml"><meta name=theme-color content="#d2e9f8"><meta property="og:title" content="Learned Indexes: When Models Replace B‑Trees · Leonardo Benicio"><meta property="og:description" content="A practitioner's guide to learned indexes: how they work, when they beat classic data structures, and what it takes to ship them without getting paged."><meta property="og:url" content="https://blog.lbenicio.dev/blog/learned-indexes-when-models-replace-btrees/"><meta property="og:type" content="article"><meta property="og:image" content="https://blog.lbenicio.dev/static/assets/images/blog/learned-indexes.png"><meta name=twitter:card content="summary_large_image"><meta name=twitter:title content="Learned Indexes: When Models Replace B‑Trees · Leonardo Benicio"><meta name=twitter:description content="A practitioner's guide to learned indexes: how they work, when they beat classic data structures, and what it takes to ship them without getting paged."><meta name=twitter:site content="@lbenicio_"><script type=application/ld+json>"{\"@context\":\"https://schema.org\",\"@type\":\"WebSite\",\"name\":\"About Leonardo Benicio\",\"url\":\"https://blog.lbenicio.dev\"}"</script><script type=application/ld+json>"{\"@context\":\"https://schema.org\",\"@type\":\"Person\",\"name\":\"Leonardo Benicio\",\"sameAs\":[\"https://github.com/lbenicio\",\"https://www.linkedin.com/in/leonardo-benicio\",\"https://twitter.com/lbenicio_\"],\"url\":\"https://blog.lbenicio.dev\"}"</script><script type=application/ld+json>"{\"@context\":\"https://schema.org\",\"@type\":\"BreadcrumbList\",\"itemListElement\":[{\"@type\":\"ListItem\",\"item\":\"https://blog.lbenicio.dev/\",\"name\":\"Home\",\"position\":1},{\"@type\":\"ListItem\",\"item\":\"https://blog.lbenicio.dev/\",\"name\":\"Blog\",\"position\":2},{\"@type\":\"ListItem\",\"item\":\"https://blog.lbenicio.dev/blog/learned-indexes-when-models-replace-btrees/\",\"name\":\"Learned Indexes When Models Replace Btrees\",\"position\":3}]}"</script><link rel="stylesheet" href="/assets/css/main.min.23cb77fd3186d94b425cf879bfff3195d7648b23b860d880dbb47fe2e115b884.css" crossorigin="anonymous" integrity="sha256-owHVkwE1+9dguAma85DLJbKG8+7vYa137CVrUeaaaxk="></head><body class="c6942b3 c03620d cf3bd2e"><script>(function(){try{document.addEventListener("gesturestart",function(e){e.preventDefault()}),document.addEventListener("touchstart",function(e){e.touches&&e.touches.length>1&&e.preventDefault()},{passive:!1});var e=0;document.addEventListener("touchend",function(t){var n=Date.now();n-e<=300&&t.preventDefault(),e=n},{passive:!1})}catch{}})()</script><a href=#content class="cba5854 c21e770 caffa6e cc5f604 cf2c31d cdd44dd c10dda9 c43876e c787e9b cddc2d2 cf55a7b c6dfb1e c9391e2">Skip to content</a>
<script>(function(){try{const e=localStorage.getItem("theme");e==="dark"&&document.documentElement.classList.add("dark");const t=document.querySelector('button[aria-label="Toggle theme"]');t&&t.setAttribute("aria-pressed",String(e==="dark"))}catch{}})();function toggleTheme(e){const s=document.documentElement,t=s.classList.toggle("dark");try{localStorage.setItem("theme",t?"dark":"light")}catch{}try{var n=e&&e.nodeType===1?e:document.querySelector('button[aria-label="Toggle theme"]');n&&n.setAttribute("aria-pressed",String(!!t))}catch{}}</script><header class="cd019ba c98dfae cdd44dd cfdda01 c9ee25d ce2dc7a cd72dd7 cc0dc37" role=banner><div class="cfdda01 c6942b3 ccf47f4 c7c11d8"><a href=/ class="c87e2b0 c6942b3 c7c11d8 c1838fa cb594e4" aria-label=Home><img src=/static/assets/favicon/favicon.svg alt=Logo width=32 height=32 class="c3de71a c4d5191">
<span class="cf8f011 c4d1253 cbd72bc cd7e69e">Leonardo Benicio</span></a><div class="c6942b3 c85cbd4 c7c11d8 ca798da c1838fa c7a0580"><nav class="cc1689c cd9b445 c75065d c04bab1" aria-label=Main><a href=/ class="c4d1253 c9e4539 cbbda39 c01f421 c19ee42 c3ecea6">Home</a>
<a href=https://lbenicio.dev/about target=_blank rel="noopener noreferrer" class="c4d1253 c9e4539 cbbda39 c01f421 c19ee42 c3ecea6">About</a>
<a href=https://lbenicio.dev/timeline target=_blank rel="noopener noreferrer" class="c4d1253 c9e4539 cbbda39 c01f421 c19ee42 c3ecea6">Timeline</a>
<a href=https://lbenicio.dev/reading target=_blank rel="noopener noreferrer" class="c4d1253 c9e4539 cbbda39 c01f421 c19ee42 c3ecea6">Reading</a>
<a href=https://lbenicio.dev/publications target=_blank rel="noopener noreferrer" class="c4d1253 c9e4539 cbbda39 c01f421 c19ee42 c3ecea6">Publications</a>
<a href=https://lbenicio.dev/contact target=_blank rel="noopener noreferrer" class="c4d1253 c9e4539 cbbda39 c01f421 c19ee42 c3ecea6">Contact</a></nav><button id="i1d73d4" type=button class="c1d6c20 c81ac7c c6a899b c7c11d8 c1d0018 c10dda9 c8e184d c514027 c88daee c7a66a6 c097fa1 cfc01c7 c286dd7 c2bd687 cfdce1d cfef18f" onclick=toggleTheme(this) aria-label="Toggle theme" aria-pressed=false title="Toggle theme">
<svg class="cb26e41 c50ceea cb69a5c c4f45c8 c8c2c40" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" aria-hidden="true"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg>
<svg class="cb26e41 c8fca2b cb69a5c c4f45c8 cc1689c c9c27ff" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" aria-hidden="true"><circle cx="12" cy="12" r="4"/><path d="M12 2v4"/><path d="M12 18v4"/><path d="M2 12h4"/><path d="M18 12h4"/><path d="M4.93 4.93l2.83 2.83"/><path d="M16.24 16.24l2.83 2.83"/><path d="M6.34 17.66l2.83-2.83"/><path d="M14.83 9.17l2.83-2.83"/></svg>
<span class="cba5854">Toggle theme</span></button><div class="c658bcf c097fa1"><details class="ccd45bf"><summary class="cc7a258 c1d6c20 c7c11d8 c1d0018 c10dda9 c000b66 cf55a7b"><svg class="c20e4eb cb58471" fill="none" viewBox="0 0 24 24" stroke="currentColor"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M4 6h16M4 12h16M4 18h16"/></svg>
<span class="cba5854">Open menu</span></summary><div class="ce49c1e c437fa9 c1b4412 c8c0110 c887979 c43876e c10dda9 c60a4cc c401fa1 cb2c551 cf514a5 cadfe0b ce3dbb2 c72ad85 cbd4710 c6988b4"><a href=/ class="c62aaf0 c364589 c6942b3 c7c11d8 c1838fa" aria-label=Home><img src=/static/assets/favicon/favicon.svg alt=Logo width=24 height=24 class="c20e4eb cb58471">
<span class="cf8f011 c7c1b66 cbd72bc cbac0b8">Leonardo Benicio</span></a><nav class="c6942b3 c03620d cd69733"><a href=/ class="c4d1253 cbbda39 c3ecea6 c19ee42">Home</a>
<a href=https://lbenicio.dev/about target=_blank rel="noopener noreferrer" class="c4d1253 cbbda39 c3ecea6 c19ee42">About</a>
<a href=https://lbenicio.dev/timeline target=_blank rel="noopener noreferrer" class="c4d1253 cbbda39 c3ecea6 c19ee42">Timeline</a>
<a href=https://lbenicio.dev/reading target=_blank rel="noopener noreferrer" class="c4d1253 cbbda39 c3ecea6 c19ee42">Reading</a>
<a href=https://lbenicio.dev/publications target=_blank rel="noopener noreferrer" class="c4d1253 cbbda39 c3ecea6 c19ee42">Publications</a>
<a href=https://lbenicio.dev/contact target=_blank rel="noopener noreferrer" class="c4d1253 cbbda39 c3ecea6 c19ee42">Contact</a></nav></div></details></div></div></div></header><div class="caffa6e c437fa9 ce9aced c97bba6 c15da2a c975cba" role=complementary aria-label="GitHub repository"><div class="c9d056d c252f85 ca22532 ca88a1a c876315"><div class="c6942b3 c7c11d8 c1d0018 cd1fd22 c6066e4 c43876e ce3d5b6 caa20d2 c3ecea6 c0cd2e2 cddc2d2 c3ed5c9 cd4074c c876315"><a href=https://github.com/lbenicio/aboutme target=_blank rel="noopener noreferrer" class="c6942b3 c7c11d8 cd1fd22 c71bae8 cfac1ac c19ee42 c25dc7c cb40739 cbbda39 cf55a7b" aria-label="View source on GitHub"><svg width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="cb26e41 c41bcd4 cf17690 cfa4e34 c78d562" aria-hidden="true"><path d="M15 22v-4a4.8 4.8.0 00-1-3.5c3 0 6-2 6-5.5.08-1.25-.27-2.48-1-3.5.28-1.15.28-2.35.0-3.5.0.0-1 0-3 1.5-2.64-.5-5.36-.5-8 0C6 2 5 2 5 2c-.3 1.15-.3 2.35.0 3.5A5.403 5.403.0 004 9c0 3.5 3 5.5 6 5.5-.39.49-.68 1.05-.85 1.65-.17.6-.22 1.23-.15 1.85v4"/><path d="M9 18c-4.51 2-5-2-7-2"/></svg>
<span class="cb5c327 cd7e69e">Fork me</span></a></div></div></div><main id="i7eccc0" class="cfdda01 c5df473 c0eecc8 c85cbd4" role=main aria-label=Content><nav class="cb545ce c8d8ae4 c277478" aria-label=Breadcrumb><ol class="c6942b3 c3adaf2 c7c11d8 cd365ee c3ecea6"><li><a href=/ class="c19ee42 c71bae8 cfac1ac">Home</a></li><li class="c6942b3 c7c11d8 cd365ee"><span class="cb82ddd">/</span><a href=/ class="c19ee42 c71bae8 cfac1ac">Blog</a></li><li class="c6942b3 c7c11d8 cd365ee"><span class="cb82ddd">/</span><span class="c88daee">Learned Indexes When Models Replace Btrees</span></li></ol></nav><article class="c461ba0 c1c203f cfb6084 c995404 c6ca165"><nav class="cb545ce c8d8ae4 c277478" aria-label=Breadcrumb><ol class="c6942b3 c3adaf2 c7c11d8 cd365ee c3ecea6"><li><a href=/ class="c19ee42 c71bae8 cfac1ac">Home</a></li><li class="c6942b3 c7c11d8 cd365ee"><span class="cb82ddd">/</span><a href=/ class="c19ee42 c71bae8 cfac1ac">Blog</a></li><li class="c6942b3 c7c11d8 cd365ee"><span class="cb82ddd">/</span><span class="c88daee">Learned Indexes When Models Replace Btrees</span></li></ol></nav><header class="c8aedc7"><h1 class="cf304bc c6fb0fe cf8f011 cc484e1">Learned Indexes: When Models Replace B‑Trees</h1><div class="c277478 c3ecea6 c8fb24a">2025-10-04
· Leonardo Benicio</div><div class="c1a1a3f c8124f2"><img src=/static/assets/images/blog/learned-indexes.png alt class="cfdda01 c524300 c677556"></div><p class="lead c3ecea6">A practitioner's guide to learned indexes: how they work, when they beat classic data structures, and what it takes to ship them without getting paged.</p></header><div class="content"><p>If you’ve spent a career trusting B‑trees and hash tables, the idea of using a machine‑learned model as an index can feel like swapping a torque wrench for a Ouija board. But learned indexes aren’t a gimmick. They exploit a simple observation: real data isn’t uniformly random. It has shape—monotonic keys, skewed distributions, natural clusters—and a model can learn that shape to predict where a key lives in a sorted array. The payoff is smaller indexes, fewer cache misses, and—sometimes—dramatically faster lookups.</p><p>This post is a field guide to learned indexes that you can take beyond the whiteboard. We’ll cover how they work, when they win, the trade‑offs that bite, and a practical path to evaluating and deploying them next to (not instead of) your trusty structures.</p><ul><li>A three‑minute refresher on what an index actually does</li><li>The core learned‑index recipe (prediction + error bounds + fallback)</li><li>Models that work in practice (RMI, splines, piecewise linear, tiny MLPs)</li><li>Writes, updates, and tail behavior under pressure</li><li>On‑disk vs in‑memory, SSD realities, and compression</li><li>Observability, failure modes, and a migration playbook</li></ul><p>By the end, you’ll know whether your workload is a candidate and how to prove it with a weekend experiment.</p><h2 id="what-an-index-really-does-and-why-btrees-are-great">What an index really does (and why B‑trees are great)</h2><p>An index is a function f that maps a key k to the location of its value—either directly (hash) or to a small neighborhood in a sorted order (tree). The work you pay for on a lookup is: pointer chasing, cache misses, and comparisons. B‑trees win because they minimize height and maximize fanout; each internal node fetch brings in a whole cache line of keys, and you descend O(log_B N) levels with good locality.</p><p>But B‑trees assume nothing about the data distribution. They’re worst‑case safe. If your keys are smooth and monotonic (timestamps, user IDs, metric offsets, lexicographically incremental strings), a model can often predict within a tiny window where a key should be in a sorted array—shrinking the index and the number of cache lines you touch.</p><p>Two consequences matter in production:</p><ol><li>Space efficiency. Smaller metadata means more hot data in cache and fewer I/O operations on cold paths.</li><li>Latency shape. If predictions land within tight windows, you do minimal scanning. If they miss, you fall back to a binary search or a local miniature tree—but that tail risk must be bounded.</li></ol><h2 id="the-learnedindex-recipe">The learned‑index recipe</h2><p>At heart, learned indexes approximate the cumulative distribution function (CDF) of keys. Given sorted keys K[0..N‑1], define CDF(k) ≈ rank(k)/N. If you can learn g(k) ≈ CDF(k), then N·g(k) predicts the position of k in the array. You then search only a small neighborhood around that predicted position to confirm existence and retrieve the value.</p><p>The recipe looks like this:</p><ol><li>Fit a model g(k) to approximate rank/N.</li><li>Measure the worst‑case absolute error ε across your training/validation keys.</li><li>At lookup, compute p = ⌊N · g(k)⌋ and search in [p‑ε, p+ε] (clamped to [0,N)).</li><li>If not found, use a fallback (binary search, or escalate to a secondary structure).</li></ol><p>This is robust if ε is small and stable as data evolves. If ε drifts, you retrain or rebalance.</p><h3 id="a-tiny-sketch">A tiny sketch</h3><div class="highlight"><pre tabindex=0 style=color:#e6edf3;background-color:#0d1117;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class="language-fallback" data-lang=fallback><span style=display:flex><span>function get(key):
</span></span><span style=display:flex><span>  p = floor(N * g(key))             # model predicts approximate position
</span></span><span style=display:flex><span>  lo = max(0, p - EPS)
</span></span><span style=display:flex><span>  hi = min(N-1, p + EPS)
</span></span><span style=display:flex><span>  # scan or binary search inside [lo, hi]
</span></span><span style=display:flex><span>  i = binary_search_in_window(K, key, lo, hi)
</span></span><span style=display:flex><span>  if i != NOT_FOUND:
</span></span><span style=display:flex><span>    return V[i]
</span></span><span style=display:flex><span>  # fallback if prediction failed badly
</span></span><span style=display:flex><span>  return global_binary_search(K, key)
</span></span></code></pre></div><p>The whole game is choosing g and bounding EPS (ε). The less you scan, the better your cache behavior and the tighter your tail latency.</p><h2 id="models-that-actually-ship">Models that actually ship</h2><p>Learned indexes don’t need huge neural networks. In shipped systems, the winning choices are boring:</p><ul><li>Piecewise linear models: Split key space into M segments; each segment gets a slope/intercept. Accurate, tiny, branch‑free.</li><li>RMIs (Recursive Model Index): A two‑level (or few‑level) model: a root routes to a leaf model that predicts the position. Each leaf is simple (linear or spline).</li><li>Splines: Monotone piecewise polynomials with continuity constraints—more accurate than pure linear, still cheap.</li><li>Tiny MLPs: One or two hidden layers with ReLU or tanh. Useful when key distributions have bends. Use only if they fit in L1/L2 and vectorize well.</li></ul><p>What you avoid: heavyweight models, dynamic control flow, or anything that inflates prediction latency more than it saves on search.</p><h3 id="capacity-planning-for-the-model">Capacity planning for the model</h3><p>Let’s say you have 100M keys. A classic B‑tree might consume ~8–16 bytes per key in overhead (fanout, pointers, node headers), often more on disk. A learned index might fit in ~0.5–2 bytes per key of model parameters if the distribution is friendly. Even a 5× reduction turns into a different cache profile.</p><p>The key is the ε you can guarantee. If a model predicts positions with ε ≤ 64, your windowed binary search costs at most 7 comparisons (log2(129) ≈ 7) instead of a cross‑node walk. Multiply by fan‑out and cacheline effects, and you’ll feel it.</p><h2 id="writes-updates-and-reality">Writes, updates, and reality</h2><p>Everything above sounds neat until you start mutating data.</p><ul><li>Appends. If keys are mostly increasing (time series), you can maintain a learned index with periodic incremental retraining or by reserving a “delta buffer” that’s merged when it gets large.</li><li>In‑place updates. Changing a value doesn’t move a key; no problem.</li><li>Inserts (out‑of‑order keys). Inserts into the middle shift ranks; your ε bound can drift. Solutions: segmented structures (per‑segment models), indirection layers (a pointer array), or a small B‑tree overlay for recent inserts that you periodically fold into base storage.</li><li>Deletes. Tombstone then compact; compaction triggers retrain or local segment refits.</li></ul><p>Two patterns dominate deployed systems:</p><ol><li>LSM‑style layering: Base data is sorted arrays with learned indexes; a mutable memtable (tree or skiplist) takes writes. Background compaction merges and retrains segment models as needed.</li><li>Segmented arrays: Key space is partitioned into chunks (e.g., by top bits or by equal‑count buckets). Each segment has its own tiny model and error bound. Insertions only affect local segments; hot segments can be re‑fit.</li></ol><h3 id="tail-behavior-under-churn">Tail behavior under churn</h3><p>Even when median costs drop, you must protect the tail. Choose one or more:</p><ul><li>Bound scan windows conservatively: track ε as a high percentile plus slack (e.g., P99 + margin) and use that as your window.</li><li>Time budgets and fallbacks: if the in‑window search exceeds a micro‑budget, jump straight to a conventional index.</li><li>Versioned models: route lookups based on a version tag so that in‑flight queries don’t straddle an update with different ε.</li></ul><h2 id="ondisk-vs-inmemory-and-ssd-realities">On‑disk vs in‑memory and SSD realities</h2><p>In memory, learned indexes shine when cachelines are precious. On SSD, it’s more nuanced:</p><ul><li>Sequential scans are cheap; random reads are expensive. A learned index that narrows the search range to a few adjacent pages is valuable, but only if you don’t add extra random reads for the model itself.</li><li>Page layouts dominate. Keep model parameters co‑located with data pages or resident in memory. Avoid a second random read to fetch model state.</li><li>Compression helps more than you think. Sorted arrays compress fantastically with delta coding and variable‑byte schemes. A learned index plus compressed arrays can drastically reduce I/O.</li></ul><h3 id="hybrid-model--fence-pointers">Hybrid: model + fence pointers</h3><p>A pragmatic on‑disk design: store sparse fence pointers (every Nth key with its page offset). Use a learned model to jump near the right fence, then a minibsearch among fences, then one or two page reads and a local search. Space stays small, and you bound random I/O.</p><h2 id="memory-simd-and-branch-prediction">Memory, SIMD, and branch prediction</h2><p>To make this fast on CPUs:</p><ul><li>Keep models tiny and contiguous. A two‑level RMI where the leaf table fits in L1 can outperform anything larger that spills to L3.</li><li>Favor linear pieces and splines that compile to fused multiply‑add (FMA) without branches.</li><li>Use SIMD binary search for the in‑window probe (galloping search or small unrolled comparisons).</li><li>Align arrays, remove bounds checks in hot loops, and prefetch the predicted window.</li></ul><p>On GPUs, learned indexes can batch lookups and exploit massive parallel search. But unless your workload naturally batches, hovering on CPU with cache‑friendly models is often better.</p><h2 id="when-learned-indexes-win-and-when-they-dont">When learned indexes win (and when they don’t)</h2><p>They win when:</p><ul><li>Keys are monotone or near‑monotone and the CDF is smooth (timestamps, IDs, lexicographic prefixes that grow in practice).</li><li>Read‑heavy workloads with high locality; write rates are moderate or append‑dominated.</li><li>Memory pressure is real; shrinking the index frees RAM for the working set.</li></ul><p>They don’t when:</p><ul><li>Keys are adversarial or highly irregular (cryptographic hashes, uniformly random keys).</li><li>The distribution shifts rapidly; retraining cost or ε drift dominates.</li><li>Write‑heavy workloads where compaction or segment refitting will thrash.</li></ul><h2 id="observability-and-slos">Observability and SLOs</h2><p>Treat a learned index like a cache: it must earn its keep and have strong guardrails.</p><ul><li>Export ε metrics per segment and per model version. Track P50/P95/P99 and max.</li><li>Record window sizes, fallback hit rates, and latency distributions for model predictions vs. fallback.</li><li>Emit a “budget overrun” counter when in‑window searches exceed micro‑budgets so you can tighten windows or retrain.</li></ul><p>If ε balloons or fallbacks spike, roll back to a classic index. Feature‑flag the learned path behind a router so you can dial traffic.</p><h2 id="migration-playbook">Migration playbook</h2><p>Here’s a safe, incremental approach:</p><ol><li>Shadow mode: Build a read‑only learned index next to your existing structure. On a fraction of read requests, compute predictions and log the implied window size and hypothetical steps; don’t serve from it.</li><li>Offline evaluation: Using logs, compute what ε would have been and what fraction of queries would have fallen back. Estimate RAM wins and I/O savings.</li><li>Canary serve: Route 1% of get() calls through the learned path with tight budgets and immediate fallback on miss. Compare SLOs.</li><li>Expand or roll back: If medians improve and tails hold, increase traffic. If ε drifts, retrain or tighten windows. Keep a kill switch.</li><li>Writes: Add a small write buffer (memtable) and a background task to reconcile with the base arrays and refresh segment models.</li></ol><h2 id="a-worked-example-piecewise-linear-on-timestamps">A worked example: piecewise linear on timestamps</h2><p>Imagine a log store indexed by event time. Keys are nearly monotone but have small out‑of‑order inserts (late arrivals). The distribution over a day looks like a gentle slope with lunchtime spikes.</p><p>Design:</p><ul><li>Partition the day into M segments such that each segment has ~N/M keys.</li><li>Fit a line y = ax + b per segment that maps timestamps to rank.</li><li>Measure ε per segment on a validation set; store ε as P99 + safety margin.</li><li>At lookup, compute segment from a top‑level router (either by time range or a tiny model), then do windowed search inside the segment.</li><li>Maintain a memtable for late inserts and merge hourly; adjust only the affected segments.</li></ul><p>Outcomes you should expect to measure:</p><ul><li>Index footprint drops by ~5–10× vs B‑tree fanout overhead.</li><li>Median lookup improves thanks to fewer cache lines touched.</li><li>Tail stays controlled if ε bounds are conservative; fallbacks rarely trip.</li><li>Write cost increases modestly due to segment refresh, but remains amortized.</li></ul><h2 id="correctness-and-guarantees">Correctness and guarantees</h2><p>Learned indexes don’t change correctness criteria—only how you locate candidates to compare. If your in‑window search and fallback logic are correct, you never return an incorrect value; at worst you pay extra steps to find the key. The risk is purely performance: if ε was underestimated, tails suffer. That’s why per‑segment ε metrics and fallbacks exist.</p><p>If your workload needs strict worst‑case bounds, pair the learned index with a small auxiliary structure that catches long tails:</p><ul><li>A tiny B‑tree or skiplist per segment that only stores keys which exceeded the window in the last X minutes (an “exception table”).</li><li>A capped fallback budget: after Y steps, jump to the classic index regardless.</li></ul><h2 id="variants-youll-encounter">Variants you’ll encounter</h2><ul><li>ALEX: Adaptive learned index that organizes data into dynamic model‑guided nodes; good for mixed read/write.</li><li>PGM Index: A succinct structure based on piecewise linear models with provable bounds on ε; great practical baseline.</li><li>SOSD benchmarks: Standard suite to compare learned and classic indexes across datasets and hardware.</li></ul><p>If you’re prototyping, start with PGM. It’s simple, fast, and comes with strong guarantees.</p><h2 id="frequently-asked-questions">Frequently asked questions</h2><p>• What about composite keys?<br>Map the composite key to a comparable scalar (e.g., lexicographic mapping, feature hashing that preserves order for components that matter). Or build a learned router that chooses sub‑indexes per prefix.</p><p>• What if my distribution shifts daily?<br>Version models by day (or hour) and keep a small ensemble. Route lookups to the appropriate version by a cheap heuristic, and garbage‑collect old ones.</p><p>• Can I combine with Bloom filters?<br>Yes. A Bloom filter catches negative lookups cheaply, reducing wasted window scans for absent keys.</p><p>• Do I need GPUs?<br>No. The models used here are tiny and run in a handful of CPU cycles. Spend your budget on layout and cache behavior.</p><h2 id="a-minimal-evaluation-plan-you-can-run-this-week">A minimal evaluation plan you can run this week</h2><ol><li>Export a sample of 100M keys from production (or a representative environment). Keep them sorted.</li><li>Split into train/validation (e.g., 80/20). Fit a PGM or piecewise‑linear model.</li><li>Measure ε on validation, distribution of window sizes, and compare to B‑tree search steps.</li><li>Microbench: pin CPU frequency, warm caches, and measure predicted‑window search vs B‑tree.</li><li>End‑to‑end: replace only the get() path on a read‑heavy service and route 1% traffic behind a flag. Track medians and tails for a week.</li></ol><p>If you don’t see a clear win (RAM saved, medians down, tails flat), don’t ship it. Learned indexes must earn their keep like any optimization.</p><h2 id="the-fine-print-pitfalls-to-avoid">The fine print: pitfalls to avoid</h2><ul><li>Overfitting ε. A pretty validation ε can blow up on production skew or seasonality. Track it live; cap windows.</li><li>Model drift during compaction. Don’t swap models mid‑query; use versioned routes.</li><li>Hidden random reads. If your model lives off‑page on SSD, you defeat the purpose. Keep it in memory and colocated.</li><li>Microbranching. A fancy model that introduces unpredictable branches can murder branch predictors. Linear pieces avoid that.</li><li>Ignoring concurrency. Parallel lookups contend for shared structures; test under load to catch tail regressions.</li></ul><h2 id="closing-modern-indexes-are-interfaces-not-idols">Closing: modern indexes are interfaces, not idols</h2><p>A learned index is not a religion, it’s an interface with two interchangeable parts: a predictor and a verifier. When your data has shape, a compact predictor can shrink the search and make caches happy. When the shape shifts, a verifier with fallbacks keeps you correct. Ship both. Measure both. And keep the kill switch within reach.</p><p>The real win isn’t novelty; it’s choice. You can keep B‑trees where they shine and layer learned predictors where they pay—mixing tools to fit your workload’s contours. That’s systems engineering.</p><h2 id="production-case-studies-what-wins-and-where-it-hurts">Production case studies (what wins and where it hurts)</h2><p>To make this concrete, here are anonymized patterns from real deployments:</p><ul><li>Time‑series metrics store (billions of points/day). Keys are (tenant, metric, timestamp). The CDF per (tenant, metric) is nearly linear with lunch‑time spikes. Piecewise linear segments with per‑segment ε tracked at P99 + 10% reduced index RAM ~8×. Median gets improved ~1.6×; tails stayed flat with a strict in‑window budget and fallback to a sparse fence pointer index. Retraining hourly for hot tenants, daily for cold, avoided drift. The main operational risk was segment hotspots after a tenant’s surprise cardinality explosion; auto‑splitting segments based on ε growth fixed it.</li><li>Read‑mostly KV cache with lexicographic keys. A two‑level RMI with 256 leaf models fit in L2. Under steady traffic, lookups dropped to 2–3 cachelines on average from 6–8 in the legacy B‑tree. However, a quarterly key rollover changed prefixes and bent the CDF badly. Versioned models (old+new) with a 14‑day dual‑read period made the shift safe; then old models were retired.</li><li>SSD‑backed object directory. A hybrid learned+fence index cut random reads by ~30%. The surprise cost: during compaction, re‑writing segment metadata caused tail bumps. Gating compaction to off‑peak windows and making model pages memory‑resident addressed the regression.</li></ul><h2 id="antipatterns-smells-to-catch-in-review">Anti‑patterns (smells to catch in review)</h2><ul><li>Model lives on SSD. Any design that requires random reads to fetch model parameters defeats the point. Keep parameters in RAM; co‑locate per‑segment parameters with the data page they index.</li><li>Tiny improvements, huge complexity. If the learned path only trims 1–2 comparisons over a mature B‑tree, the extra operational risk may not be worth it. Demand a budget: RAM saved, medians improved, tails flat.</li><li>Single global ε. Different regions of keyspace bend differently. Track ε per segment and route with a tiny root model; using one global bound turns into scanning.</li><li>Retrain in place. Swap models under active traffic without versioning and you’ll have probes using mismatched ε. Always version, route by version, then GC.</li></ul><h2 id="a-readiness-checklist-preship">A readiness checklist (pre‑ship)</h2><ul><li><input disabled type=checkbox> Per‑segment ε metrics exported (P50/P95/P99/max)</li><li><input disabled type=checkbox> Fallback hit‑rate monitored and alert thresholds defined</li><li><input disabled type=checkbox> Budget timers for in‑window probes (micro‑budgets)</li><li><input disabled type=checkbox> Versioned routing, canary flag, and kill switch</li><li><input disabled type=checkbox> Model build pipeline with deterministic artifacts (hashes)</li><li><input disabled type=checkbox> Backfill tooling to rebuild models from snapshots</li><li><input disabled type=checkbox> Scale test including write bursts and compactions</li><li><input disabled type=checkbox> Runbook for drift (what to tighten, when to retrain, how to roll back)</li></ul><h2 id="beyond-point-lookups-ranges-composite-keys-and-secondary-structures">Beyond point lookups: ranges, composite keys, and secondary structures</h2><p>Learned indexes are excellent at point predictions, but many workloads rely on range scans and complex predicates.</p><ul><li>Ranges. Predict start position and either predict end or scan until the predicate fails. Keep page‑level fence pointers to jump page boundaries without per‑row checks.</li><li>Composite keys. Routable prefixes at the root, then leaf models per suffix; or learn a projection (e.g., a monotone mapping) that preserves ordering. Avoid hash‑based projections if you need order.</li><li>Secondary structures. Pair the learned predictor with a Bloom filter for negative lookups and a tiny exception table catching keys that exceeded window budgets recently. The exception table doubles as drift telemetry.</li></ul><h2 id="where-this-heads-next">Where this heads next</h2><p>Three promising directions:</p><ol><li>CPU specialization. SVE/AVX‑512 make small vector models cheaper; expect more vectorized leaf predictors.</li><li>Learned routers for LSM compactions. Route keys into compaction tiers that match their observed drift; keep ε stable under churn.</li><li>Hybrid vector search. Use a learned index to route high‑recall candidate retrieval for vector ANN structures (HNSW/IVF), cutting the candidate set and memory traffic.</li></ol><p>If you treat the predictor as a cacheable hint with guardrails, you can adopt learned indexes incrementally and widen their scope as evidence accumulates.</p></div><footer class="ce1a612 c6dfb1e c3ecea6"><div class="c364589">Categories:
<a href=/categories/Engineering/>Engineering</a></div><div>Tags:
<a href=/tags/databases/>#databases</a>, <a href=/tags/indexes/>#indexes</a>, <a href=/tags/machine-learning/>#machine-learning</a>, <a href=/tags/systems/>#systems</a>, <a href=/tags/storage/>#storage</a>, <a href=/tags/performance/>#performance</a></div></footer></article></main><footer class="ccdf0e8" role=contentinfo aria-label=Footer><div class="cfdda01 c133889 c5df473 c0eecc8 c69618a c6942b3 c03620d c2a9f27 c7c11d8 c82c52d c14527b"><div class="c6dfb1e c3ecea6 c39ef11 c88ae6f">&copy; 2025 Leonardo Benicio. All rights
reserved.</div><div class="c6942b3 c7c11d8 cd1fd22"><a href=https://github.com/lbenicio target=_blank rel="noopener noreferrer" aria-label=GitHub class="c1d6c20 c7c11d8 c1d0018 cd1fd22 cb5c327 c10dda9 c6dfb1e cbbda39 cfc01c7 c01f421 c286dd7 c2bd687 cfdce1d cfef18f c000b66 cf55a7b c514027"><svg width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M15 22v-4a4.8 4.8.0 00-1-3.5c3 0 6-2 6-5.5.08-1.25-.27-2.48-1-3.5.28-1.15.28-2.35.0-3.5.0.0-1 0-3 1.5-2.64-.5-5.36-.5-8 0C6 2 5 2 5 2c-.3 1.15-.3 2.35.0 3.5A5.403 5.403.0 004 9c0 3.5 3 5.5 6 5.5-.39.5-.67 1.08-.82 1.7s-.2 1.27-.18 1.9V22"/></svg>
<span class="cba5854">GitHub</span>
</a><a href=https://www.linkedin.com/in/leonardo-benicio target=_blank rel="noopener noreferrer" aria-label=LinkedIn class="c1d6c20 c7c11d8 c1d0018 cd1fd22 cb5c327 c10dda9 c6dfb1e cbbda39 cfc01c7 c01f421 c286dd7 c2bd687 cfdce1d cfef18f c000b66 cf55a7b c514027"><svg width="20" height="20" viewBox="0 0 24 24" fill="currentColor"><path d="M20.447 20.452H17.21V14.86c0-1.333-.027-3.046-1.858-3.046-1.86.0-2.145 1.45-2.145 2.948v5.69H9.069V9h3.112v1.561h.044c.434-.82 1.494-1.686 3.074-1.686 3.29.0 3.897 2.165 3.897 4.983v6.594zM5.337 7.433a1.805 1.805.0 11-.002-3.61 1.805 1.805.0 01.002 3.61zM6.763 20.452H3.911V9h2.852v11.452z"/></svg>
<span class="cba5854">LinkedIn</span>
</a><a href=https://twitter.com/lbenicio_ target=_blank rel="noopener noreferrer" aria-label=Twitter class="c1d6c20 c7c11d8 c1d0018 cd1fd22 cb5c327 c10dda9 c6dfb1e cbbda39 cfc01c7 c01f421 c286dd7 c2bd687 cfdce1d cfef18f c000b66 cf55a7b c514027"><svg width="20" height="20" viewBox="0 0 24 24" fill="currentColor"><path d="M19.633 7.997c.013.177.013.354.013.53.0 5.386-4.099 11.599-11.6 11.599-2.31.0-4.457-.676-6.265-1.842.324.038.636.05.972.05 1.91.0 3.67-.65 5.07-1.755a4.099 4.099.0 01-3.827-2.84c.25.039.5.064.763.064.363.0.726-.051 1.065-.139A4.091 4.091.0 012.542 9.649v-.051c.538.3 1.162.482 1.824.507A4.082 4.082.0 012.54 6.7c0-.751.2-1.435.551-2.034a11.63 11.63.0 008.44 4.281 4.615 4.615.0 01-.101-.938 4.091 4.091.0 017.078-2.799 8.1 8.1.0 002.595-.988 4.112 4.112.0 01-1.8 2.261 8.2 8.2.0 002.357-.638A8.824 8.824.0 0119.613 7.96z"/></svg>
<span class="cba5854">Twitter</span></a></div></div></footer></body></html>