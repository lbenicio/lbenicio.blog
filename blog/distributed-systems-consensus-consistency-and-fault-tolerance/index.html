<!doctype html><html lang=en><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,maximum-scale=1,minimum-scale=1,user-scalable=no"><title>Distributed Systems: Consensus, Consistency, and Fault Tolerance · Leonardo Benicio</title><meta name=description content="Fundamentals of distributed systems: failure models, consensus algorithms (Paxos, Raft), CAP theorem, consistency models, gossip, membership, CRDTs, and practical testing strategies like Jepsen."><link rel=alternate type=application/rss+xml title=RSS href=https://lbenicio.dev/index.xml><link rel=canonical href=https://blog.lbenicio.dev/blog/distributed-systems-consensus-consistency-and-fault-tolerance/><link rel=preload href=/static/fonts/OpenSans-Regular.ttf as=font type=font/ttf crossorigin><link rel="stylesheet" href="/assets/css/fonts.min.40e2054b739ac45a0f9c940f4b44ec00c3b372356ebf61440a413c0337c5512e.css" crossorigin="anonymous" integrity="sha256-QOIFS3OaxFoPnJQPS0TsAMOzcjVuv2FECkE8AzfFUS4="><link rel="shortcut icon" href=/static/assets/favicon/favicon.ico><link rel=icon type=image/x-icon href=/static/assets/favicon/favicon.ico><link rel=icon href=/static/assets/favicon/favicon.svg type=image/svg+xml><link rel=icon href=/static/assets/favicon/favicon-32x32.png sizes=32x32 type=image/png><link rel=icon href=/static/assets/favicon/favicon-16x16.png sizes=16x16 type=image/png><link rel=apple-touch-icon href=/static/assets/favicon/apple-touch-icon.png><link rel=manifest href=/static/assets/favicon/site.webmanifest><link rel=mask-icon href=/static/assets/favicon/safari-pinned-tab.svg color=#209cee><meta name=msapplication-TileColor content="#209cee"><meta name=msapplication-config content="/static/assets/favicon/browserconfig.xml"><meta name=theme-color content="#d2e9f8"><meta property="og:title" content="Distributed Systems: Consensus, Consistency, and Fault Tolerance · Leonardo Benicio"><meta property="og:description" content="Fundamentals of distributed systems: failure models, consensus algorithms (Paxos, Raft), CAP theorem, consistency models, gossip, membership, CRDTs, and practical testing strategies like Jepsen."><meta property="og:url" content="https://blog.lbenicio.dev/blog/distributed-systems-consensus-consistency-and-fault-tolerance/"><meta property="og:type" content="article"><meta property="og:image" content="https://blog.lbenicio.dev/static/assets/images/blog/distributed-systems-consensus-cap-gossip.png"><meta name=twitter:card content="summary_large_image"><meta name=twitter:title content="Distributed Systems: Consensus, Consistency, and Fault Tolerance · Leonardo Benicio"><meta name=twitter:description content="Fundamentals of distributed systems: failure models, consensus algorithms (Paxos, Raft), CAP theorem, consistency models, gossip, membership, CRDTs, and practical testing strategies like Jepsen."><meta name=twitter:site content="@lbenicio_"><script type=application/ld+json>{"@context":"https://schema.org","@type":"WebSite","name":"About Leonardo Benicio","url":"https://blog.lbenicio.dev"}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"Person","name":"Leonardo Benicio","sameAs":["https://github.com/lbenicio","https://www.linkedin.com/in/leonardo-benicio","https://twitter.com/lbenicio_"],"url":"https://blog.lbenicio.dev"}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","item":"https://blog.lbenicio.dev/","name":"Home","position":1},{"@type":"ListItem","item":"https://blog.lbenicio.dev/","name":"Blog","position":2},{"@type":"ListItem","item":"https://blog.lbenicio.dev/blog/distributed-systems-consensus-consistency-and-fault-tolerance/","name":"Distributed Systems Consensus Consistency and Fault Tolerance","position":3}]}</script><link rel="stylesheet" href="/assets/css/main.min.1e8a566ac8bc3f0664d0db4ec8a015b07421c33fa11d336a6b914522a9cabf30.css" crossorigin="anonymous" integrity="sha256-6lhUOpwCHMSMROmggsVSp3AHKud6gBrIFGTzl3GV4BY="></head><body class="c6942b3 c03620d cf3bd2e"><script>(function(){try{document.addEventListener("gesturestart",function(e){e.preventDefault()}),document.addEventListener("touchstart",function(e){e.touches&&e.touches.length>1&&e.preventDefault()},{passive:!1});var e=0;document.addEventListener("touchend",function(t){var n=Date.now();n-e<=300&&t.preventDefault(),e=n},{passive:!1})}catch{}})()</script><a href=#content class="cba5854 c21e770 caffa6e cc5f604 cf2c31d cdd44dd c10dda9 c43876e c787e9b cddc2d2 cf55a7b c6dfb1e c9391e2">Skip to content</a>
<script>(function(){try{const e=localStorage.getItem("theme");e==="dark"&&document.documentElement.classList.add("dark");const t=document.querySelector('button[aria-label="Toggle theme"]');t&&t.setAttribute("aria-pressed",String(e==="dark"))}catch{}})();function toggleTheme(e){const s=document.documentElement,t=s.classList.toggle("dark");try{localStorage.setItem("theme",t?"dark":"light")}catch{}try{var n=e&&e.nodeType===1?e:document.querySelector('button[aria-label="Toggle theme"]');n&&n.setAttribute("aria-pressed",String(!!t))}catch{}}(function(){function e(){try{return document.documentElement.classList.contains("dark")?"dark":"light"}catch{return"light"}}function n(t){const n=document.getElementById("i98aca2"),s=document.getElementById("iad2af0"),o=document.getElementById("i975fb5");if(!n||!s||!o)return;try{n.style.transform="translateX(0)",n.style.transition||(n.style.transition="transform 200ms ease-out")}catch{}try{s.hidden=!1,s.style.display="block"}catch{}o.setAttribute("aria-expanded","true"),n.setAttribute("aria-hidden","false");try{document.body.classList.add("c150bbe")}catch{}const i=document.getElementById("i190984");i&&i.focus();try{window.umami&&typeof window.umami.track=="function"&&window.umami.track("mobile_menu_open",{page:location.pathname,theme:e(),source:t||"programmatic"})}catch{}}function t(t){const n=document.getElementById("i98aca2"),s=document.getElementById("iad2af0"),o=document.getElementById("i975fb5");if(!n||!s||!o)return;try{n.style.transform="translateX(100%)",n.style.transition||(n.style.transition="transform 200ms ease-out")}catch{}try{s.hidden=!0,s.style.display="none"}catch{}o.setAttribute("aria-expanded","false"),n.setAttribute("aria-hidden","true");try{document.body.classList.remove("c150bbe")}catch{}o.focus();try{window.umami&&typeof window.umami.track=="function"&&window.umami.track("mobile_menu_close",{page:location.pathname,theme:e(),source:t||"programmatic"})}catch{}}function s(e){e.key==="Escape"&&t("escape")}window.__openMobileMenu=n,window.__closeMobileMenu=t;try{window.addEventListener("keydown",s,!0)}catch{}})()</script><header class="cd019ba c98dfae cdd44dd cfdda01 c9ee25d ce2dc7a cd72dd7 cc0dc37" role=banner><div class="cfdda01 c6942b3 ccf47f4 c7c11d8"><a href=/ class="c87e2b0 c6942b3 c7c11d8 c1838fa cb594e4" aria-label=Home><img src=/static/assets/favicon/favicon.svg alt=Logo width=32 height=32 class="c3de71a c4d5191">
<span class="cf8f011 c4d1253 cbd72bc cd7e69e">Leonardo Benicio</span></a><div class="c6942b3 c85cbd4 c7c11d8 ca798da c1838fa c7a0580"><nav class="cc1689c cd9b445 c75065d c04bab1" aria-label=Main><a href=/ class="c4d1253 c9e4539 cbbda39 c01f421 c19ee42 c3ecea6">Home</a>
<a href=https://lbenicio.dev/about target=_blank rel="noopener noreferrer" class="c4d1253 c9e4539 cbbda39 c01f421 c19ee42 c3ecea6">About</a>
<a href=https://lbenicio.dev/timeline target=_blank rel="noopener noreferrer" class="c4d1253 c9e4539 cbbda39 c01f421 c19ee42 c3ecea6">Timeline</a>
<a href=https://lbenicio.dev/reading target=_blank rel="noopener noreferrer" class="c4d1253 c9e4539 cbbda39 c01f421 c19ee42 c3ecea6">Reading</a>
<a href=https://publications.lbenicio.dev target=_blank rel="noopener noreferrer" class="c4d1253 c9e4539 cbbda39 c01f421 c19ee42 c3ecea6">Publications</a>
<a href=https://lbenicio.dev/contact target=_blank rel="noopener noreferrer" class="c4d1253 c9e4539 cbbda39 c01f421 c19ee42 c3ecea6">Contact</a></nav><button id="i1d73d4" type=button class="c1d6c20 c81ac7c c6a899b c7c11d8 c1d0018 c10dda9 c8e184d c514027 c88daee c7a66a6 c097fa1 cfc01c7 c286dd7 c2bd687 cfdce1d cfef18f" onclick=toggleTheme(this) aria-label="Toggle theme" aria-pressed=false title="Toggle theme">
<svg class="cb26e41 c50ceea cb69a5c c4f45c8 c8c2c40" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" aria-hidden="true"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg>
<svg class="cb26e41 c8fca2b cb69a5c c4f45c8 cc1689c c9c27ff" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" aria-hidden="true"><circle cx="12" cy="12" r="4"/><path d="M12 2v4"/><path d="M12 18v4"/><path d="M2 12h4"/><path d="M18 12h4"/><path d="M4.93 4.93l2.83 2.83"/><path d="M16.24 16.24l2.83 2.83"/><path d="M6.34 17.66l2.83-2.83"/><path d="M14.83 9.17l2.83-2.83"/></svg>
<span class="cba5854">Toggle theme</span></button><div class="c658bcf c097fa1"><button id="i975fb5" type=button class="c1d6c20 c81ac7c c6a899b c7c11d8 c1d0018 c10dda9 c8e184d c514027 c88daee c7a66a6 cfc01c7 c286dd7 c2bd687 cfdce1d cfef18f" aria-label="Open menu" aria-controls="i98aca2" aria-expanded=false onclick='window.__openMobileMenu("button")' data-d38f920=mobile_menu_open_click>
<svg class="c20e4eb cb58471" fill="none" viewBox="0 0 24 24" stroke="currentColor"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M4 6h16M4 12h16M4 18h16"/></svg>
<span class="cba5854">Open menu</span></button></div></div></div></header><div id="iad2af0" class="caffa6e ce4b5f4 c14639a" style=background-color:hsl(var(--background)) hidden onclick='window.__closeMobileMenu("overlay")' data-d38f920=mobile_menu_overlay_click></div><aside id="i98aca2" class="caffa6e c9efbc5 c437fa9 c49e97e c6c6936 c7cacca c7b34a4 c787e9b c88daee cad071a c6942b3 c03620d" role=dialog aria-modal=true aria-hidden=true aria-label="Mobile navigation" style="transform:translateX(100%);transition:transform 200ms ease-out;will-change:transform"><div class="c6942b3 c7c11d8 c82c52d c5df473 ccf47f4 c9ee25d"><a href=/ class="c6942b3 c7c11d8 c1838fa" aria-label=Home><img src=/static/assets/favicon/favicon.svg alt=Logo width=24 height=24 class="c20e4eb cb58471">
<span class="c62aaf0 c7c1b66 cbd72bc">Leonardo Benicio</span>
</a><button id="i190984" type=button class="c1d6c20 c81ac7c c6a899b c7c11d8 c1d0018 c10dda9 c514027 c286dd7 c2bd687 cfdce1d" aria-label="Close menu" onclick='window.__closeMobileMenu("button")' data-d38f920=mobile_menu_close_click>
<svg class="c16e528 c61f467" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M18 6 6 18"/><path d="m6 6 12 12"/></svg>
<span class="cba5854">Close</span></button></div><nav class="c85cbd4 ca0eaa4 c5df473 c6689b9"><ul class="cd69733"><li><a href=/ class="c3b5299 c10dda9 cddc2d2 cf55a7b c7c1b66 cbbda39 c3ecea6 c19ee42 c514027" onclick=window.__closeMobileMenu()>Home</a></li><li><a href=https://lbenicio.dev/about target=_blank rel="noopener noreferrer" class="c3b5299 c10dda9 cddc2d2 cf55a7b c7c1b66 cbbda39 c3ecea6 c19ee42 c514027" onclick=window.__closeMobileMenu()>About</a></li><li><a href=https://lbenicio.dev/timeline target=_blank rel="noopener noreferrer" class="c3b5299 c10dda9 cddc2d2 cf55a7b c7c1b66 cbbda39 c3ecea6 c19ee42 c514027" onclick=window.__closeMobileMenu()>Timeline</a></li><li><a href=https://lbenicio.dev/reading target=_blank rel="noopener noreferrer" class="c3b5299 c10dda9 cddc2d2 cf55a7b c7c1b66 cbbda39 c3ecea6 c19ee42 c514027" onclick=window.__closeMobileMenu()>Reading</a></li><li><a href=https://publications.lbenicio.dev target=_blank rel="noopener noreferrer" class="c3b5299 c10dda9 cddc2d2 cf55a7b c7c1b66 cbbda39 c3ecea6 c19ee42 c514027" onclick=window.__closeMobileMenu()>Publications</a></li><li><a href=https://lbenicio.dev/contact target=_blank rel="noopener noreferrer" class="c3b5299 c10dda9 cddc2d2 cf55a7b c7c1b66 cbbda39 c3ecea6 c19ee42 c514027" onclick=window.__closeMobileMenu()>Contact</a></li></ul></nav><div class="c60a4cc ccdf0e8 c277478 c13044e"><p>&copy; 2026 Leonardo Benicio</p></div></aside><div class="caffa6e c437fa9 ce9aced c97bba6 c15da2a c975cba" role=complementary aria-label="GitHub repository"><div class="c9d056d c252f85 ca22532 ca88a1a c876315"><div class="c6942b3 c7c11d8 c1d0018 cd1fd22 c6066e4 c43876e ce3d5b6 caa20d2 c3ecea6 c0cd2e2 cddc2d2 c3ed5c9 cd4074c c876315"><a href=https://github.com/lbenicio/aboutme target=_blank rel="noopener noreferrer" class="c6942b3 c7c11d8 cd1fd22 c71bae8 cfac1ac c19ee42 c25dc7c cb40739 cbbda39 cf55a7b" aria-label="View source on GitHub"><svg width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="cb26e41 c41bcd4 cf17690 cfa4e34 c78d562" aria-hidden="true"><path d="M15 22v-4a4.8 4.8.0 00-1-3.5c3 0 6-2 6-5.5.08-1.25-.27-2.48-1-3.5.28-1.15.28-2.35.0-3.5.0.0-1 0-3 1.5-2.64-.5-5.36-.5-8 0C6 2 5 2 5 2c-.3 1.15-.3 2.35.0 3.5A5.403 5.403.0 004 9c0 3.5 3 5.5 6 5.5-.39.49-.68 1.05-.85 1.65-.17.6-.22 1.23-.15 1.85v4"/><path d="M9 18c-4.51 2-5-2-7-2"/></svg>
<span class="cb5c327 cd7e69e">Fork me</span></a></div></div></div><main id="i7eccc0" class="cfdda01 c5df473 c0eecc8 c85cbd4" role=main aria-label=Content><nav class="cb545ce c8d8ae4 c277478" aria-label=Breadcrumb><ol class="c6942b3 c3adaf2 c7c11d8 cd365ee c3ecea6"><li><a href=/ class="c19ee42 c71bae8 cfac1ac">Home</a></li><li class="c6942b3 c7c11d8 cd365ee"><span class="cb82ddd">/</span><a href=/ class="c19ee42 c71bae8 cfac1ac">Blog</a></li><li class="c6942b3 c7c11d8 cd365ee"><span class="cb82ddd">/</span><span class="c88daee">Distributed Systems Consensus Consistency and Fault Tolerance</span></li></ol></nav><article class="c461ba0 c1c203f cfb6084 c995404 c6ca165"><nav class="cb545ce c8d8ae4 c277478" aria-label=Breadcrumb><ol class="c6942b3 c3adaf2 c7c11d8 cd365ee c3ecea6"><li><a href=/ class="c19ee42 c71bae8 cfac1ac">Home</a></li><li class="c6942b3 c7c11d8 cd365ee"><span class="cb82ddd">/</span><a href=/ class="c19ee42 c71bae8 cfac1ac">Blog</a></li><li class="c6942b3 c7c11d8 cd365ee"><span class="cb82ddd">/</span><span class="c88daee">Distributed Systems Consensus Consistency and Fault Tolerance</span></li></ol></nav><header class="c8aedc7"><h1 class="cf304bc c6fb0fe cf8f011 cc484e1">Distributed Systems: Consensus, Consistency, and Fault Tolerance</h1><div class="c277478 c3ecea6 c8fb24a">2025-10-20
· Leonardo Benicio</div><div class="c1a1a3f c8124f2"><img src=/static/assets/images/blog/distributed-systems-consensus-cap-gossip.png alt class="cfdda01 c524300 c677556"></div><p class="lead c3ecea6">Fundamentals of distributed systems: failure models, consensus algorithms (Paxos, Raft), CAP theorem, consistency models, gossip, membership, CRDTs, and practical testing strategies like Jepsen.</p></header><div class="content"><p>Distributed systems are deceptively simple to describe and maddeningly difficult to build. When a single process is replaced by a collection of cooperating processes that communicate over unreliable networks, familiar assumptions break: partial failure becomes the norm, time is not globally synchronized, and correctness requires making explicit trade-offs. This post covers the foundations you need to reason about building correct, robust, and performant distributed systems: failure models, consensus and leader election, replication and consistency models, gossip and membership, CRDTs for conflict-free replication, testing strategies (including Jepsen-style fault injection), and operational best practices.</p><h2 id="1-definitions-and-failure-models">1. Definitions and failure models</h2><p>Before designing systems, be explicit about the failure assumptions.</p><h3 id="11-process-failure-modes">1.1 Process failure modes</h3><ul><li>Crash-stop: Node halts and does not recover without external action. This is the simplest failure model and the basis for many algorithms.</li><li>Crash-recovery: Node can crash and later rejoin, possibly with partial state or after a restart. Requires durable logs for recovery.</li><li>Byzantine failures: Arbitrary (malicious or arbitrary) behavior — requires specialized protocols (PBFT, quorum systems with signatures).</li></ul><p>Most practical systems assume crash-recovery or crash-stop, and treat Byzantine failures as out-of-scope.</p><h3 id="12-network-failures">1.2 Network failures</h3><ul><li>Packet loss: Messages dropped due to congestion or errors.</li><li>Delay and reordering: Messages may arrive late or in a different order.</li><li>Partition: Network splits the cluster into disjoint sets that cannot communicate.</li></ul><p>Design for asynchrony: don&rsquo;t assume bounded message delivery times; instead design protocols that make progress when the network restores connectivity.</p><h3 id="13-timing-assumptions">1.3 Timing assumptions</h3><ul><li>Synchronous model: Bounded message delay and processing time. Strong but unrealistic in many environments.</li><li>Eventually synchronous model: System may be asynchronous for some time but becomes synchronous later (used by many consensus proofs).</li><li>Asynchronous model: No timing guarantees—used for impossibility results like FLP.</li></ul><p>The FLP impossibility result states that in a purely asynchronous system with even a single crash failure, deterministic consensus cannot be guaranteed. Practical consensus algorithms therefore rely on randomness, timeouts, and leader-based optimizations to make progress in the common case.</p><h2 id="2-safety-vs-liveness-and-the-cap-theorem">2. Safety vs Liveness and the CAP theorem</h2><p>Two fundamental properties when designing distributed algorithms:</p><ul><li>Safety: &ldquo;Nothing bad happens&rdquo; — e.g., strong consistency properties like linearizability or serializability.</li><li>Liveness: &ldquo;Something good eventually happens&rdquo; — e.g., the system makes progress (commits, responds).</li></ul><p>Designs trade these properties: during partitions, systems might sacrifice liveness for safety (reject writes) or vice versa (accept writes but risk conflicts/lost updates).</p><p>CAP theorem (Brewer): In the presence of a network partition, a distributed system must choose between consistency (C) and availability (A). This often maps to practical trade-offs:</p><ul><li>CP systems: Maintain consistency, reject or block requests during partitions (e.g., leader-based Raft with strict quorums).</li><li>AP systems: Allow reads/writes during partitions but need reconciliation/merge strategies (eventual consistency, CRDTs).</li></ul><p>CAP is a coarse guideline — modern systems reason in terms of stronger consistency models (linearizability, causal consistency) and more nuanced availability metrics (latency percentiles, staleness windows).</p><h2 id="3-replication-strategies-and-consistency-models">3. Replication strategies and consistency models</h2><p>Replication gives durability and scale; how you replicate dictates consistency semantics.</p><h3 id="31-primary-replica-leader-replication">3.1 Primary-replica (leader) replication</h3><ul><li>One node acts as leader (primary) and serializes writes.</li><li>Followers replicate the leader&rsquo;s log and serve reads either synchronously or with lag.</li></ul><p>Pros:</p><ul><li>Simple to reason about for strong consistency (leader linearizes writes).</li><li>Efficient single-writer paths with batching.</li></ul><p>Cons:</p><ul><li>Leader is a single point of write contention and potential availability bottleneck.</li><li>Failover requires leader election and catch-up.</li></ul><h3 id="32-leaderless-replication-gossipquorum">3.2 Leaderless replication (gossip/quorum)</h3><ul><li>Writes are sent to multiple replicas and considered successful when a quorum ack is received.</li><li>Systems like Dynamo-style designs use vector clocks or logical timestamps to detect and reconcile conflicts.</li></ul><p>Pros:</p><ul><li>High availability and lower write latency if quorum is small.</li><li>No single leader bottleneck.</li></ul><p>Cons:</p><ul><li>Conflict resolution often pushed to application or via anti-entropy.</li><li>Harder to provide strong semantics like linearizability.</li></ul><h3 id="33-consistency-models-overview">3.3 Consistency models overview</h3><ul><li>Linearizability: Single global order respecting real-time. Strongest model for single-object operations.</li><li>Sequential consistency: Operations appear in some global order consistent across processes, but may not respect real-time.</li><li>Causal consistency: Preserves causality; concurrent writes can be seen in different orders if causally independent.</li><li>Eventual consistency: If no new updates, replicas eventually converge.</li></ul><p>Most production systems choose a middle ground. For example, Spanner provides external consistency (linearizability across distributed transactions) using synchronized clocks; Cassandra offers tunable consistency (quorum reads/writes) with tunable latency/consistency tradeoffs.</p><h2 id="4-consensus-and-leader-election">4. Consensus and leader election</h2><p>A central primitive for consistent replication is consensus: agreeing on a value (e.g., the next log entry) across a group of nodes despite failures.</p><h3 id="41-consensus-problem">4.1 Consensus problem</h3><p>Consensus requires three properties:</p><ul><li>Agreement: All non-faulty processes agree on the same value.</li><li>Validity: If all propose the same value, that value is chosen.</li><li>Termination (Liveness): All non-faulty processes eventually decide.</li></ul><p>Practical consensus algorithms implement safety at all times and aim for liveness under stable leader conditions.</p><h3 id="42-paxos-single-decree-and-multi-paxos">4.2 Paxos (single-decree and multi-Paxos)</h3><p>Paxos (Lamport) describes a protocol for achieving consensus via prepare and accept phases. Multi-Paxos amortizes leader election costs: after a stable leader is elected, the leader can propose a sequence of commands with fewer round trips.</p><p>Key ideas:</p><ul><li>Phases: Proposer sends Prepare(n); Acceptors respond with promises including any previously accepted value; if the proposer gets a majority it sends Accept(n, value); acceptors accept and persist the value.</li><li>Unique proposal numbers ensure newer proposals supersede older ones.</li></ul><h3 id="421-paxos-walk-through-and-multi-paxos-optimization">4.2.1 Paxos walk-through and multi-Paxos optimization</h3><p>Walk-through (single-decree Paxos):</p><ol><li>Proposer P chooses a proposal number n and sends Prepare(n) to all Acceptors.</li><li>Each Acceptor that hasn&rsquo;t promised a higher-numbered proposal replies with a Promise(n) and includes any previously accepted value with the highest proposal number.</li><li>If P receives promises from a majority, it selects the value with the highest-numbered accepted proposal (if any), or its own value otherwise, and sends Accept(n, value) to the Acceptors.</li><li>Acceptors persist the accepted value and reply Accepted.</li><li>Once a majority of Acceptors have accepted, the value is chosen and learners are informed.</li></ol><p>Multi-Paxos optimization:</p><ul><li>A stable leader can skip the Prepare phase for subsequent instances and directly send Accept(leader_term, value) for new slots, greatly reducing the number of message RTTs for each decision.</li><li>Leader changes still require a Prepare phase to re-establish safety.</li></ul><p>Practical pitfalls:</p><ul><li>Implementations must carefully handle leader changes, dropped messages, and duplicates.</li><li>Garbage-collecting old instances and snapshotting state are necessary for practical deployments.</li></ul><h3 id="422-paxos-failure-scenario-split-acceptors">4.2.2 Paxos failure scenario (split acceptors)</h3><p>A simple failure mode illustrates why majority quorums are required:</p><ol><li>Proposer P1 issues Prepare(1) and gets promises from a majority including acceptor set {A, B, C}.</li><li>P1 sends Accept(1, v1) and gets acks from {A, B} (majority) — v1 chosen.</li><li>Later, P2 with higher number 2 sends Prepare(2) but due to network conditions only reaches acceptor C and D (not a majority), so it doesn&rsquo;t get sufficient promises to proceed.</li><li>P2 retries or times out and may cause more message exchange, but because P1&rsquo;s value v1 was accepted by a majority, any proposer that obtains a majority of promises must use v1 as its chosen value — Paxos safety prevents two different values from both being chosen.</li></ol><p>Lessons:</p><ul><li>Progress requires liveness assumptions (leaders getting majority support).</li><li>Handling minority partitions requires timeouts and retries; multi-Paxos reduces per-instance overhead once a stable leader is present.</li></ul><p>Although Paxos is the theoretical foundation, many production systems prefer Raft or Paxos variants that explicitly encode leader behavior to simplify implementation and reasoning.</p><h3 id="43-raft-understandable-consensus">4.3 Raft: understandable consensus</h3><p>Raft rephrases consensus with clearer invariants: leader election, log replication, and membership changes.</p><p>Components:</p><ul><li>Terms: Incrementing epochs used to ensure a single leader per term.</li><li>Leader election: Followers vote for a candidate during election timeouts.</li><li>Log replication: Leader appends and replicates entries, commits them when a quorum acknowledges.</li><li>Safety: Raft ensures a leader&rsquo;s log contains all committed entries by ensuring leaders are up-to-date during election.</li></ul><p>Raft gained adoption due to its readability and robust reference implementations (etcd, Consul).</p><h3 id="44-raft-internals-leader-state-replication-and-commit-rules">4.4 Raft internals: leader state, replication, and commit rules</h3><p>A deeper look at the data structures and invariants that make Raft work in practice.</p><p>Leader state (per follower):</p><ul><li>nextIndex[f] : The index of the next log entry the leader will send to follower f.</li><li>matchIndex[f] : The highest log index known to be replicated on follower f.</li></ul><p>Replication algorithm (simplified):</p><ol><li>Leader receives client command and appends entry to its local log at index i.</li><li>Leader sends AppendEntries RPCs to followers, including the previous log index/term and the new entries.</li><li>Follower validates the prevLogIndex/prevLogTerm; if it matches its local log, it appends entries and replies success.</li><li>On successful replication to a majority, the leader updates matchIndex and can consider the entry committed.</li></ol><p>Commit rule:</p><ul><li>A leader may mark an entry at index i as committed when i is stored on a majority of servers and the entry is in the leader&rsquo;s current term (this avoids committing entries from previous terms that could cause safety issues during leader changes).</li><li>Once committed, the leader applies the entry to the state machine and returns success to the client.</li></ul><p>Pipelined replication and batch append:</p><ul><li>Leaders send multiple AppendEntries in flight to keep followers&rsquo; IO busy.</li><li>Batching many small client requests into one log append amortizes per-request overhead and fsync costs.</li></ul><p>Snapshotting and log compaction:</p><ul><li>When the log grows large, the leader can take a snapshot of the current state machine and discard old log entries up to the snapshot index.</li><li>Followers that are far behind can install snapshots rather than fetch long histories.</li></ul><p>Failure scenario (example):</p><ol><li>Leader L appends entries 101–110 but a network partition prevents L from replicating them to a majority.</li><li>A new leader L2 is elected from a partition containing a majority whose logs end at index 100.</li><li>Once L2 becomes leader, L&rsquo;s entries 101–110 are not considered committed; L will either be deposed or its entries will be overwritten when it rejoins and syncs with the new leader&rsquo;s log.</li></ol><p>Recovery and durability:</p><ul><li>Logs must be persisted to stable storage before acknowledging a commit if you require durability across crashes.</li><li>Snapshotting reduces recovery time by allowing a restarted node to install the latest snapshot and then fetch only subsequent entries.</li></ul><h3 id="45-optimizations-and-practicalities">4.5 Optimizations and practicalities</h3><ul><li>Leader stickiness and leases minimize elections by keeping the same leader active while the network is healthy.</li><li>Batching entries and using pipelined replication improves throughput by keeping disks and network saturated.</li><li>Log compaction snapshots reduce storage of historical entries and accelerate recovery for newly promoted or restarted replicas.</li></ul><h3 id="46-raft-timeline-example-detailed">4.6 Raft timeline example (detailed)</h3><p>A concrete timeline helps understand corner cases. Consider a cluster A, B, C with A as leader.</p><p>T0: A&rsquo;s current term is t. A has committed entries up to index 100.</p><p>T1: Client writes entry 101 to leader A. A appends it to its log and sends AppendEntries(101) to B and C.</p><p>T2: Network partition prevents messages to B; C receives and appends 101 and replies success. A receives reply from C and has majority (A and C) — it marks 101 committed and applies it to the state machine, returning success to client.</p><p>T3: A becomes isolated from majority and is partitioned alone.</p><p>T4: B and C remain connected. B times out, becomes candidate for term t+1, and requests votes. C votes for B if B&rsquo;s log is at least as up-to-date as C&rsquo;s. If B gets majority, B becomes leader and starts appending entries at term t+1.</p><p>T5: Because A&rsquo;s entry 101 was committed on a majority (A and C) when A was leader, safety requires that any future leader&rsquo;s log contains 101; the election protocol ensures this because C, which has 101, will only vote for candidates with logs at least as up-to-date.</p><p>Key takeaways:</p><ul><li>Committing requires majority replication; once committed, entries survive leader changes.</li><li>Partition-tolerant designs prevent two different values from being committed simultaneously by enforcing majority quorums and log-up-to-date checks during elections.</li></ul><h2 id="5-failure-detectors-and-membership">5. Failure detectors and membership</h2><p>Consensus and replication rely on accurate membership information. Failure detection is inherently unreliable in asynchronous systems and requires careful design.</p><h3 id="51-heartbeating-and-accrual-failure-detectors">5.1 Heartbeating and accrual failure detectors</h3><ul><li>Simple detector: Missing heartbeats after a timeout signals a failure (but timeouts are unreliable in overloaded networks).</li><li>Phi accrual detector: Computes a suspicion level (phi) based on historical heartbeat inter-arrival times; offers tunable sensitivity.</li></ul><p>Trade-offs:</p><ul><li>Aggressive timeouts: Fast detection but higher false positives (causing unnecessary elections).</li><li>Conservative timeouts: Fewer false positives but slower to react to real failures.</li></ul><h3 id="52-membership-changes-and-reconfiguration">5.2 Membership changes and reconfiguration</h3><ul><li>Replacing nodes must preserve quorum invariants; joint-consensus (as in Raft) transitions membership safely.</li><li>Rolling upgrades require careful sequencing to avoid violating majority requirements.</li></ul><h2 id="6-anti-entropy-and-gossip-protocols">6. Anti-entropy and gossip protocols</h2><p>Gossip-based protocols provide scalable dissemination, membership, and anti-entropy (eventual convergence) in large clusters.</p><h3 id="61-gossip-basics">6.1 Gossip basics</h3><ul><li>Nodes periodically select random peers and exchange state digests (e.g., vector clocks, version vectors, hash summaries).</li><li>These pairwise exchanges eventually spread updates to entire cluster (probabilistic guarantees).</li></ul><p>Advantages:</p><ul><li>Scales to thousands of nodes with gentle load distribution.</li><li>Resilient to partial failures and network churn.</li></ul><p>Disadvantages:</p><ul><li>Probabilistic convergence time and potential temporary inconsistency.</li></ul><h3 id="62-anti-entropy-and-merkle-trees">6.2 Anti-entropy and Merkle trees</h3><ul><li>Merkle trees allow efficient detection of differences between replicas by comparing hash roots and recursing into divergent subtrees.</li><li>Widely used in distributed databases and peer-to-peer systems for efficient synchronization.</li></ul><h2 id="7-causality-vector-clocks-and-versioning">7. Causality, vector clocks, and versioning</h2><p>Tracking causality helps with conflict resolution and determining whether two updates are concurrent.</p><h3 id="71-vector-clocks">7.1 Vector clocks</h3><ul><li>Each node maintains a vector of counters; when sending an event, it includes a copy.</li><li>Merge and compare operations determine causal relationships: a ≤ b if every component is ≤.</li></ul><p>Vector clocks solve causality detection but grow in size with number of participants; practical systems often use compacted or approximate versions.</p><p>Example:</p><ul><li><p>Three nodes A, B, C start with vectors [0,0,0].</p></li><li><p>A writes x → A increments its counter: A: [1,0,0], sends event with vector [1,0,0].</p></li><li><p>B reads x (gets vector [1,0,0]) then writes y → increments its counter: B: [1,1,0] (merging read vector), sends event with [1,1,0].</p></li><li><p>C concurrently writes z without seeing A or B: C: [0,0,1].</p></li></ul><p>Comparisons:</p><ul><li>[1,1,0] and [0,0,1] are concurrent (neither ≤ the other); conflict detected.</li></ul><p>Practical considerations:</p><ul><li>Vector clocks must be compacted (pruning old entries or using coarse-grained membership) to avoid unbounded growth.</li><li>In large clusters, CRDTs or HLCs are often preferred for scalability.</li></ul><h3 id="72-hybrid-logical-clocks">7.2 Hybrid logical clocks</h3><ul><li>HLCs combine physical time with logical counters to bound skew and provide causality without keeping full vectors. Useful for systems that require causality plus compactness (e.g., Spanner/HLC variants).</li></ul><h2 id="8-conflict-free-replicated-data-types-crdts">8. Conflict-free Replicated Data Types (CRDTs)</h2><p>CRDTs provide deterministic, mergeable data types that converge without coordination.</p><ul><li>State-based (convergent) CRDTs: Each replica periodically sends its full state; merging is commutative, associative, and idempotent.</li><li>Operation-based (commutative) CRDTs: Send operations that are guaranteed to commute under delivery order assumptions.</li></ul><p>Examples:</p><ul><li>G-Counters (grow-only), PN-Counters (positive/negative), LWW-Register (last-writer-wins), OR-Set (observed-remove set).</li></ul><h3 id="73-or-set-observed-remove-set-example">7.3 OR-Set (Observed-Remove Set) example</h3><p>OR-Set stores add and remove operations with unique tags so removes only affect observed adds. A simple implementation:</p><p>State example:</p><ul><li><code>adds</code>: map from element → set of tags (e.g., <code>adds['x'] = {t1, t2}</code>)</li><li><code>removes</code>: map from element → set of tags observed at remove time</li></ul><p>Operations:</p><ul><li><code>add(e)</code>: generate unique tag <code>t</code>, do <code>adds[e].add(t)</code></li><li><code>remove(e)</code>: <code>removes[e] |= adds[e]</code> (record tags observed at remove time)</li><li><code>lookup(e)</code>: present if <code>adds[e] \ removes[e] != ∅</code></li></ul><p>Merge (state-based): union the <code>adds</code> and <code>removes</code> maps (element-wise union of tag sets). The set converges because unions are commutative, associative, and idempotent.</p><p>Example:</p><ul><li>Replica A: <code>add(x)</code> with tag <code>t1</code> → <code>adds[x] = {t1}</code></li><li>Replica B: <code>add(x)</code> with tag <code>t2</code> → <code>adds[x] = {t2}</code></li><li>If B removes <code>x</code> before seeing <code>t1</code>: <code>removes[x] = {t2}</code></li><li>Merge: <code>adds[x] = {t1,t2}</code>, <code>removes[x] = {t2}</code> → present because <code>{t1,t2} \ {t2} = {t1}</code></li></ul><p>OR-Sets allow removes without coordination and avoid classic lost-delete anomalies.</p><p>CRDTs are powerful when you need high availability and eventual consistency without complex conflict resolution.</p><h2 id="9-distributed-transactions-and-atomic-commit">9. Distributed transactions and atomic commit</h2><p>Transactions spanning multiple nodes are expensive and require careful protocols.</p><h3 id="91-two-phase-commit-2pc">9.1 Two-Phase Commit (2PC)</h3><ul><li>Coordinator asks participants to prepare and persist the prepared state (voting phase).</li><li>If all vote yes, coordinator sends commit; otherwise, abort.</li></ul><p>2PC blocks participants waiting for the global decision if coordinator fails — several variants and optimizations exist, including presumed commit/abort and participant involvement in coordinator recovery.</p><h3 id="911-transaction-commit-path-wal-replication-and-durability">9.1.1 Transaction commit path (WAL, replication, and durability)</h3><p>A typical distributed commit path with WAL and replication looks like this:</p><ol><li>Client submits transaction to coordinator which assigns a transaction id and begins distribution.</li><li>Coordinator sends prepare requests to participants; each participant writes a &ldquo;prepare&rdquo; record to its WAL and fsyncs to durable storage before replying prepared.</li><li>When coordinator receives prepared ACKs from a quorum (or all, depending on policy), it writes a commit record to its WAL and broadcasts commit messages.</li><li>Participants apply the commit, make changes durable, and acknowledge commit to the coordinator.</li><li>Coordinator returns success to client once durability guarantees are satisfied (either after coordinator WAL flush or after participant commit ACKs per chosen durability semantics).</li></ol><p>Notes:</p><ul><li>Durability depends on where and when fsyncs happen; syncing only the coordinator may lead to data loss if the coordinator crashes before replication completes.</li><li>Group commit and batching reduce latency by amortizing fsync costs across many transactions.</li><li>Optimizations such as &lsquo;presumed commit&rsquo; reduce log records in the common case but complicate recovery bookkeeping.</li></ul><p>Drawbacks and mitigations:</p><ul><li>Participants may remain blocked in prepared state when the coordinator crashes; recovery protocols or coordinator replication can mitigate blocking by electing a recovery coordinator.</li><li>Using consensus (e.g., Raft) to replicate a commit decision reduces single-coordinator blocking at the cost of additional complexity.</li></ul><h3 id="92-three-phase-commit-3pc-and-non-blocking-variants">9.2 Three-Phase Commit (3PC) and non-blocking variants</h3><p>3PC aims to be non-blocking under certain failure assumptions by adding an extra phase and requiring additional timing properties; in practice, it is less commonly used due to complexity and stronger timing assumptions.</p><h3 id="93-distributed-transactions-with-consensus">9.3 Distributed transactions with consensus</h3><ul><li>Some systems implement distributed transactions using Paxos/Raft for ledger replication and use consensus to serialize commit decisions (e.g., Calvin, Spanner&rsquo;s two-phase commit over timestamps).</li><li>Optimistic snapshot isolation and partitioning reduce cross-node coordination.</li></ul><h3 id="94-sagas-and-compensation">9.4 Sagas and compensation</h3><p>Sagas break distributed updates into a sequence of local transactions with compensating actions to undo effects when later steps fail — a pragmatic pattern for long-running workflows.</p><h2 id="10-testing-and-fault-injection">10. Testing and fault injection</h2><p>Reality is harsh: adopt chaos engineering and formal tests to gain confidence.</p><h3 id="101-jepsen-style-testing">10.1 Jepsen-style testing</h3><p>Jepsen injects network partitions, process kills, and clock skew and verifies correctness properties (linearizability, snapshot isolation) under stress. It has uncovered subtle bugs in many distributed systems.</p><h3 id="102-model-checking-and-systematic-exploration">10.2 Model checking and systematic exploration</h3><ul><li>Tools like TLA+, PlusCal, and model checkers can validate protocols under exhaustive interleavings.</li><li>State space explosion limits coverage, but these tools catch design-level errors early.</li></ul><h3 id="103-chaos-engineering-and-resilience-testing">10.3 Chaos engineering and resilience testing</h3><ul><li>Run fault injection in production-like environments: simulate partitions, disk faults, and slow networks.</li><li>Observe system behavior, failure modes, and recovery procedures; automate and monitor rollbacks.</li></ul><h2 id="11-practical-optimizations-and-performance">11. Practical optimizations and performance</h2><p>Real systems add engineering to make consensus and replication practical at scale.</p><h3 id="111-batch-pipeline-and-leader-batching">11.1 Batch, pipeline, and leader batching</h3><ul><li>Aggregate multiple client requests into single log entries to amortize per-request overhead.</li><li>Pipeline log replication to keep network and disk busy.</li></ul><h3 id="112-log-compaction-and-snapshotting">11.2 Log compaction and snapshotting</h3><ul><li>Periodic snapshots of in-memory state and truncation of logs reduce recovery times and disk usage.</li><li>Make snapshots incremental and use copy-on-write techniques to avoid long pauses.</li></ul><h3 id="113-read-optimization">11.3 Read optimization</h3><ul><li>Serve weakly-consistent reads from followers for low-latency operations when freshness isn&rsquo;t critical.</li><li>Use leader lease or read-index mechanisms to serve linearizable reads without extra round trips.</li></ul><h3 id="114-partitioning-consistent-hashing-and-rebalancing">11.4 Partitioning, consistent hashing, and rebalancing</h3><p>Sharding and partitioning allow systems to scale horizontally, but moving data between nodes safely and efficiently is non-trivial.</p><p>Consistent hashing:</p><ul><li>Map keys to a hash space (e.g., 0..2^64-1) and assign nodes to points in the space.</li><li>Each key maps to the nearest node clockwise from its hash position.</li><li>Adding/removing a node only affects keys between the node and its predecessor, reducing data movement compared to range-based sharding.</li></ul><p>Virtual nodes:</p><ul><li>Assign each physical node many virtual nodes (tokens) spread across the hash space to produce better load balance.</li><li>Rebalancing: When a node joins or leaves, only its virtual nodes&rsquo; ranges need migrating.</li></ul><p>Rebalancing strategies:</p><ul><li>Repartitioning via streaming: Move data incrementally and serve reads/writes from both source and destination during handoff.</li><li>Throttling migration: Limit copy bandwidth to avoid interfering with normal traffic.</li><li>Maintaining replication factor: Ensure new replicas are fully caught up before demoting a source replica to avoid data loss.</li></ul><p>Range-based sharding (ordered by key):</p><ul><li>Easier for range queries since contiguous keys map to the same shard.</li><li>Rebalancing requires splitting and moving entire ranges; often done with background copy + switch-over.</li></ul><p>Operational concerns:</p><ul><li>Coordinate rebalancing with load and compaction to avoid overload.</li><li>Use metrics for migratory throughput and lag; alert on stalled rebalances.</li></ul><h2 id="12-observability-debugging-and-runbooks">12. Observability, debugging, and runbooks</h2><p>Metrics, traces, and playbooks are essential for operating distributed systems.</p><h3 id="121-key-observability-signals">12.1 Key observability signals</h3><ul><li>Commit latency, replication lag, leader election rates, and error rates.</li><li>Tail latency (p99/p999) often more important than averages.</li><li>Heartbeat/phi counts for failure detectors and their false-positive rates.</li></ul><h3 id="122-runbooks-and-incident-response">12.2 Runbooks and incident response</h3><ul><li>Define clear runbooks for leader failover, split-brain scenarios, and unsafe reconfigurations.</li><li>Practice recovery steps in staging and runbooks must include rollback imagers, snapshot restores, and safety checks.</li></ul><h2 id="13-real-world-case-studies">13. Real-world case studies</h2><p>A few systems and notable design decisions:</p><ul><li>Apache Kafka: Log-centric architecture with partition leaders, high-throughput replication, and pluggable consistency settings (acks=all/quorum).</li><li>Google Spanner: Global transactions with TrueTime-synchronized timestamps enabling external consistency.</li></ul><h3 id="131-spanner-and-truetime-external-consistency">13.1 Spanner and TrueTime (external consistency)</h3><p>Spanner provides external consistency (a strong form of linearizability across distributed transactions) by relying on tightly synchronized clocks and a time API called TrueTime that returns an interval <code>[earliest, latest]</code> capturing clock uncertainty.</p><p>Key protocol:</p><ul><li>When committing a transaction, Spanner assigns a commit timestamp greater than any read timestamp previously observed and waits until the TrueTime <code>earliest</code> exceeds that timestamp (or specifically until <code>latest &lt; commit_timestamp</code>), ensuring no causally later events can have an earlier timestamp — this wait is the commit-wait, and it uses physical time to make transactions appear serialized in global time.</li></ul><p>Trade-offs:</p><ul><li>Achieves very strong consistency semantics, simplifying application reasoning at the cost of increased commit latency (the commit-wait) and the operational burden of maintaining low clock uncertainty (GPS/atomic clocks or specialized synchronization).</li><li>If clock uncertainty is large, commit-waits increase and throughput can suffer.</li></ul><p>Hybrid logical clocks (HLC) and other techniques attempt to approximate some of these guarantees with less operational complexity, but TrueTime provides a clean model for external consistency when you can invest in clock infrastructure.</p><ul><li><p>Cassandra: Tunable consistency with gossip-based membership and hinted handoff for temporary failures.</p></li><li><p>etcd/Consul: Use Raft for strong consistency and leader-based coordination for service discovery and configuration storage.</p></li></ul><h2 id="14-checklist-and-best-practices">14. Checklist and best practices</h2><p>Quick checklist for architects and operators:</p><ul><li>Define your consistency SLA and failure model explicitly.</li><li>Choose replication style (leader vs leaderless) based on workload and operational complexity.</li><li>Use consensus for metadata and configuration — avoid using distributed consensus for every write unless necessary.</li><li>Adopt formal spec/model checking for core consensus and membership protocols.</li><li>Run chaos experiments and Jepsen-style tests regularly.</li><li>Monitor tail latencies, election rates, and long-running heartbeats.</li><li>Automate backups, snapshot restores, and recovery drills.</li></ul><h2 id="15-worked-example-partition-with-conflicting-writes-timeline-and-resolution">15. Worked example: partition with conflicting writes (timeline and resolution)</h2><p>Consider a 3-node cluster A, B, C configured with Raft (leader-based) and a client performing writes:</p><ol><li>Leader is A. Client writes key k -> A appends entry and replicates to B, C, but due to a network partition A cannot reach B, only C.</li><li>A replicates to C and receives a majority ack (A and C), commits the entry and replies success to the client.</li><li>A&rsquo;s link to the majority is severed, and B and C form a partition where C is reachable to B and elect C as new leader (if B has higher logs); or if B and C can&rsquo;t form a majority, leader election may stall.</li><li>If C becomes leader and a client writes a conflicting value for k to C, that value may be committed on the other partition depending on quorum; when partitions heal, Raft&rsquo;s log safety ensures only one value is committed across a majority — conflicting entries from deposed leaders are overwritten.</li></ol><p>How different systems handle this:</p><ul><li>CP (Raft/Paxos): Enforce majority quorums for commits. A write committed by majority cannot be revoked; split-brain scenarios where both sides think they&rsquo;re leaders are prevented by quorum rules and election semantics.</li><li>AP (Dynamo-style): Writes can be accepted in both partitions (if quorum requirement is relaxed), creating concurrent versions that must be reconciled via vector clocks, last-writer-wins, or application-specific merging.</li><li>CRDT approach: Use commutative data types so concurrent updates automatically converge.</li></ul><p>Resolution steps and runbook:</p><ol><li>Identify which entries are committed by checking the leader&rsquo;s commit index and replication state on a majority.</li><li>If conflicting replicas exist, prefer the leader with the highest term/commit index or run a repair process that reconciles last-applied states.</li><li>If data loss risk exists (writes accepted on minority), inform affected clients and run reconciliation if necessary.</li></ol><h2 id="16-jepsen-style-case-a-typical-bug-narrative">16. Jepsen-style case: a typical bug narrative</h2><p>A typical Jepsen discovery path:</p><ol><li>Inject partitions and delayed fsyncs while running a workload with a mix of reads and writes and a client assertion for linearizability.</li><li>Observe a linearizability violation: A read returns a value earlier than a write that was supposedly committed.</li><li>Trace timeline: the write was acknowledged by the primary (which didn&rsquo;t ensure replication durability), then primary crashed before flushing to disk; a new leader was elected without the write, causing clients to see stale state.</li></ol><p>Lessons:</p><ul><li>Acknowledging writes before replication persistence or participant fsyncs can yield durability anomalies on primary crashes.</li><li>Jepsen tests force you to decide where to place durability and replication guarantees in the write path.</li></ul><h2 id="17-tuning-elections-timeouts-and-heuristics">17. Tuning elections, timeouts, and heuristics</h2><p>Election/heartbeat tuning rules of thumb:</p><ul><li>Heartbeat interval (h): How often leaders send heartbeats (e.g., 50-200ms in LANs).</li><li>Election timeout (E): Randomized between [T, 2T] where T ≈ 3-5×h to avoid live lock.</li></ul><p>Example:</p><ul><li>h = 100ms, set E ∈ [300ms, 500ms] randomized per node.</li></ul><p>Observations:</p><ul><li>Network jitter and GC pauses can cause spurious elections; increase E in noisy environments.</li><li>Too-large E increases failover latency; too-small E increases election rate and instability.</li></ul><p>Other heuristics:</p><ul><li>Use leader stickiness: avoid immediate re-election after transient issues by preferring an existing leader if it still holds a lease.</li><li>Monitor election rate and correlate with GC/pause and CPU saturation to find root causes.</li></ul><h2 id="18-operational-commands-and-useful-metrics">18. Operational commands and useful metrics</h2><p>Examples (generic):</p><ul><li>Check cluster health and leader: <code>ctl cluster status</code> (system-specific), <code>kubectl get endpoints</code> for k8s services.</li><li>Check logs and term/election events: <code>journalctl -u your-service | grep election</code> or search for &ldquo;Term&rdquo; updates.</li><li>Inspect replication lag and pending logs: system-specific &lsquo;replication&rsquo; or &lsquo;queue&rsquo; commands.</li><li>Prometheus queries: <code>histogram_quantile(0.99, sum(rate(request_duration_seconds_bucket[5m])) by (le))</code> for tail latencies.</li></ul><h2 id="19-further-reading-and-resources">19. Further reading and resources</h2><ul><li>Leslie Lamport&rsquo;s Paxos papers</li><li>Diego Ongaro & John Ousterhout&rsquo;s Raft paper and extended thesis</li><li>Adya&rsquo;s paper on weak isolation levels</li><li>Jepsen repository and blog posts for practical tests</li><li>CRDT survey papers for data-type specifics</li></ul><h3 id="191-consistency-anomalies-short-catalog-with-examples">19.1 Consistency anomalies: short catalog with examples</h3><p>Understanding anomalies helps pick correct isolation models:</p><ul><li><p>Dirty read: Transaction reads uncommitted data written by another transaction.</p><ul><li>SQL examplified: T1 writes x=10 (not committed); T2 reads x and sees 10. Under Read Committed, this cannot happen.</li></ul></li><li><p>Non-repeatable read: A transaction reads the same row twice and sees different values because another transaction committed between reads.</p><ul><li>T1: read x (gets 10); T2: write x=20 commit; T1: read x again → 20.</li></ul></li><li><p>Phantom read: Range queries return different sets due to concurrent inserts/deletes.</p><ul><li>T1: SELECT * WHERE amount > 100 returns N rows; T2 inserts a qualifying row and commits; T1 runs the same query and sees an extra row.</li></ul></li><li><p>Write skew: Two concurrent transactions read overlapping data and write disjoint sets, leading to invariant violation under snapshot isolation.</p><ul><li>Example: Two doctors on-call scheduling: both read other doctor&rsquo;s schedule and both decide to go off-call, violating the constraint &ldquo;always at least one doctor on-call.&rdquo;</li></ul></li></ul><p>Recognizing these anomalies helps choose stricter isolation (e.g., serializable) or application-level invariants and checks.</p><h3 id="192-crdt-operation-based-example-op-based-or-set">19.2 CRDT operation-based example (op-based OR-Set)</h3><p>Operation-based OR-Set sends operations instead of states:</p><ul><li><code>add(e, t)</code> sends op with unique tag <code>t</code>.</li><li><code>remove(e)</code> sends remove operation referencing observed tags or tombstones depending on protocol.</li></ul><p>Delivered op order is guaranteed by reliable dissemination; if delivery is unreliable, the op-based approach requires stable broadcast or idempotency guarantees.</p><h3 id="193-jepsen-reproduction-steps-practical">19.3 Jepsen reproduction steps (practical)</h3><ul><li>Define the model: e.g., linearizability for a set of operations and a key subset.</li><li>Implement client workloads that assert invariants and log operation timelines.</li><li>Use network partitioning, clock skew, process kills, and disk stalls to create adversarial conditions.</li><li>Run multiple iterations, collect histories, and feed them to a checker (Jepsen provides model checkers).</li></ul><p>Sample command (conceptual):</p><ul><li><code>lein run test --checkers linearizable --nemesis partition</code> (system- and test-specific)</li></ul><h3 id="194-performance-tuning-measuring-and-interpreting">19.4 Performance tuning: measuring and interpreting</h3><ul><li><p>Use YCSB for key-value workloads: vary read/write mix and record distributions (zipfian). Example:</p><p><code>./bin/ycsb load cassandra-cql -P workloads/workloadc -p recordcount=1000000 -p threadcount=64</code></p></li><li><p>Watch for high p99/p999 latencies and correlate with background tasks like compaction or GC.</p></li><li><p>When latency tails spike during compaction, consider lowering compaction IO priority or increasing compaction parallelism with careful throttling.</p></li></ul><h3 id="195-security-and-operational-hygiene">19.5 Security and operational hygiene</h3><ul><li>Encrypt node-to-node traffic and client connections with TLS; enable mutual authentication for control planes.</li><li>Audit and rotate keys (KMS) regularly; keep WAL archives encrypted.</li><li>Limit admin APIs and use RBAC for operational controls.</li></ul><h2 id="20-wrap-up">20. Wrap-up</h2><p>Distributed systems require trade-offs and explicit reasoning. Knowing the guarantees and costs of consensus, replication, and failure detectors lets you design systems that behave predictably under load and adversity. Use the checklists above as starting points, and invest in testing, observability, and rehearsal — the hardest bugs are the ones you haven&rsquo;t yet experienced.</p><h3 id="201-debugging-checklist">20.1 Debugging checklist</h3><div class="highlight"><pre tabindex=0 style=color:#e6edf3;background-color:#0d1117;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-text-size-adjust:none><code class="language-text" data-lang=text><span style=display:flex><span>□ Is the system making progress (commits/ops)?
</span></span><span style=display:flex><span>□ Did an election or configuration change occur recently?
</span></span><span style=display:flex><span>□ Is replication lag or log backlog increasing?
</span></span><span style=display:flex><span>□ Are timeouts and heartbeat intervals tuned for your environment?
</span></span><span style=display:flex><span>□ Are long-running transactions or snapshots blocking progress?
</span></span><span style=display:flex><span>□ Can you reproduce the fault with partition/kill tests in staging?
</span></span></code></pre></div></div><footer class="ce1a612 c6dfb1e c3ecea6"><div class="c364589">Categories:
<a href=/categories/fundamentals/>fundamentals</a>, <a href=/categories/systems/>systems</a></div><div>Tags:
<a href=/tags/distributed-systems/>#distributed-systems</a>, <a href=/tags/consensus/>#consensus</a>, <a href=/tags/raft/>#raft</a>, <a href=/tags/paxos/>#paxos</a>, <a href=/tags/cap/>#cap</a>, <a href=/tags/crdt/>#crdt</a>, <a href=/tags/jepsen/>#jepsen</a>, <a href=/tags/fundamentals/>#fundamentals</a></div></footer></article></main><footer class="ccdf0e8" role=contentinfo aria-label=Footer><div class="cfdda01 c133889 c5df473 c0eecc8 c69618a c6942b3 c03620d c2a9f27 c7c11d8 c82c52d c14527b"><div class="c6dfb1e c3ecea6 c39ef11 c88ae6f">&copy; 2026 Leonardo Benicio. All rights
reserved.</div><div class="c6942b3 c7c11d8 cd1fd22"><a href=https://github.com/lbenicio target=_blank rel="noopener noreferrer" aria-label=GitHub class="c1d6c20 c7c11d8 c1d0018 cd1fd22 cb5c327 c10dda9 c6dfb1e cbbda39 cfc01c7 c01f421 c286dd7 c2bd687 cfdce1d cfef18f c000b66 cf55a7b c514027"><svg width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M15 22v-4a4.8 4.8.0 00-1-3.5c3 0 6-2 6-5.5.08-1.25-.27-2.48-1-3.5.28-1.15.28-2.35.0-3.5.0.0-1 0-3 1.5-2.64-.5-5.36-.5-8 0C6 2 5 2 5 2c-.3 1.15-.3 2.35.0 3.5A5.403 5.403.0 004 9c0 3.5 3 5.5 6 5.5-.39.5-.67 1.08-.82 1.7s-.2 1.27-.18 1.9V22"/></svg>
<span class="cba5854">GitHub</span>
</a><a href=https://www.linkedin.com/in/leonardo-benicio target=_blank rel="noopener noreferrer" aria-label=LinkedIn class="c1d6c20 c7c11d8 c1d0018 cd1fd22 cb5c327 c10dda9 c6dfb1e cbbda39 cfc01c7 c01f421 c286dd7 c2bd687 cfdce1d cfef18f c000b66 cf55a7b c514027"><svg width="20" height="20" viewBox="0 0 24 24" fill="currentColor"><path d="M20.447 20.452H17.21V14.86c0-1.333-.027-3.046-1.858-3.046-1.86.0-2.145 1.45-2.145 2.948v5.69H9.069V9h3.112v1.561h.044c.434-.82 1.494-1.686 3.074-1.686 3.29.0 3.897 2.165 3.897 4.983v6.594zM5.337 7.433a1.805 1.805.0 11-.002-3.61 1.805 1.805.0 01.002 3.61zM6.763 20.452H3.911V9h2.852v11.452z"/></svg>
<span class="cba5854">LinkedIn</span>
</a><a href=https://twitter.com/lbenicio_ target=_blank rel="noopener noreferrer" aria-label=Twitter class="c1d6c20 c7c11d8 c1d0018 cd1fd22 cb5c327 c10dda9 c6dfb1e cbbda39 cfc01c7 c01f421 c286dd7 c2bd687 cfdce1d cfef18f c000b66 cf55a7b c514027"><svg width="20" height="20" viewBox="0 0 24 24" fill="currentColor"><path d="M19.633 7.997c.013.177.013.354.013.53.0 5.386-4.099 11.599-11.6 11.599-2.31.0-4.457-.676-6.265-1.842.324.038.636.05.972.05 1.91.0 3.67-.65 5.07-1.755a4.099 4.099.0 01-3.827-2.84c.25.039.5.064.763.064.363.0.726-.051 1.065-.139A4.091 4.091.0 012.542 9.649v-.051c.538.3 1.162.482 1.824.507A4.082 4.082.0 012.54 6.7c0-.751.2-1.435.551-2.034a11.63 11.63.0 008.44 4.281 4.615 4.615.0 01-.101-.938 4.091 4.091.0 017.078-2.799 8.1 8.1.0 002.595-.988 4.112 4.112.0 01-1.8 2.261 8.2 8.2.0 002.357-.638A8.824 8.824.0 0119.613 7.96z"/></svg>
<span class="cba5854">Twitter</span></a></div></div></footer></body></html>