<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Blog on Leonardo Benicio</title><link>https://lbenicio.dev/blog/</link><description>Recent content in Blog on Leonardo Benicio</description><generator>Hugo</generator><language>en-us</language><lastBuildDate>Sun, 21 Dec 2025 00:00:00 +0000</lastBuildDate><atom:link href="https://lbenicio.dev/blog/index.xml" rel="self" type="application/rss+xml"/><item><title>Database Internals: Storage Engines, Transactions, and Recovery</title><link>https://lbenicio.dev/blog/database-internals-storage-engines-transactions-and-recovery/</link><pubDate>Sun, 21 Dec 2025 00:00:00 +0000</pubDate><guid>https://lbenicio.dev/blog/database-internals-storage-engines-transactions-and-recovery/</guid><description>&lt;p&gt;Databases are much more than SQL parsers and client libraries — their core is a storage engine that durably stores and efficiently retrieves data while preserving the guarantees applications depend upon. This article unpacks how modern databases manage on-disk data structures, coordinate concurrent access, provide transactional semantics, and recover from crashes. We&amp;rsquo;ll look at B-trees and LSM-trees, the write-ahead log, MVCC and isolation levels, recovery algorithms, and practical tuning advice for real-world systems.&lt;/p&gt;</description></item><item><title>CPU Microarchitecture: Pipelines, Out-of-Order Execution, and Modern Performance</title><link>https://lbenicio.dev/blog/cpu-microarchitecture-pipelines-out-of-order-execution-and-modern-performance/</link><pubDate>Thu, 04 Dec 2025 00:00:00 +0000</pubDate><guid>https://lbenicio.dev/blog/cpu-microarchitecture-pipelines-out-of-order-execution-and-modern-performance/</guid><description>&lt;p&gt;Modern CPUs are marvels of engineering designed to extract instruction-level parallelism (ILP) from sequential programs while hiding long latencies — memory, multiplies, or long dependency chains. To understand why some code runs orders of magnitude faster than other code that &amp;ldquo;does the same work,&amp;rdquo; you need to understand microarchitectural components such as pipelines, superscalar issue, branch prediction, out-of-order (OoO) execution, register renaming, reorder buffers, and vector (SIMD) units. This article takes a practical tour of these features, how they affect instruction throughput and latency, and pragmatic tips for writing high-performance code.&lt;/p&gt;</description></item><item><title>TLS, PKI, and Secure Protocols: How Encrypted Web Traffic Works</title><link>https://lbenicio.dev/blog/tls-pki-and-secure-protocols-how-encrypted-web-traffic-works/</link><pubDate>Tue, 18 Nov 2025 00:00:00 +0000</pubDate><guid>https://lbenicio.dev/blog/tls-pki-and-secure-protocols-how-encrypted-web-traffic-works/</guid><description>&lt;p&gt;Encrypted network connections are the foundation of modern secure applications. TLS (Transport Layer Security) protects confidentiality, integrity, and — when properly used — authenticity for traffic between clients and servers. Behind the simple &amp;ldquo;https&amp;rdquo; URL there are layers of protocol design, public-key cryptography, symmetric ciphers, certificate chains and revocation systems, ephemeral key exchanges, and subtle deployment pitfalls. This post explores TLS in depth: the record protocol, handshakes (with a focus on TLS 1.3), certificate validation and PKI, cipher suites and AEAD, performance considerations, QUIC integration, operational security, and testing and hardening advice.&lt;/p&gt;</description></item><item><title>Inside Vector Databases: Building Retrieval-Augmented Systems that Scale</title><link>https://lbenicio.dev/blog/inside-vector-databases-building-retrieval-augmented-systems-that-scale/</link><pubDate>Sun, 26 Oct 2025 00:00:00 +0000</pubDate><guid>https://lbenicio.dev/blog/inside-vector-databases-building-retrieval-augmented-systems-that-scale/</guid><description>&lt;p&gt;Vector search used to be a research curiosity. Today it sits in the critical path of customer support bots, developer copilots, fraud monitors, and every product marketing team experimenting with &amp;ldquo;retrieval-augmented&amp;rdquo; workflows. The excitement is deserved, but so is the sober engineering required to keep these systems accurate and available. Building a production vector database is more than storing tensors and calling cosine similarity. It demands a full stack of ingestion, indexing, storage management, failure handling, evaluation, and a constant feedback loop with the language models that consume those results.&lt;/p&gt;</description></item><item><title>Distributed Systems: Consensus, Consistency, and Fault Tolerance</title><link>https://lbenicio.dev/blog/distributed-systems-consensus-consistency-and-fault-tolerance/</link><pubDate>Mon, 20 Oct 2025 00:00:00 +0000</pubDate><guid>https://lbenicio.dev/blog/distributed-systems-consensus-consistency-and-fault-tolerance/</guid><description>&lt;p&gt;Distributed systems are deceptively simple to describe and maddeningly difficult to build. When a single process is replaced by a collection of cooperating processes that communicate over unreliable networks, familiar assumptions break: partial failure becomes the norm, time is not globally synchronized, and correctness requires making explicit trade-offs. This post covers the foundations you need to reason about building correct, robust, and performant distributed systems: failure models, consensus and leader election, replication and consistency models, gossip and membership, CRDTs for conflict-free replication, testing strategies (including Jepsen-style fault injection), and operational best practices.&lt;/p&gt;</description></item><item><title>Learned Indexes: When Models Replace B‑Trees</title><link>https://lbenicio.dev/blog/learned-indexes-when-models-replace-btrees/</link><pubDate>Sat, 04 Oct 2025 10:00:00 +0000</pubDate><guid>https://lbenicio.dev/blog/learned-indexes-when-models-replace-btrees/</guid><description>&lt;p&gt;If you’ve spent a career trusting B‑trees and hash tables, the idea of using a machine‑learned model as an index can feel like swapping a torque wrench for a Ouija board. But learned indexes aren’t a gimmick. They exploit a simple observation: real data isn’t uniformly random. It has shape—monotonic keys, skewed distributions, natural clusters—and a model can learn that shape to predict where a key lives in a sorted array. The payoff is smaller indexes, fewer cache misses, and—sometimes—dramatically faster lookups.&lt;/p&gt;</description></item><item><title>The 100‑Microsecond Rule: Why Tail Latency Eats Your Throughput (and How to Fight Back)</title><link>https://lbenicio.dev/blog/the-100microsecond-rule-why-tail-latency-eats-your-throughput-and-how-to-fight-back/</link><pubDate>Sat, 04 Oct 2025 10:00:00 +0000</pubDate><guid>https://lbenicio.dev/blog/the-100microsecond-rule-why-tail-latency-eats-your-throughput-and-how-to-fight-back/</guid><description>&lt;p&gt;If you stare at a performance dashboard long enough, you’ll eventually see a ghost—an outlier that refuses to go away. It’s the P99 spike that surfaces at the worst time; the one you “fix” three times and then rediscover during a product launch or a perfectly normal Tuesday.&lt;/p&gt;
&lt;p&gt;Here’s the hard truth: tail latency doesn’t just ruin your service levels; it compounds into lost throughput and broken guarantees. In systems with fan‑out, retries, and microservices, the slowest 1% isn’t “rare”—it’s the norm you ship to users most of the time. This is the 100‑microsecond rule in practice: small latencies multiply brutally at scale, and the invisible cost often starts below a single millisecond.&lt;/p&gt;</description></item><item><title>The Quiet Calculus of Probabilistic Commutativity</title><link>https://lbenicio.dev/blog/the-quiet-calculus-of-probabilistic-commutativity/</link><pubDate>Sat, 27 Sep 2025 00:00:00 +0000</pubDate><guid>https://lbenicio.dev/blog/the-quiet-calculus-of-probabilistic-commutativity/</guid><description>&lt;h2 id="abstract"&gt;Abstract&lt;/h2&gt;
&lt;p&gt;Eventual consistency dominates many internet-scale systems, but reasoning about concurrency under minimal coordination remains ad hoc. This post introduces &amp;ldquo;probabilistic commutativity&amp;rdquo; — a lightweight calculus for reasoning about whether concurrent operations, under reasonable stochastic assumptions about ordering and visibility delays, are likely to commute in practice. Probabilistic commutativity offers an intermediate lens between strict algebraic commutativity and empirical test-driven guarantees, enabling low-overhead coordination strategies and probabilistic correctness arguments for producing practically consistent distributed services.&lt;/p&gt;</description></item><item><title>The Hidden Backbone of Parallelism: How Prefix Sums Power Distributed Computation</title><link>https://lbenicio.dev/blog/the-hidden-backbone-of-parallelism-how-prefix-sums-power-distributed-computation/</link><pubDate>Sun, 21 Sep 2025 00:00:00 +0000</pubDate><guid>https://lbenicio.dev/blog/the-hidden-backbone-of-parallelism-how-prefix-sums-power-distributed-computation/</guid><description>&lt;h2 id="introduction"&gt;Introduction&lt;/h2&gt;
&lt;p&gt;When most people think about parallel computing, they imagine splitting a massive task into smaller chunks and running them simultaneously. That’s the Hollywood version: thousands of processors blazing through data, crunching numbers in parallel, and finishing jobs in seconds.&lt;/p&gt;
&lt;p&gt;The reality is both more fascinating and more subtle. Beneath the surface of supercomputers and distributed systems lies a set of seemingly modest mathematical operations—building blocks that make large-scale parallelism possible. Among these, one stands out for its simplicity and profound impact: the &lt;strong&gt;parallel prefix sum&lt;/strong&gt;, also called a &lt;em&gt;scan&lt;/em&gt;.&lt;/p&gt;</description></item><item><title>GPUDirect Storage in 2025: Optimizing the End-to-End Data Path</title><link>https://lbenicio.dev/blog/gpudirect-storage-in-2025-optimizing-the-end-to-end-data-path/</link><pubDate>Tue, 16 Sep 2025 00:00:00 +0000</pubDate><guid>https://lbenicio.dev/blog/gpudirect-storage-in-2025-optimizing-the-end-to-end-data-path/</guid><description>&lt;p&gt;High-performance analytics and training pipelines increasingly hinge on how &lt;em&gt;fast&lt;/em&gt; and &lt;em&gt;efficiently&lt;/em&gt; data reaches GPU memory. Compute has outpaced I/O: a single multi-GPU node can sustain tens of TFLOPs while starved by a few misconfigured storage or copy stages. GPUDirect Storage (GDS) extends the GPUDirect family (peer-to-peer, RDMA) to allow DMA engines (NVMe, NIC) to move bytes directly between storage and GPU memory—bypassing redundant copies through host DRAM and reducing CPU intervention.&lt;/p&gt;</description></item><item><title>MPI vs. OpenMP in 2025: Where Each Wins</title><link>https://lbenicio.dev/blog/mpi-vs.-openmp-in-2025-where-each-wins/</link><pubDate>Fri, 04 Jul 2025 00:00:00 +0000</pubDate><guid>https://lbenicio.dev/blog/mpi-vs.-openmp-in-2025-where-each-wins/</guid><description>&lt;p&gt;Modern clusters have fat nodes (many cores, large memory) and fast interconnects. That’s why hybrid patterns—MPI between nodes and OpenMP within a node—are common.&lt;/p&gt;
&lt;h3 id="when-mpi-wins"&gt;When MPI wins&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Distributed memory by necessity: datasets exceed a node’s RAM.&lt;/li&gt;
&lt;li&gt;Clear data ownership and minimal sharing.&lt;/li&gt;
&lt;li&gt;Coarse-grained decomposition with small surface/volume ratio.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="when-openmp-wins"&gt;When OpenMP wins&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Shared-memory parallel loops and tasks with modest synchronization.&lt;/li&gt;
&lt;li&gt;NUMA-aware data placement still local to a node.&lt;/li&gt;
&lt;li&gt;Rapid prototyping and incremental parallelization of CPU-bound kernels.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="hybrid-design-tips"&gt;Hybrid design tips&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Bind MPI ranks to NUMA domains; spawn OpenMP threads within each. Avoid oversubscription.&lt;/li&gt;
&lt;li&gt;Use non-blocking collectives (MPI_Iallreduce) to hide latency.&lt;/li&gt;
&lt;li&gt;Pin memory and leverage huge pages where available.&lt;/li&gt;
&lt;li&gt;Profile: check MPI time vs. compute vs. OpenMP overhead; fix the biggest slice first.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="example-openmp-reduction-vs-mpi-allreduce"&gt;Example: OpenMP reduction vs. MPI allreduce&lt;/h3&gt;
&lt;div class="highlight"&gt;&lt;pre tabindex="0" style="color:#e6edf3;background-color:#0d1117;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-text-size-adjust:none;"&gt;&lt;code class="language-c" data-lang="c"&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;&lt;span style="color:#8b949e;font-style:italic"&gt;// OpenMP reduction inside a rank
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;&lt;span style="color:#8b949e;font-weight:bold;font-style:italic"&gt;#pragma omp parallel for reduction(+:sum)
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;&lt;span style="color:#ff7b72"&gt;for&lt;/span&gt; (&lt;span style="color:#ff7b72"&gt;int&lt;/span&gt; i &lt;span style="color:#ff7b72;font-weight:bold"&gt;=&lt;/span&gt; &lt;span style="color:#a5d6ff"&gt;0&lt;/span&gt;; i &lt;span style="color:#ff7b72;font-weight:bold"&gt;&amp;lt;&lt;/span&gt; n; &lt;span style="color:#ff7b72;font-weight:bold"&gt;++&lt;/span&gt;i) sum &lt;span style="color:#ff7b72;font-weight:bold"&gt;+=&lt;/span&gt; a[i] &lt;span style="color:#ff7b72;font-weight:bold"&gt;*&lt;/span&gt; b[i];
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;&lt;span style="color:#8b949e;font-style:italic"&gt;// Then global reduce across ranks
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;&lt;span style="color:#d2a8ff;font-weight:bold"&gt;MPI_Allreduce&lt;/span&gt;(&lt;span style="color:#ff7b72;font-weight:bold"&gt;&amp;amp;&lt;/span&gt;sum, &lt;span style="color:#ff7b72;font-weight:bold"&gt;&amp;amp;&lt;/span&gt;global, &lt;span style="color:#a5d6ff"&gt;1&lt;/span&gt;, MPI_DOUBLE, MPI_SUM, MPI_COMM_WORLD);
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Rule of thumb: start simple (single model) and add hybrid complexity only when measurements show you must.&lt;/p&gt;</description></item><item><title>From MapReduce to Spark: The Arc of Data-Parallel Systems</title><link>https://lbenicio.dev/blog/from-mapreduce-to-spark-the-arc-of-data-parallel-systems/</link><pubDate>Mon, 19 May 2025 00:00:00 +0000</pubDate><guid>https://lbenicio.dev/blog/from-mapreduce-to-spark-the-arc-of-data-parallel-systems/</guid><description>&lt;p&gt;MapReduce popularized large-scale batch processing with a simple model (map, shuffle, reduce) and immutable intermediate state on HDFS. It optimized for throughput and fault tolerance via re-execution.&lt;/p&gt;
&lt;p&gt;Spark expanded the model:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;RDDs: immutable, partitioned datasets with lineage, enabling recomputation on failure.&lt;/li&gt;
&lt;li&gt;DAG scheduler: plans multi-stage jobs, pipelining narrow transformations and materializing wide ones.&lt;/li&gt;
&lt;li&gt;In-memory caching: keeps hot datasets in RAM to accelerate iterative workloads.&lt;/li&gt;
&lt;li&gt;Higher-level APIs: DataFrames/Datasets and SQL, plus MLlib and Structured Streaming.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="checkpointing-and-lineage"&gt;Checkpointing and lineage&lt;/h3&gt;
&lt;p&gt;RDD lineage can grow large; Spark checkpoints to cut recomputation cost. For streaming, write-ahead logs plus checkpoints enable recovery.&lt;/p&gt;</description></item><item><title>Auditing the Algorithm: Building a Responsible AI Pipeline That Scales</title><link>https://lbenicio.dev/blog/auditing-the-algorithm-building-a-responsible-ai-pipeline-that-scales/</link><pubDate>Sat, 05 Apr 2025 13:25:00 +0000</pubDate><guid>https://lbenicio.dev/blog/auditing-the-algorithm-building-a-responsible-ai-pipeline-that-scales/</guid><description>&lt;p&gt;When regulators asked for evidence that our AI systems behave responsibly, we realized spreadsheets and ad hoc reviews wouldn&amp;rsquo;t suffice. Our models influenced credit decisions, hiring suggestions, and medical triage. Stakeholders demanded more than accuracy—they wanted fairness, explainability, and accountability. We responded by building a responsible AI audit pipeline woven into development, deployment, and operations. This article shares how we did it: the processes, tooling, and cultural shifts that turned compliance into continuous curiosity.&lt;/p&gt;</description></item><item><title>Memory Allocation and Garbage Collection: How Programs Manage Memory</title><link>https://lbenicio.dev/blog/memory-allocation-and-garbage-collection-how-programs-manage-memory/</link><pubDate>Thu, 20 Feb 2025 00:00:00 +0000</pubDate><guid>https://lbenicio.dev/blog/memory-allocation-and-garbage-collection-how-programs-manage-memory/</guid><description>&lt;p&gt;Every program needs memory, and every byte eventually becomes garbage. Between allocation and collection lies a fascinating world of algorithms, data structures, and engineering trade-offs that profoundly affect application performance. Whether you&amp;rsquo;re debugging memory leaks, optimizing allocation patterns, or choosing between languages, understanding memory management internals gives you the mental models to make better decisions.&lt;/p&gt;
&lt;h2 id="1-the-memory-landscape"&gt;1. The Memory Landscape&lt;/h2&gt;
&lt;p&gt;Before diving into allocation strategies, let&amp;rsquo;s understand the terrain.&lt;/p&gt;
&lt;h3 id="11-virtual-address-space-layout"&gt;1.1 Virtual Address Space Layout&lt;/h3&gt;
&lt;div class="highlight"&gt;&lt;pre tabindex="0" style="color:#e6edf3;background-color:#0d1117;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-text-size-adjust:none;"&gt;&lt;code class="language-text" data-lang="text"&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;Typical Process Memory Layout (Linux x86-64):
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;┌─────────────────────────────────┐ 0x7FFFFFFFFFFF (high)
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;│ Stack │ ← Grows downward
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;│ ↓ │
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;├─────────────────────────────────┤
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;│ │
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;│ Unmapped Region │
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;│ │
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;├─────────────────────────────────┤
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;│ ↑ │
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;│ Heap │ ← Grows upward
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;├─────────────────────────────────┤
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;│ BSS Segment │ ← Uninitialized globals
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;├─────────────────────────────────┤
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;│ Data Segment │ ← Initialized globals
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;├─────────────────────────────────┤
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;│ Text Segment │ ← Program code (read-only)
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;└─────────────────────────────────┘ 0x400000 (low)
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;The stack and heap grow toward each other from opposite ends of the address space. The stack handles function call frames automatically, while the heap requires explicit management—either by the programmer or by a garbage collector.&lt;/p&gt;</description></item><item><title>Scheduling: Trading Latency for Throughput (and Back Again)</title><link>https://lbenicio.dev/blog/scheduling-trading-latency-for-throughput-and-back-again/</link><pubDate>Wed, 12 Feb 2025 10:00:00 +0000</pubDate><guid>https://lbenicio.dev/blog/scheduling-trading-latency-for-throughput-and-back-again/</guid><description>&lt;p&gt;Schedulers encode policy: who runs next, on which core, and for how long. Those choices shuffle latency and throughput. Let’s make the trade‑offs explicit.&lt;/p&gt;
&lt;h2 id="fifo-vs-priority-vs-fair"&gt;FIFO vs. priority vs. fair&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;FIFO: simple, minimal overhead; tail prone under bursty arrivals.&lt;/li&gt;
&lt;li&gt;Priority: protects critical work; risks starvation without aging.&lt;/li&gt;
&lt;li&gt;Fair (CFQ‑like): shares CPU evenly; may underutilize when work is imbalanced.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="work-stealing"&gt;Work stealing&lt;/h2&gt;
&lt;p&gt;Great for irregular parallel workloads. Each worker has a deque; thieves steal from the tail. Pros: high utilization; Cons: cache locality losses and noisy tails under high contention.&lt;/p&gt;</description></item><item><title>Exactly-Once in Streaming: What It Means and How Systems Achieve It</title><link>https://lbenicio.dev/blog/exactly-once-in-streaming-what-it-means-and-how-systems-achieve-it/</link><pubDate>Wed, 22 Jan 2025 00:00:00 +0000</pubDate><guid>https://lbenicio.dev/blog/exactly-once-in-streaming-what-it-means-and-how-systems-achieve-it/</guid><description>&lt;p&gt;&amp;ldquo;Exactly-once&amp;rdquo; doesn’t mean an event packet traverses the network a single time. It means that in the &lt;em&gt;observable&lt;/em&gt; outputs (state, emitted records, external side effects) each logical input is reflected &lt;strong&gt;at most once&lt;/strong&gt; and &lt;strong&gt;at least once&lt;/strong&gt;—so exactly once—despite retries, replays, failovers, or speculative execution. It is an &lt;em&gt;end-to-end&lt;/em&gt; property requiring cooperation across producer, broker, processing engine, and sinks.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id="1-taxonomy-of-delivery--processing-guarantees"&gt;1. Taxonomy of Delivery &amp;amp; Processing Guarantees&lt;/h2&gt;
&lt;table&gt;
 &lt;thead&gt;
 &lt;tr&gt;
 &lt;th&gt;Term&lt;/th&gt;
 &lt;th&gt;Network Delivery&lt;/th&gt;
 &lt;th&gt;Processing Attempts&lt;/th&gt;
 &lt;th&gt;External Effect Guarantee&lt;/th&gt;
 &lt;th&gt;Typical Mechanisms&lt;/th&gt;
 &lt;/tr&gt;
 &lt;/thead&gt;
 &lt;tbody&gt;
 &lt;tr&gt;
 &lt;td&gt;At most once&lt;/td&gt;
 &lt;td&gt;Messages may be lost&lt;/td&gt;
 &lt;td&gt;≤1&lt;/td&gt;
 &lt;td&gt;Missing effects possible&lt;/td&gt;
 &lt;td&gt;Fire-and-forget, no ack replay&lt;/td&gt;
 &lt;/tr&gt;
 &lt;tr&gt;
 &lt;td&gt;At least once&lt;/td&gt;
 &lt;td&gt;Redelivery possible&lt;/td&gt;
 &lt;td&gt;≥1&lt;/td&gt;
 &lt;td&gt;Duplicated effects if not idempotent&lt;/td&gt;
 &lt;td&gt;Offsets/checkpoints + replay&lt;/td&gt;
 &lt;/tr&gt;
 &lt;tr&gt;
 &lt;td&gt;Exactly once (processing)&lt;/td&gt;
 &lt;td&gt;Redelivery possible&lt;/td&gt;
 &lt;td&gt;≥1&lt;/td&gt;
 &lt;td&gt;Each logical record causes effect once&lt;/td&gt;
 &lt;td&gt;Idempotence + dedupe or transactions + snapshots&lt;/td&gt;
 &lt;/tr&gt;
 &lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Note the &lt;em&gt;processing&lt;/em&gt; dimension: we usually allow redelivery at the transport layer; the system masks duplicates before they become externally visible.&lt;/p&gt;</description></item><item><title>Tuning CUDA with the GPU Memory Hierarchy</title><link>https://lbenicio.dev/blog/tuning-cuda-with-the-gpu-memory-hierarchy/</link><pubDate>Wed, 27 Nov 2024 00:00:00 +0000</pubDate><guid>https://lbenicio.dev/blog/tuning-cuda-with-the-gpu-memory-hierarchy/</guid><description>&lt;p&gt;CUDA performance hinges on moving data efficiently through a &lt;em&gt;hierarchy&lt;/em&gt; of memories that differ by latency, bandwidth, scope (visibility), and capacity. Raw FLOP throughput is rarely the first limiter—memory behavior, access ordering, and reuse patterns almost always dominate performance envelopes.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id="1-the-hierarchy-at-a-glance"&gt;1. The Hierarchy at a Glance&lt;/h2&gt;
&lt;table&gt;
 &lt;thead&gt;
 &lt;tr&gt;
 &lt;th&gt;Level&lt;/th&gt;
 &lt;th&gt;Scope / Visibility&lt;/th&gt;
 &lt;th&gt;Approx Latency (cycles)*&lt;/th&gt;
 &lt;th&gt;Bandwidth&lt;/th&gt;
 &lt;th&gt;Capacity (per SM / device)&lt;/th&gt;
 &lt;th&gt;Notes&lt;/th&gt;
 &lt;/tr&gt;
 &lt;/thead&gt;
 &lt;tbody&gt;
 &lt;tr&gt;
 &lt;td&gt;Registers&lt;/td&gt;
 &lt;td&gt;Thread private&lt;/td&gt;
 &lt;td&gt;~1&lt;/td&gt;
 &lt;td&gt;Extreme&lt;/td&gt;
 &lt;td&gt;Tens of k per SM (allocated per thread)&lt;/td&gt;
 &lt;td&gt;Allocation affects occupancy; spilling -&amp;gt; local memory&lt;/td&gt;
 &lt;/tr&gt;
 &lt;tr&gt;
 &lt;td&gt;Shared Memory (SMEM)&lt;/td&gt;
 &lt;td&gt;Block (CTA)&lt;/td&gt;
 &lt;td&gt;~20–35&lt;/td&gt;
 &lt;td&gt;Very high&lt;/td&gt;
 &lt;td&gt;48–228 KB configurable (arch dependent)&lt;/td&gt;
 &lt;td&gt;Banked; subject to conflicts; optional split w/ L1&lt;/td&gt;
 &lt;/tr&gt;
 &lt;tr&gt;
 &lt;td&gt;L1 / Texture Cache&lt;/td&gt;
 &lt;td&gt;SM&lt;/td&gt;
 &lt;td&gt;~30–60&lt;/td&gt;
 &lt;td&gt;High&lt;/td&gt;
 &lt;td&gt;~128–256 KB (unified)&lt;/td&gt;
 &lt;td&gt;Serves global loads; spatial locality &amp;amp; coalescing still matter&lt;/td&gt;
 &lt;/tr&gt;
 &lt;tr&gt;
 &lt;td&gt;L2 Cache&lt;/td&gt;
 &lt;td&gt;Device-wide&lt;/td&gt;
 &lt;td&gt;~200–300&lt;/td&gt;
 &lt;td&gt;High&lt;/td&gt;
 &lt;td&gt;Multi-MB&lt;/td&gt;
 &lt;td&gt;Coherent across SMs; crucial for global data reuse&lt;/td&gt;
 &lt;/tr&gt;
 &lt;tr&gt;
 &lt;td&gt;Global DRAM&lt;/td&gt;
 &lt;td&gt;Device-wide&lt;/td&gt;
 &lt;td&gt;~400–800&lt;/td&gt;
 &lt;td&gt;High (GB/s)&lt;/td&gt;
 &lt;td&gt;Many GB&lt;/td&gt;
 &lt;td&gt;Long latency—hide with parallelism &amp;amp; coalescing&lt;/td&gt;
 &lt;/tr&gt;
 &lt;tr&gt;
 &lt;td&gt;Constant Cache&lt;/td&gt;
 &lt;td&gt;Device-wide (read-only)&lt;/td&gt;
 &lt;td&gt;~ L1 hit if cached&lt;/td&gt;
 &lt;td&gt;High (broadcast)&lt;/td&gt;
 &lt;td&gt;64 KB&lt;/td&gt;
 &lt;td&gt;Broadcast to warp if all threads read same address&lt;/td&gt;
 &lt;/tr&gt;
 &lt;tr&gt;
 &lt;td&gt;Texture / Read-Only Cache&lt;/td&gt;
 &lt;td&gt;Device-wide (cached)&lt;/td&gt;
 &lt;td&gt;Similar to L1&lt;/td&gt;
 &lt;td&gt;High&lt;/td&gt;
 &lt;td&gt;N/A&lt;/td&gt;
 &lt;td&gt;Provides specialized spatial filtering &amp;amp; relaxed coalescing&lt;/td&gt;
 &lt;/tr&gt;
 &lt;tr&gt;
 &lt;td&gt;Local Memory&lt;/td&gt;
 &lt;td&gt;Thread (spill/backing)&lt;/td&gt;
 &lt;td&gt;DRAM latency&lt;/td&gt;
 &lt;td&gt;DRAM&lt;/td&gt;
 &lt;td&gt;Per-thread virtual&lt;/td&gt;
 &lt;td&gt;“Local” is misnomer if spilled—same as global latency&lt;/td&gt;
 &lt;/tr&gt;
 &lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;&lt;em&gt;Indicative ranges; varies by architecture generation (e.g., Turing, Ampere, Hopper). Absolute numbers less important than ratio gaps.&lt;/em&gt;&lt;/p&gt;</description></item><item><title>Write-Ahead Logging: The Unsung Hero of Database Durability</title><link>https://lbenicio.dev/blog/write-ahead-logging-the-unsung-hero-of-database-durability/</link><pubDate>Tue, 10 Sep 2024 00:00:00 +0000</pubDate><guid>https://lbenicio.dev/blog/write-ahead-logging-the-unsung-hero-of-database-durability/</guid><description>&lt;p&gt;Every database makes a promise: your committed data will survive crashes, power failures, and hardware hiccups. This promise—durability—seems simple until you consider the physics. Disks are slow. Memory is volatile. Writes can be reordered. Yet somehow, databases deliver both durability and performance. The secret weapon? Write-ahead logging.&lt;/p&gt;
&lt;h2 id="1-the-durability-problem"&gt;1. The Durability Problem&lt;/h2&gt;
&lt;p&gt;Consider what happens when you execute &lt;code&gt;UPDATE accounts SET balance = balance - 100 WHERE id = 42&lt;/code&gt;. The database must:&lt;/p&gt;</description></item><item><title>Bloom Filters and Probabilistic Data Structures: Trading Certainty for Speed</title><link>https://lbenicio.dev/blog/bloom-filters-and-probabilistic-data-structures-trading-certainty-for-speed/</link><pubDate>Thu, 22 Aug 2024 00:00:00 +0000</pubDate><guid>https://lbenicio.dev/blog/bloom-filters-and-probabilistic-data-structures-trading-certainty-for-speed/</guid><description>&lt;p&gt;Sometimes the right answer is &amp;ldquo;probably yes.&amp;rdquo; In a world obsessed with correctness, probabilistic data structures offer a heretical bargain: give up certainty, and in return receive orders-of-magnitude improvements in memory, speed, or both. This post explores the theory and practice behind Bloom filters, Count-Min sketches, HyperLogLog, and their variants—structures that power everything from database query optimization to network traffic analysis at scale.&lt;/p&gt;
&lt;h2 id="1-the-case-for-uncertainty"&gt;1. The Case for Uncertainty&lt;/h2&gt;
&lt;p&gt;Traditional data structures promise exact answers. A hash set tells you definitively whether an element exists. A counter tells you precisely how many times something occurred. But exactness has a cost: memory consumption grows with the number of distinct elements, and lookups may require following chains or probing sequences.&lt;/p&gt;</description></item><item><title>Seeing in the Dark: Observability for Edge AI Fleets</title><link>https://lbenicio.dev/blog/seeing-in-the-dark-observability-for-edge-ai-fleets/</link><pubDate>Fri, 16 Aug 2024 10:55:00 +0000</pubDate><guid>https://lbenicio.dev/blog/seeing-in-the-dark-observability-for-edge-ai-fleets/</guid><description>&lt;p&gt;Our edge AI deployment began with a handful of pilot devices in retail stores. Within months, thousands of cameras, sensors, and point-of-sale terminals joined the fleet. They detected shelves running low, predicted queue lengths, and flagged suspicious transactions. But when a customer called asking why a device misclassified bananas as tennis balls, we realized our observability blurred at the edge. Logs vanished into the ether, metrics arrived sporadically, and models drifted silently. This article shares how we built observability robust enough for flaky networks, sensitive data, and autonomous updates.&lt;/p&gt;</description></item><item><title>Adaptive Feature Flag Frameworks for Hyper-Growth SaaS</title><link>https://lbenicio.dev/blog/adaptive-feature-flag-frameworks-for-hyper-growth-saas/</link><pubDate>Thu, 15 Aug 2024 00:00:00 +0000</pubDate><guid>https://lbenicio.dev/blog/adaptive-feature-flag-frameworks-for-hyper-growth-saas/</guid><description>&lt;p&gt;Hyper-growth SaaS companies live in perpetual motion. Every day brings new customer segments, compliance regimes, infrastructure bottlenecks, and competitive threats. Feature flags evolved from simple booleans to mission-critical systems that guard deployments, power experiments, share risk across teams, and surface real-time signals about user delight. Building an adaptive feature flag framework is no longer optional; it is the backbone of progressive delivery and organizational learning. This article offers a deep dive into the patterns, guardrails, and operating models needed to keep feature flags safe, observable, and aligned with business outcomes when your product is shipping at warp speed.&lt;/p&gt;</description></item><item><title>Lock-Free Data Structures: Concurrency Without the Wait</title><link>https://lbenicio.dev/blog/lock-free-data-structures-concurrency-without-the-wait/</link><pubDate>Thu, 18 Jul 2024 00:00:00 +0000</pubDate><guid>https://lbenicio.dev/blog/lock-free-data-structures-concurrency-without-the-wait/</guid><description>&lt;p&gt;Traditional locks have served concurrent programming for decades, but they come with costs: contention, priority inversion, and the ever-present risk of deadlock. Lock-free data structures offer an alternative—algorithms that guarantee system-wide progress even when individual threads stall. This post explores the theory, challenges, and practical implementations of lock-free programming.&lt;/p&gt;
&lt;h2 id="1-why-lock-free"&gt;1. Why Lock-Free?&lt;/h2&gt;
&lt;p&gt;Consider a simple counter shared among threads. With a mutex:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre tabindex="0" style="color:#e6edf3;background-color:#0d1117;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-text-size-adjust:none;"&gt;&lt;code class="language-c" data-lang="c"&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;&lt;span style="color:#d2a8ff;font-weight:bold"&gt;pthread_mutex_lock&lt;/span&gt;(&lt;span style="color:#ff7b72;font-weight:bold"&gt;&amp;amp;&lt;/span&gt;mutex);
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;counter&lt;span style="color:#ff7b72;font-weight:bold"&gt;++&lt;/span&gt;;
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;&lt;span style="color:#d2a8ff;font-weight:bold"&gt;pthread_mutex_unlock&lt;/span&gt;(&lt;span style="color:#ff7b72;font-weight:bold"&gt;&amp;amp;&lt;/span&gt;mutex);
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;This works, but has problems:&lt;/p&gt;</description></item><item><title>Amdahl’s Law vs. Gustafson’s Law: What They Really Predict</title><link>https://lbenicio.dev/blog/amdahls-law-vs.-gustafsons-law-what-they-really-predict/</link><pubDate>Sat, 15 Jun 2024 00:00:00 +0000</pubDate><guid>https://lbenicio.dev/blog/amdahls-law-vs.-gustafsons-law-what-they-really-predict/</guid><description>&lt;p&gt;Amdahl’s Law and Gustafson’s Law are often presented as opposites, but they model different scenarios.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Amdahl assumes a fixed problem size and asks: how much faster can I make this workload with P processors when a fraction s is inherently serial? The upper bound is:&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;$$\text{Speedup}_A(P) = \frac{1}{s + \frac{1-s}{P}}.$$&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Gustafson assumes a fixed execution time and asks: given P processors, how much bigger can the problem become if parallel parts scale? The scaled speedup is:&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;$$\text{Speedup}_G(P) = s + (1-s),P.$$&lt;/p&gt;</description></item><item><title>Concurrency Primitives and Synchronization: From Spinlocks to Lock-Free Data Structures</title><link>https://lbenicio.dev/blog/concurrency-primitives-and-synchronization-from-spinlocks-to-lock-free-data-structures/</link><pubDate>Fri, 15 Mar 2024 00:00:00 +0000</pubDate><guid>https://lbenicio.dev/blog/concurrency-primitives-and-synchronization-from-spinlocks-to-lock-free-data-structures/</guid><description>&lt;p&gt;Concurrent programming transforms sequential code into parallel execution, unlocking the power of modern multicore processors. But with parallelism comes the challenge of coordinating access to shared resources. Race conditions lurk in code that appears correct, and subtle bugs emerge only under specific timing conditions. Understanding synchronization primitives from the hardware level up through high-level abstractions provides the foundation for writing correct, efficient concurrent programs.&lt;/p&gt;
&lt;h2 id="1-the-concurrency-challenge"&gt;1. The Concurrency Challenge&lt;/h2&gt;
&lt;p&gt;Why synchronization matters and what problems it solves.&lt;/p&gt;</description></item><item><title>Unicode and Character Encoding: From ASCII to UTF-8 and Beyond</title><link>https://lbenicio.dev/blog/unicode-and-character-encoding-from-ascii-to-utf-8-and-beyond/</link><pubDate>Fri, 15 Mar 2024 00:00:00 +0000</pubDate><guid>https://lbenicio.dev/blog/unicode-and-character-encoding-from-ascii-to-utf-8-and-beyond/</guid><description>&lt;p&gt;Text seems simple until you try to handle it correctly. A single question—&amp;ldquo;how many characters are in this string?&amp;quot;—can have multiple valid answers depending on what you mean by &amp;ldquo;character.&amp;rdquo; Understanding Unicode and character encoding is essential for any programmer working with internationalized text, file formats, network protocols, or databases.&lt;/p&gt;
&lt;h2 id="1-the-history-of-character-encoding"&gt;1. The History of Character Encoding&lt;/h2&gt;
&lt;p&gt;Before we can understand where we are, we need to know how we got here.&lt;/p&gt;</description></item><item><title>Countdown to Quantum: Migrating an Enterprise to Post-Quantum Cryptography</title><link>https://lbenicio.dev/blog/countdown-to-quantum-migrating-an-enterprise-to-post-quantum-cryptography/</link><pubDate>Mon, 29 Jan 2024 16:40:00 +0000</pubDate><guid>https://lbenicio.dev/blog/countdown-to-quantum-migrating-an-enterprise-to-post-quantum-cryptography/</guid><description>&lt;p&gt;When the first credible quantum threat report reached our CISO&amp;rsquo;s desk, the reaction was cautious curiosity. Quantum computers capable of breaking RSA-2048 remain years away, but data harvested today could be decrypted decades later. We handle sensitive customer contracts, healthcare records, and government workloads. Waiting for the quantum threat to manifest felt irresponsible. So we embarked on a multi-year migration to post-quantum cryptography (PQC). This post documents the journey—strategy, tooling, experiments, and the human moments that kept the program afloat.&lt;/p&gt;</description></item><item><title>Sealing the Supply Chain: Zero-Trust Build Pipelines That Scale</title><link>https://lbenicio.dev/blog/sealing-the-supply-chain-zero-trust-build-pipelines-that-scale/</link><pubDate>Sun, 08 Oct 2023 21:10:00 +0000</pubDate><guid>https://lbenicio.dev/blog/sealing-the-supply-chain-zero-trust-build-pipelines-that-scale/</guid><description>&lt;p&gt;The day we discovered a compromised dependency in our build pipeline felt like waking up to find the locks on our headquarters replaced. The package arrived through a trusted mirror, signed with a familiar key, yet embedded a subtle time bomb. We escaped without production impact thanks to layered defenses, but the scare pushed us to redesign our build system around zero trust. This article chronicles that transformation—technical and human—from ad hoc pipelines to hermetic, verifiable, auditable delivery.&lt;/p&gt;</description></item><item><title>File Systems and Storage Internals: How Data Persists on Disk</title><link>https://lbenicio.dev/blog/file-systems-and-storage-internals-how-data-persists-on-disk/</link><pubDate>Fri, 22 Sep 2023 00:00:00 +0000</pubDate><guid>https://lbenicio.dev/blog/file-systems-and-storage-internals-how-data-persists-on-disk/</guid><description>&lt;p&gt;Every file you save, every application you install, every database record—all must survive power failures and system crashes. File systems provide this durability guarantee while making storage appear as a simple hierarchy of named files and directories. Behind this abstraction lies sophisticated machinery for organizing billions of bytes, recovering from failures, and optimizing access patterns. Understanding file system internals illuminates why some operations are fast and others slow, why disks fill up unexpectedly, and how your data survives the unexpected.&lt;/p&gt;</description></item><item><title>Memory Allocators: From malloc to Modern Arena Allocators</title><link>https://lbenicio.dev/blog/memory-allocators-from-malloc-to-modern-arena-allocators/</link><pubDate>Thu, 14 Sep 2023 00:00:00 +0000</pubDate><guid>https://lbenicio.dev/blog/memory-allocators-from-malloc-to-modern-arena-allocators/</guid><description>&lt;p&gt;Memory allocation is one of those fundamental operations that most programmers take for granted. You call &lt;code&gt;malloc()&lt;/code&gt;, memory appears; you call &lt;code&gt;free()&lt;/code&gt;, it goes away. But beneath this simple interface lies a fascinating world of algorithms, trade-offs, and optimizations that can make or break your application&amp;rsquo;s performance. This post explores memory allocation from first principles to modern high-performance allocators.&lt;/p&gt;
&lt;h2 id="1-why-memory-allocation-matters"&gt;1. Why Memory Allocation Matters&lt;/h2&gt;
&lt;p&gt;Consider a web server handling thousands of requests per second. Each request might allocate dozens of objects: strings, buffers, data structures. If each allocation takes 1 microsecond, and a request needs 50 allocations, that&amp;rsquo;s 50 microseconds just in allocation overhead—potentially more than the actual request processing time.&lt;/p&gt;</description></item><item><title>Reverse Indexing and Inverted Files: How Search Engines Fly</title><link>https://lbenicio.dev/blog/reverse-indexing-and-inverted-files-how-search-engines-fly/</link><pubDate>Wed, 19 Jul 2023 10:00:00 +0000</pubDate><guid>https://lbenicio.dev/blog/reverse-indexing-and-inverted-files-how-search-engines-fly/</guid><description>&lt;p&gt;Full‑text search is a masterclass in practical data structures. The inverted index—also called a reverse index—maps terms to the list of documents in which they occur. Everything else in a production search engine is optimization: reducing bytes, minimizing random I/O, and avoiding work you don’t have to do.&lt;/p&gt;
&lt;p&gt;In this deep dive we’ll build a complete mental model of inverted files and the techniques that make them fast:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Parsing pipeline: tokenization, normalization, stemming/lemmatization, and multilingual realities&lt;/li&gt;
&lt;li&gt;Index structure: vocabulary, postings (doc IDs), term frequencies, and positions for phrase queries&lt;/li&gt;
&lt;li&gt;Compression: delta encoding, Variable‑Byte (VB), Simple‑8b, PForDelta, SIMD‑BP128, QMX, and how they trade space for CPU&lt;/li&gt;
&lt;li&gt;Skipping and acceleration: skip lists, block max indexes, WAND/BMW dynamic pruning&lt;/li&gt;
&lt;li&gt;Scoring: BM25, term and document statistics, field boosts, and normalization&lt;/li&gt;
&lt;li&gt;Updates and merges: segment architecture (Lucene‑style), in‑place deletes, and background compaction&lt;/li&gt;
&lt;li&gt;Caching and tiering: hot vs cold shards, result caching, and Bloom‑like structures&lt;/li&gt;
&lt;li&gt;Distributed search: sharding, replication, and query fan‑out under tail latency pressure&lt;/li&gt;
&lt;li&gt;Measuring and tuning: from recall/precision to p95 query time, heap usage, and GC pauses&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;By the end you’ll be able to reason about why each knob exists and which ones matter for your workload.&lt;/p&gt;</description></item><item><title>Latency-Aware Edge Inference Platforms: Engineering Consistent AI Experiences</title><link>https://lbenicio.dev/blog/latency-aware-edge-inference-platforms-engineering-consistent-ai-experiences/</link><pubDate>Sun, 12 Mar 2023 00:00:00 +0000</pubDate><guid>https://lbenicio.dev/blog/latency-aware-edge-inference-platforms-engineering-consistent-ai-experiences/</guid><description>&lt;p&gt;Large language models, recommender systems, and computer vision pipelines increasingly run where users are: on kiosks, factory floors, AR headsets, connected vehicles, and retail shelves. But pushing models to the edge without a latency-aware strategy is a recipe for jittery UX, missed detections, and compliance headaches. This guide dissects what it takes to build an edge inference platform that meets strict latency budgets—even when networks wobble, models evolve weekly, and hardware varies wildly.&lt;/p&gt;</description></item><item><title>Keeping the Model Awake: Building a Self-Healing ML Inference Platform</title><link>https://lbenicio.dev/blog/keeping-the-model-awake-building-a-self-healing-ml-inference-platform/</link><pubDate>Tue, 14 Feb 2023 07:20:00 +0000</pubDate><guid>https://lbenicio.dev/blog/keeping-the-model-awake-building-a-self-healing-ml-inference-platform/</guid><description>&lt;p&gt;During a winter holiday freeze, our recommendation API refused to scale. GPUs idled waiting for models to load, autoscalers fought each other, and on-call engineers reheated leftovers at 3 a.m. while spike traffic slammed into origin. We promised leadership that this would never happen again. The solution wasn&amp;rsquo;t magic; it was a self-healing inference platform blending old-school reliability, modern ML tooling, and relentless experimentation.&lt;/p&gt;
&lt;p&gt;This post documents the rebuild. We&amp;rsquo;ll explore model packaging, warm-up rituals, adaptive scheduling, observability, chaos drills, and the social contract between ML researchers and production engineers. The goal: keep models awake, snappy, and trustworthy even when the world throws curveballs.&lt;/p&gt;</description></item><item><title>TCP Congestion Control: From Slow Start to BBR</title><link>https://lbenicio.dev/blog/tcp-congestion-control-from-slow-start-to-bbr/</link><pubDate>Sat, 11 Feb 2023 00:00:00 +0000</pubDate><guid>https://lbenicio.dev/blog/tcp-congestion-control-from-slow-start-to-bbr/</guid><description>&lt;p&gt;Every time you load a webpage, stream a video, or download a file, TCP congestion control algorithms work invisibly to maximize throughput while preventing network collapse. These algorithms represent decades of research into one of networking&amp;rsquo;s hardest problems: how do independent senders share a network fairly without overwhelming it? Let&amp;rsquo;s explore how TCP congestion control evolved from simple beginnings to sophisticated model-based approaches.&lt;/p&gt;
&lt;h2 id="1-the-congestion-problem"&gt;1. The Congestion Problem&lt;/h2&gt;
&lt;p&gt;Without congestion control, networks collapse under load.&lt;/p&gt;</description></item><item><title>Floating Point: How Computers Represent Real Numbers</title><link>https://lbenicio.dev/blog/floating-point-how-computers-represent-real-numbers/</link><pubDate>Wed, 08 Feb 2023 00:00:00 +0000</pubDate><guid>https://lbenicio.dev/blog/floating-point-how-computers-represent-real-numbers/</guid><description>&lt;p&gt;Every programmer eventually encounters the classic puzzle: why does &lt;code&gt;0.1 + 0.2&lt;/code&gt; not equal &lt;code&gt;0.3&lt;/code&gt;? The answer lies in how computers represent real numbers using floating point arithmetic. Understanding IEEE 754 floating point is essential for anyone writing numerical code, financial software, scientific simulations, or graphics applications.&lt;/p&gt;
&lt;h2 id="1-the-challenge-of-representing-real-numbers"&gt;1. The Challenge of Representing Real Numbers&lt;/h2&gt;
&lt;p&gt;Computers work with finite binary representations, but real numbers are infinite.&lt;/p&gt;
&lt;h3 id="11-the-fundamental-problem"&gt;1.1 The Fundamental Problem&lt;/h3&gt;
&lt;div class="highlight"&gt;&lt;pre tabindex="0" style="color:#e6edf3;background-color:#0d1117;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-text-size-adjust:none;"&gt;&lt;code class="language-text" data-lang="text"&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;Integers are straightforward:
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;42 in binary = 101010
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;-7 in binary (two&amp;#39;s complement, 8-bit) = 11111001
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;But real numbers have infinite precision:
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;π = 3.14159265358979323846...
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;1/3 = 0.33333333... (repeating)
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;Even simple decimals can be infinite in binary:
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;0.1 (decimal) = 0.0001100110011... (repeating in binary)
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h3 id="12-why-not-fixed-point"&gt;1.2 Why Not Fixed Point?&lt;/h3&gt;
&lt;p&gt;Fixed point representations dedicate a fixed number of bits to the integer and fractional parts:&lt;/p&gt;</description></item><item><title>Garbage Collection Algorithms: From Mark-and-Sweep to ZGC</title><link>https://lbenicio.dev/blog/garbage-collection-algorithms-from-mark-and-sweep-to-zgc/</link><pubDate>Tue, 22 Nov 2022 00:00:00 +0000</pubDate><guid>https://lbenicio.dev/blog/garbage-collection-algorithms-from-mark-and-sweep-to-zgc/</guid><description>&lt;p&gt;Manual memory management is powerful but error-prone. Forget to free memory and you leak; free too early and you corrupt. Garbage collection promises to solve this by automatically reclaiming unused memory. But this convenience comes with costs and trade-offs that every systems programmer should understand. This post explores garbage collection from first principles to cutting-edge concurrent collectors.&lt;/p&gt;
&lt;h2 id="1-why-garbage-collection"&gt;1. Why Garbage Collection?&lt;/h2&gt;
&lt;p&gt;Consider the challenges of manual memory management:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre tabindex="0" style="color:#e6edf3;background-color:#0d1117;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-text-size-adjust:none;"&gt;&lt;code class="language-c" data-lang="c"&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;&lt;span style="color:#ff7b72"&gt;void&lt;/span&gt; &lt;span style="color:#d2a8ff;font-weight:bold"&gt;process_request&lt;/span&gt;(Request&lt;span style="color:#ff7b72;font-weight:bold"&gt;*&lt;/span&gt; req) {
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; &lt;span style="color:#ff7b72"&gt;char&lt;/span&gt;&lt;span style="color:#ff7b72;font-weight:bold"&gt;*&lt;/span&gt; buffer &lt;span style="color:#ff7b72;font-weight:bold"&gt;=&lt;/span&gt; &lt;span style="color:#d2a8ff;font-weight:bold"&gt;malloc&lt;/span&gt;(&lt;span style="color:#a5d6ff"&gt;1024&lt;/span&gt;);
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; Result&lt;span style="color:#ff7b72;font-weight:bold"&gt;*&lt;/span&gt; result &lt;span style="color:#ff7b72;font-weight:bold"&gt;=&lt;/span&gt; &lt;span style="color:#d2a8ff;font-weight:bold"&gt;compute&lt;/span&gt;(req, buffer);
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; &lt;span style="color:#ff7b72"&gt;if&lt;/span&gt; (result&lt;span style="color:#ff7b72;font-weight:bold"&gt;-&amp;gt;&lt;/span&gt;error) {
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; &lt;span style="color:#8b949e;font-style:italic"&gt;// Oops! Forgot to free buffer
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; &lt;span style="color:#ff7b72"&gt;return&lt;/span&gt;;
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; }
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; &lt;span style="color:#d2a8ff;font-weight:bold"&gt;send_response&lt;/span&gt;(result);
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; &lt;span style="color:#d2a8ff;font-weight:bold"&gt;free&lt;/span&gt;(buffer);
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; &lt;span style="color:#d2a8ff;font-weight:bold"&gt;free&lt;/span&gt;(result); &lt;span style="color:#8b949e;font-style:italic"&gt;// Did compute() allocate this? Or is it static?
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;}
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Common bugs include:&lt;/p&gt;</description></item><item><title>Timeouts, Retries, and Idempotency Keys: A Practical Guide</title><link>https://lbenicio.dev/blog/timeouts-retries-and-idempotency-keys-a-practical-guide/</link><pubDate>Thu, 08 Sep 2022 10:00:00 +0000</pubDate><guid>https://lbenicio.dev/blog/timeouts-retries-and-idempotency-keys-a-practical-guide/</guid><description>&lt;p&gt;Every distributed call needs three decisions: how long to wait, how to retry, and how to avoid duplicate effects. Done right, these three give you calm dashboards during partial failures; done wrong, they turn blips into incidents. This is a practical guide you can wire into client libraries and services without heroics.&lt;/p&gt;
&lt;h2 id="budget-your-time-not-perhop-timeouts"&gt;Budget your time, not per‑hop timeouts&lt;/h2&gt;
&lt;p&gt;Propagate a request deadline. Each hop consumes a slice and passes the remainder. Treat it as a budget: if it’s nearly spent, fail fast. Per‑hop fixed timeouts accumulate and blow your SLOs because each layer waits its entire local timeout.&lt;/p&gt;</description></item><item><title>Teaching GraphQL to Cache at the Edge</title><link>https://lbenicio.dev/blog/teaching-graphql-to-cache-at-the-edge/</link><pubDate>Sat, 03 Sep 2022 12:15:00 +0000</pubDate><guid>https://lbenicio.dev/blog/teaching-graphql-to-cache-at-the-edge/</guid><description>&lt;p&gt;GraphQL promises tailor-made responses, but tailor-made payloads resist caching. For years, we treated GraphQL responses as ephemeral: generated on demand, personalized, too unique to reuse. Then mobile latency complaints reached a boiling point. Edge locations sat underutilized while origin clusters sweated. We set out to teach GraphQL how to cache—respecting declarative queries, personalization boundaries, and real-time freshness. This is the story of building an edge caching layer that felt invisible to developers yet shaved hundreds of milliseconds off user interactions.&lt;/p&gt;</description></item><item><title>Designing CRDT-Powered Collaboration Platforms that Stay Consistent</title><link>https://lbenicio.dev/blog/designing-crdt-powered-collaboration-platforms-that-stay-consistent/</link><pubDate>Wed, 17 Aug 2022 00:00:00 +0000</pubDate><guid>https://lbenicio.dev/blog/designing-crdt-powered-collaboration-platforms-that-stay-consistent/</guid><description>&lt;p&gt;Real-time collaboration has shifted from a feature to a baseline expectation. Teams sketch ideas in shared whiteboards, edit code together, and annotate product specs while sitting on unreliable networks. Delivering that experience demands more than WebSockets and optimistic UI code. The hard part is consistency: making sure each participant converges on the same state despite offline edits, concurrent operations, and shaky connectivity. Conflict-free replicated data types (CRDTs) emerged as the workhorse that keeps these experiences coherent.&lt;/p&gt;</description></item><item><title>CPU Caches and Cache Coherence: The Memory Hierarchy That Makes Modern Computing Fast</title><link>https://lbenicio.dev/blog/cpu-caches-and-cache-coherence-the-memory-hierarchy-that-makes-modern-computing-fast/</link><pubDate>Tue, 12 Jul 2022 00:00:00 +0000</pubDate><guid>https://lbenicio.dev/blog/cpu-caches-and-cache-coherence-the-memory-hierarchy-that-makes-modern-computing-fast/</guid><description>&lt;p&gt;Modern CPUs can execute billions of instructions per second, but main memory takes hundreds of cycles to respond. Without caches, processors would spend most of their time waiting for data. The cache hierarchy is one of the most important innovations in computer architecture, and understanding it is essential for writing high-performance software.&lt;/p&gt;
&lt;h2 id="1-the-memory-wall-problem"&gt;1. The Memory Wall Problem&lt;/h2&gt;
&lt;p&gt;The fundamental challenge that caches solve.&lt;/p&gt;
&lt;h3 id="11-the-speed-gap"&gt;1.1 The Speed Gap&lt;/h3&gt;
&lt;div class="highlight"&gt;&lt;pre tabindex="0" style="color:#e6edf3;background-color:#0d1117;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-text-size-adjust:none;"&gt;&lt;code class="language-text" data-lang="text"&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;Component Access Time Relative Speed
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;─────────────────────────────────────────────────
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;CPU Register ~0.3 ns 1x (baseline)
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;L1 Cache ~1 ns ~3x slower
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;L2 Cache ~3-4 ns ~10x slower
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;L3 Cache ~10-20 ns ~30-60x slower
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;Main Memory ~50-100 ns ~150-300x slower
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;NVMe SSD ~20,000 ns ~60,000x slower
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;HDD ~10,000,000 ns ~30,000,000x slower
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h3 id="12-why-the-gap-exists"&gt;1.2 Why the Gap Exists&lt;/h3&gt;
&lt;div class="highlight"&gt;&lt;pre tabindex="0" style="color:#e6edf3;background-color:#0d1117;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-text-size-adjust:none;"&gt;&lt;code class="language-text" data-lang="text"&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;Memory technology tradeoffs:
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;SRAM (caches):
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;├─ Fast: 6 transistors per bit
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;├─ Expensive: ~100x cost per bit vs DRAM
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;├─ Power hungry
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;└─ Low density
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;DRAM (main memory):
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;├─ Slow: 1 transistor + 1 capacitor per bit
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;├─ Cheap: high density
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;├─ Needs refresh (capacitors leak)
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;└─ Better power per bit
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h3 id="13-the-solution-caching"&gt;1.3 The Solution: Caching&lt;/h3&gt;
&lt;p&gt;Caches exploit two key principles:&lt;/p&gt;</description></item><item><title>Virtual Memory and Page Tables: How Modern Systems Manage Memory</title><link>https://lbenicio.dev/blog/virtual-memory-and-page-tables-how-modern-systems-manage-memory/</link><pubDate>Thu, 19 May 2022 00:00:00 +0000</pubDate><guid>https://lbenicio.dev/blog/virtual-memory-and-page-tables-how-modern-systems-manage-memory/</guid><description>&lt;p&gt;Every process believes it has exclusive access to a vast, contiguous memory space. This elegant illusion—virtual memory—is one of the most important abstractions in computing. Behind it lies a sophisticated system of page tables, TLBs, and hardware-software cooperation that enables memory isolation, efficient sharing, and seemingly infinite memory. Let&amp;rsquo;s explore how it all works.&lt;/p&gt;
&lt;h2 id="1-the-problem-why-virtual-memory"&gt;1. The Problem: Why Virtual Memory?&lt;/h2&gt;
&lt;p&gt;Before virtual memory, programs used physical addresses directly. This created serious problems.&lt;/p&gt;</description></item><item><title>Process Scheduling and Context Switching: How Operating Systems Share the CPU</title><link>https://lbenicio.dev/blog/process-scheduling-and-context-switching-how-operating-systems-share-the-cpu/</link><pubDate>Wed, 18 May 2022 00:00:00 +0000</pubDate><guid>https://lbenicio.dev/blog/process-scheduling-and-context-switching-how-operating-systems-share-the-cpu/</guid><description>&lt;p&gt;A single CPU core can only execute one instruction stream at a time, yet modern systems run hundreds of processes simultaneously. This illusion of parallelism requires the operating system to rapidly switch between processes, giving each a slice of CPU time. The scheduler—the component that decides who runs next—profoundly affects system responsiveness, throughput, and fairness. Understanding scheduling reveals why your interactive applications feel smooth or sluggish and why some workloads perform better than others.&lt;/p&gt;</description></item><item><title>Software Supply Chain Security: SBOMs, Sigstore, Reproducible Builds, and Attestation</title><link>https://lbenicio.dev/blog/software-supply-chain-security-sboms-sigstore-reproducible-builds-and-attestation/</link><pubDate>Tue, 19 Apr 2022 00:00:00 +0000</pubDate><guid>https://lbenicio.dev/blog/software-supply-chain-security-sboms-sigstore-reproducible-builds-and-attestation/</guid><description>&lt;p&gt;The software supply chain is the set of processes, tools, and artifacts that transform source code into software that runs in production. Protecting that supply chain is critical: compromises at any step — compromised dependencies, corrupted CI/CD pipelines, or forged releases — can lead to large-scale incidents that affect thousands of downstream systems.&lt;/p&gt;
&lt;p&gt;This post explains the modern toolbox for defending the software supply chain: Software Bill of Materials (SBOM) formats and generation, provenance and attestation concepts, Sigstore for transparent signature &amp;amp; provenance, reproducible builds, SLSA (Supply chain Levels for Software Artifacts), secure signing and key management, package ecosystem-specific hazards (npm, PyPI, crates.io), and operational practices for CI/CD and incident response.&lt;/p&gt;</description></item><item><title>Branch Prediction and Speculative Execution: How Modern CPUs Gamble on the Future</title><link>https://lbenicio.dev/blog/branch-prediction-and-speculative-execution-how-modern-cpus-gamble-on-the-future/</link><pubDate>Sun, 15 Aug 2021 00:00:00 +0000</pubDate><guid>https://lbenicio.dev/blog/branch-prediction-and-speculative-execution-how-modern-cpus-gamble-on-the-future/</guid><description>&lt;p&gt;Modern CPUs are marvels of prediction. Every time your code branches—every if statement, every loop iteration, every function call—the processor makes a bet on what happens next. Get it right, and execution flows at full speed. Get it wrong, and the pipeline stalls while work is thrown away. Understanding branch prediction transforms how you think about code performance. This post explores the algorithms, trade-offs, and real-world implications of one of computing&amp;rsquo;s most important optimizations.&lt;/p&gt;</description></item><item><title>Virtual Memory and Page Tables: How Operating Systems Manage Memory</title><link>https://lbenicio.dev/blog/virtual-memory-and-page-tables-how-operating-systems-manage-memory/</link><pubDate>Thu, 12 Aug 2021 00:00:00 +0000</pubDate><guid>https://lbenicio.dev/blog/virtual-memory-and-page-tables-how-operating-systems-manage-memory/</guid><description>&lt;p&gt;Every process believes it has the entire machine to itself. It sees a vast, contiguous address space starting from zero, completely isolated from other processes. This illusion is virtual memory—one of the most important abstractions in computing. Understanding how operating systems and hardware collaborate to maintain this illusion reveals fundamental insights about performance, security, and system design.&lt;/p&gt;
&lt;h2 id="1-the-need-for-virtual-memory"&gt;1. The Need for Virtual Memory&lt;/h2&gt;
&lt;p&gt;Before virtual memory, programming was a constant juggling act.&lt;/p&gt;</description></item><item><title>B-Trees and LSM-Trees: The Foundations of Modern Storage Engines</title><link>https://lbenicio.dev/blog/b-trees-and-lsm-trees-the-foundations-of-modern-storage-engines/</link><pubDate>Wed, 14 Jul 2021 00:00:00 +0000</pubDate><guid>https://lbenicio.dev/blog/b-trees-and-lsm-trees-the-foundations-of-modern-storage-engines/</guid><description>&lt;p&gt;Every database faces the same fundamental challenge: how do you organize data on disk so that both reads and writes are fast? Two data structures have emerged as the dominant answers—B-Trees and LSM-Trees. Understanding their trade-offs is essential for anyone building or operating data-intensive systems. This post explores both in depth, from their internal mechanics to their real-world implementations.&lt;/p&gt;
&lt;h2 id="1-the-storage-engine-problem"&gt;1. The Storage Engine Problem&lt;/h2&gt;
&lt;p&gt;Before diving into specific data structures, let&amp;rsquo;s understand what we&amp;rsquo;re optimizing for.&lt;/p&gt;</description></item><item><title>CPU Caches and Memory Hierarchy: The Hidden Architecture Behind Performance</title><link>https://lbenicio.dev/blog/cpu-caches-and-memory-hierarchy-the-hidden-architecture-behind-performance/</link><pubDate>Tue, 22 Jun 2021 00:00:00 +0000</pubDate><guid>https://lbenicio.dev/blog/cpu-caches-and-memory-hierarchy-the-hidden-architecture-behind-performance/</guid><description>&lt;p&gt;The gap between processor speed and memory speed is one of the defining challenges of modern computing. While CPUs can execute billions of operations per second, main memory takes hundreds of cycles to respond to a single request. CPU caches bridge this gap through a hierarchy of progressively larger and slower memories that exploit the patterns in how programs access data. Understanding cache behavior transforms how you think about algorithm design, data structure layout, and system performance.&lt;/p&gt;</description></item><item><title>Instrumenting Without Spying: Privacy-Preserving Telemetry at Scale</title><link>https://lbenicio.dev/blog/instrumenting-without-spying-privacy-preserving-telemetry-at-scale/</link><pubDate>Thu, 27 May 2021 18:45:00 +0000</pubDate><guid>https://lbenicio.dev/blog/instrumenting-without-spying-privacy-preserving-telemetry-at-scale/</guid><description>&lt;p&gt;Three years ago, our observability dashboards flickered ominously: ingestion lag, missing metrics, red error bars. We had paused telemetry from millions of devices after discovering that our pipeline collected more than we were comfortable storing. The pause kept our promise to users—but left engineers blind. We needed a way to reinstate observability without betraying trust. The result was a radical redesign: a privacy-preserving telemetry system that treats data minimization as a feature, not a compliance chore.&lt;/p&gt;</description></item><item><title>Deterministic Monorepo CI Platforms: Engineering Consistency at Scale</title><link>https://lbenicio.dev/blog/deterministic-monorepo-ci-platforms-engineering-consistency-at-scale/</link><pubDate>Fri, 23 Apr 2021 00:00:00 +0000</pubDate><guid>https://lbenicio.dev/blog/deterministic-monorepo-ci-platforms-engineering-consistency-at-scale/</guid><description>&lt;p&gt;Monorepos promise unified visibility, atomic changes, and a single source of truth. They also create CI/CD nightmares if builds become flaky, nondeterministic, or painfully slow. Deterministic CI platforms tame that chaos by ensuring every pipeline run produces the same outputs given the same inputs—no matter which engineer kicks it off, which runner executes it, or when it happens. This article is a long-form walkthrough for platform teams charged with shipping deterministic CI in sprawling monorepos covering mobile apps, microservices, infrastructure code, and ML assets. Expect hard-earned lessons, architectural blueprints, and operational playbooks drawn from companies that run tens of thousands of jobs per day without losing their sanity.&lt;/p&gt;</description></item><item><title>System Calls: The Gateway Between User Space and Kernel</title><link>https://lbenicio.dev/blog/system-calls-the-gateway-between-user-space-and-kernel/</link><pubDate>Sun, 18 Apr 2021 00:00:00 +0000</pubDate><guid>https://lbenicio.dev/blog/system-calls-the-gateway-between-user-space-and-kernel/</guid><description>&lt;p&gt;Every time your program opens a file, allocates memory, or sends a network packet, it crosses an invisible boundary. User programs cannot directly access hardware or kernel data structures—they must ask the operating system to do it for them through system calls. Understanding this interface is fundamental to systems programming and helps explain performance characteristics, security boundaries, and the design of operating systems themselves.&lt;/p&gt;
&lt;h2 id="1-the-user-kernel-boundary"&gt;1. The User-Kernel Boundary&lt;/h2&gt;
&lt;p&gt;Modern operating systems divide the world into two privilege levels.&lt;/p&gt;</description></item><item><title>Cache‑Friendly Data Layouts: AoS vs. SoA (and the Hybrid In‑Between)</title><link>https://lbenicio.dev/blog/cachefriendly-data-layouts-aos-vs.-soa-and-the-hybrid-inbetween/</link><pubDate>Thu, 18 Mar 2021 10:00:00 +0000</pubDate><guid>https://lbenicio.dev/blog/cachefriendly-data-layouts-aos-vs.-soa-and-the-hybrid-inbetween/</guid><description>&lt;p&gt;The fastest code you’ll ever write usually isn’t a brand‑new algorithm—it’s the same algorithm organized in memory so the CPU (or GPU) can consume it efficiently. That journey often starts with a deceptively small choice: Array‑of‑Structs (AoS) or Struct‑of‑Arrays (SoA). The layout you choose determines which cachelines move, which prefetchers trigger, how branch predictors behave, and whether SIMD units stay busy or starve.&lt;/p&gt;
&lt;p&gt;In this post we’ll build an intuitive mental model for cachelines, TLBs, prefetchers, and vector units; examine AoS and SoA under realistic access patterns; then derive a hybrid (AoSoA) that quietly powers many high‑performance systems—from physics engines to databases to ML dataloaders. We’ll also walk through migration strategies, benchmarking pitfalls, and checklists you can apply this week.&lt;/p&gt;</description></item><item><title>Raft Fast‑Commit and PreVote in Practice</title><link>https://lbenicio.dev/blog/raft-fastcommit-and-prevote-in-practice/</link><pubDate>Mon, 09 Nov 2020 10:00:00 +0000</pubDate><guid>https://lbenicio.dev/blog/raft-fastcommit-and-prevote-in-practice/</guid><description>&lt;p&gt;Raft’s original paper optimizes for understandability. Production clusters optimize for availability and mean‑time‑to‑recovery. Two common extensions—PreVote and fast‑commit—reduce needless disruptions and trim the time it takes to make progress. Let’s unpack them without hand‑waving.&lt;/p&gt;
&lt;h2 id="the-pain-disruptive-elections-and-long-commit-paths"&gt;The pain: disruptive elections and long commit paths&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Without PreVote, a partitioned follower may increment term and trigger elections on rejoin, ousting a healthy leader.&lt;/li&gt;
&lt;li&gt;Without fast‑commit, clients wait for the full round‑trip alignment even when a quorum already holds the entry durably.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="prevote"&gt;PreVote&lt;/h2&gt;
&lt;p&gt;PreVote adds a “dry‑run” phase: before incrementing term, a candidate asks peers if they’d vote. Peers reject if their logs are more up‑to‑date.&lt;/p&gt;</description></item><item><title>Network Sockets and the TCP/IP Stack: How Data Travels Across Networks</title><link>https://lbenicio.dev/blog/network-sockets-and-the-tcp/ip-stack-how-data-travels-across-networks/</link><pubDate>Sun, 08 Nov 2020 00:00:00 +0000</pubDate><guid>https://lbenicio.dev/blog/network-sockets-and-the-tcp/ip-stack-how-data-travels-across-networks/</guid><description>&lt;p&gt;Every web request, database query, and API call travels through the network stack. The simple act of opening a connection hides layers of protocol machinery handling packet routing, reliable delivery, congestion control, and flow management. Understanding how sockets work and how data traverses the TCP/IP stack illuminates why networks behave as they do and how to build efficient networked applications.&lt;/p&gt;
&lt;h2 id="1-the-network-stack-overview"&gt;1. The Network Stack Overview&lt;/h2&gt;
&lt;p&gt;Before diving into details, let&amp;rsquo;s see the complete picture.&lt;/p&gt;</description></item><item><title>Safe Rollback Strategies for Distributed Databases</title><link>https://lbenicio.dev/blog/safe-rollback-strategies-for-distributed-databases/</link><pubDate>Sun, 08 Nov 2020 00:00:00 +0000</pubDate><guid>https://lbenicio.dev/blog/safe-rollback-strategies-for-distributed-databases/</guid><description>&lt;p&gt;Rollbacks are the fire escapes of distributed databases. We hope never to use them, but when outages, migrations, or bad deployments hit, a well-practiced rollback can save hours of business downtime and piles of customer tickets. Unfortunately, many organizations treat rollback planning as an afterthought—&amp;ldquo;we&amp;rsquo;ll just restore from backup&amp;rdquo;—only to discover data drift, cascading failures, and angry stakeholders when the time comes. This article lays out a disciplined approach to rollback strategies tailored for modern distributed databases (PostgreSQL clusters, cloud-native NoSQL, sharded NewSQL, event-sourced systems) where consistency, latency, and regulatory obligations collide.&lt;/p&gt;</description></item><item><title>Compiler Optimizations: From Source Code to Fast Machine Code</title><link>https://lbenicio.dev/blog/compiler-optimizations-from-source-code-to-fast-machine-code/</link><pubDate>Wed, 23 Sep 2020 00:00:00 +0000</pubDate><guid>https://lbenicio.dev/blog/compiler-optimizations-from-source-code-to-fast-machine-code/</guid><description>&lt;p&gt;When you compile your code with &lt;code&gt;-O2&lt;/code&gt; or &lt;code&gt;-O3&lt;/code&gt;, something magical happens. The compiler applies dozens of optimization passes that can make your program run 10x faster—or more. Understanding these optimizations helps you write faster code and debug mysterious performance issues. Let&amp;rsquo;s explore how modern compilers transform source code into efficient machine code.&lt;/p&gt;
&lt;h2 id="1-the-compilation-pipeline"&gt;1. The Compilation Pipeline&lt;/h2&gt;
&lt;p&gt;Before diving into optimizations, let&amp;rsquo;s understand where they happen.&lt;/p&gt;
&lt;h3 id="11-compiler-phases"&gt;1.1 Compiler Phases&lt;/h3&gt;
&lt;div class="highlight"&gt;&lt;pre tabindex="0" style="color:#e6edf3;background-color:#0d1117;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-text-size-adjust:none;"&gt;&lt;code class="language-text" data-lang="text"&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;Source Code
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; │
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; ▼
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;┌─────────────────┐
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;│ Front-end │ Parsing, type checking, AST generation
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;│ (Language- │ C, C++, Rust, Swift → IR
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;│ specific) │
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;└────────┬────────┘
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; │
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; ▼
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;┌─────────────────┐
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;│ Middle-end │ ★ MOST OPTIMIZATIONS HAPPEN HERE ★
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;│ (IR-based │ Constant folding, inlining, loop opts
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;│ optimization) │ Dead code elimination, vectorization
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;└────────┬────────┘
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; │
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; ▼
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;┌─────────────────┐
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;│ Back-end │ Instruction selection, register allocation
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;│ (Target- │ Instruction scheduling, peephole opts
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;│ specific) │
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;└────────┬────────┘
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; │
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; ▼
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; Machine Code
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h3 id="12-intermediate-representation-ir"&gt;1.2 Intermediate Representation (IR)&lt;/h3&gt;
&lt;p&gt;LLVM IR is a popular intermediate representation:&lt;/p&gt;</description></item><item><title>Merkle Trees and Content‑Addressable Storage</title><link>https://lbenicio.dev/blog/merkle-trees-and-contentaddressable-storage/</link><pubDate>Mon, 17 Aug 2020 10:00:00 +0000</pubDate><guid>https://lbenicio.dev/blog/merkle-trees-and-contentaddressable-storage/</guid><description>&lt;p&gt;Hash the content, not the location—that’s the core of content‑addressable storage (CAS). Combine it with Merkle trees (or DAGs) and you get efficient verification, deduplication, and synchronization. This post connects the dots from Git to large‑scale object stores.&lt;/p&gt;
&lt;h2 id="why-merkle"&gt;Why Merkle?&lt;/h2&gt;
&lt;p&gt;Parent hashes commit to child hashes; any change percolates up. You can verify integrity by checking a root hash and a short proof path.&lt;/p&gt;
&lt;h2 id="uses"&gt;Uses&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Git commits and trees; shallow clones via missing subtrees.&lt;/li&gt;
&lt;li&gt;Package managers with integrity checks.&lt;/li&gt;
&lt;li&gt;Deduplicated backups with chunking and rolling hashes.&lt;/li&gt;
&lt;li&gt;Object stores that replicate by exchanging missing subgraphs.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="practical-notes"&gt;Practical notes&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Hash choice (SHA‑256 vs BLAKE3) affects speed and hardware support.&lt;/li&gt;
&lt;li&gt;Chunking strategy controls dedupe granularity; rolling fingerprints (Rabin) find natural boundaries.&lt;/li&gt;
&lt;li&gt;Store metadata alongside blobs to avoid rehashing for trivial changes.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="1-from-trees-to-dags-modeling-versions-that-share-content"&gt;1) From trees to DAGs: modeling versions that share content&lt;/h2&gt;
&lt;p&gt;Classic Merkle trees have a fixed arity and two kinds of nodes: leaves containing hashes of fixed‑size blocks, and internal nodes containing hashes of their children. In real systems, multiple versions of content share substructure: two versions of a directory share most files, two backups share most chunks, two container images share many layers. That sharing naturally forms a directed acyclic graph (DAG) where a node can be referenced from multiple parents. The root is still a commitment to the whole, but now many roots can reference common subgraphs without duplication.&lt;/p&gt;</description></item><item><title>Consistent Hashing: Distributing Data Across Dynamic Clusters</title><link>https://lbenicio.dev/blog/consistent-hashing-distributing-data-across-dynamic-clusters/</link><pubDate>Sat, 28 Mar 2020 00:00:00 +0000</pubDate><guid>https://lbenicio.dev/blog/consistent-hashing-distributing-data-across-dynamic-clusters/</guid><description>&lt;p&gt;When you add a server to your distributed cache, what happens to all the cached data? With naive hashing, almost everything moves—a catastrophic reshuffling that defeats the purpose of caching. Consistent hashing solves this elegantly: only K/N keys need to move, where K is total keys and N is the number of servers. This simple idea underpins some of the most scalable systems ever built. Let&amp;rsquo;s explore how it works and why it matters.&lt;/p&gt;</description></item><item><title>Tuning the Dial: Adaptive Consistency at Planet Scale</title><link>https://lbenicio.dev/blog/tuning-the-dial-adaptive-consistency-at-planet-scale/</link><pubDate>Wed, 11 Mar 2020 14:05:00 +0000</pubDate><guid>https://lbenicio.dev/blog/tuning-the-dial-adaptive-consistency-at-planet-scale/</guid><description>&lt;p&gt;At 2:17 a.m. UTC, a partner bank in Singapore called our incident bridge. A fund transfer appeared twice in their ledger. The culprit: a replication lag spike between Singapore and Frankfurt had stretched past our standard safety buffers. Historically we would have halted writes across the fleet, cutting availability to protect consistency. Instead, our adaptive consistency layer dialed a region-specific policy: Singapore moved from &amp;ldquo;read-after-write&amp;rdquo; to &amp;ldquo;read-your-writes&amp;rdquo; guarantees, while Frankfurt raised its commit quorum. The double posting self-corrected before social media noticed. No downtime, no irreversible loss—just a story about how we learned to treat consistency as a spectrum rather than a binary.&lt;/p&gt;</description></item><item><title>When Data Centers Learned to Sleep: Energy-Aware Scheduling in Practice</title><link>https://lbenicio.dev/blog/when-data-centers-learned-to-sleep-energy-aware-scheduling-in-practice/</link><pubDate>Fri, 19 Jul 2019 09:30:00 +0000</pubDate><guid>https://lbenicio.dev/blog/when-data-centers-learned-to-sleep-energy-aware-scheduling-in-practice/</guid><description>&lt;p&gt;The first time we let a data center &amp;ldquo;sleep&amp;rdquo; during daylight hours felt reckless. Customers trusted us with near-infinite elasticity. Flipping servers into deep power states to save energy sounded like penny-pinching, not engineering. Yet the math was undeniable: idling a hyperscale fleet burned as much electricity as a mid-size city. This post tells the story of how we evolved from skepticism to confidence, building energy-aware scheduling systems that keep promises while honoring planetary limits.&lt;/p&gt;</description></item><item><title>Speculative Prefetchers: Designing Memory Systems That Read the Future</title><link>https://lbenicio.dev/blog/speculative-prefetchers-designing-memory-systems-that-read-the-future/</link><pubDate>Thu, 14 Feb 2019 10:00:00 +0000</pubDate><guid>https://lbenicio.dev/blog/speculative-prefetchers-designing-memory-systems-that-read-the-future/</guid><description>&lt;p&gt;At 2:17 a.m., the on-call performance engineer watches another alert crawl across the dashboard. The new machine image promised higher throughput for a latency-sensitive analytics service, yet caches still thrash whenever end-of-day reconciliation jobs arrive. Each job walks a sparsely linked graph of customer transactions, and the CPU spends more time waiting on memory than executing instructions. &amp;ldquo;If only the hardware could guess where the program was going next,&amp;rdquo; she sighs. That daydream is the seed of speculative prefetching—the art of reading tomorrow’s memory today.&lt;/p&gt;</description></item></channel></rss>