<!doctype html><html lang=en><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,maximum-scale=1,minimum-scale=1,user-scalable=no"><title>Write-Ahead Logging: The Unsung Hero of Database Durability · Leonardo Benicio</title><meta name=description content="Dive deep into write-ahead logging (WAL), the technique that lets databases promise durability without sacrificing performance. Learn how WAL works, why it matters, and how modern systems push its limits."><link rel=alternate type=application/rss+xml title=RSS href=https://lbenicio.dev/index.xml><link rel=canonical href=https://blog.lbenicio.dev/blog/write-ahead-logging-the-unsung-hero-of-database-durability/><link rel=preload href=/static/fonts/OpenSans-Regular.ttf as=font type=font/ttf crossorigin><link rel="stylesheet" href="/assets/css/fonts.min.40e2054b739ac45a0f9c940f4b44ec00c3b372356ebf61440a413c0337c5512e.css" crossorigin="anonymous" integrity="sha256-QOIFS3OaxFoPnJQPS0TsAMOzcjVuv2FECkE8AzfFUS4="><link rel="shortcut icon" href=/static/assets/favicon/favicon.ico><link rel=icon type=image/x-icon href=/static/assets/favicon/favicon.ico><link rel=icon href=/static/assets/favicon/favicon.svg type=image/svg+xml><link rel=icon href=/static/assets/favicon/favicon-32x32.png sizes=32x32 type=image/png><link rel=icon href=/static/assets/favicon/favicon-16x16.png sizes=16x16 type=image/png><link rel=apple-touch-icon href=/static/assets/favicon/apple-touch-icon.png><link rel=manifest href=/static/assets/favicon/site.webmanifest><link rel=mask-icon href=/static/assets/favicon/safari-pinned-tab.svg color=#209cee><meta name=msapplication-TileColor content="#209cee"><meta name=msapplication-config content="/static/assets/favicon/browserconfig.xml"><meta name=theme-color content="#d2e9f8"><meta property="og:title" content="Write-Ahead Logging: The Unsung Hero of Database Durability · Leonardo Benicio"><meta property="og:description" content="Dive deep into write-ahead logging (WAL), the technique that lets databases promise durability without sacrificing performance. Learn how WAL works, why it matters, and how modern systems push its limits."><meta property="og:url" content="https://blog.lbenicio.dev/blog/write-ahead-logging-the-unsung-hero-of-database-durability/"><meta property="og:type" content="article"><meta property="og:image" content="https://blog.lbenicio.dev/static/assets/images/blog/write-ahead-logging-durability-performance.png"><meta name=twitter:card content="summary_large_image"><meta name=twitter:title content="Write-Ahead Logging: The Unsung Hero of Database Durability · Leonardo Benicio"><meta name=twitter:description content="Dive deep into write-ahead logging (WAL), the technique that lets databases promise durability without sacrificing performance. Learn how WAL works, why it matters, and how modern systems push its limits."><meta name=twitter:site content="@lbenicio_"><script type=application/ld+json>{"@context":"https://schema.org","@type":"WebSite","name":"About Leonardo Benicio","url":"https://blog.lbenicio.dev"}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"Person","name":"Leonardo Benicio","sameAs":["https://github.com/lbenicio","https://www.linkedin.com/in/leonardo-benicio","https://twitter.com/lbenicio_"],"url":"https://blog.lbenicio.dev"}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","item":"https://blog.lbenicio.dev/","name":"Home","position":1},{"@type":"ListItem","item":"https://blog.lbenicio.dev/","name":"Blog","position":2},{"@type":"ListItem","item":"https://blog.lbenicio.dev/blog/write-ahead-logging-the-unsung-hero-of-database-durability/","name":"Write Ahead Logging the Unsung Hero of Database Durability","position":3}]}</script><link rel="stylesheet" href="/assets/css/main.min.1e8a566ac8bc3f0664d0db4ec8a015b07421c33fa11d336a6b914522a9cabf30.css" crossorigin="anonymous" integrity="sha256-6lhUOpwCHMSMROmggsVSp3AHKud6gBrIFGTzl3GV4BY="></head><body class="c6942b3 c03620d cf3bd2e"><script>(function(){try{document.addEventListener("gesturestart",function(e){e.preventDefault()}),document.addEventListener("touchstart",function(e){e.touches&&e.touches.length>1&&e.preventDefault()},{passive:!1});var e=0;document.addEventListener("touchend",function(t){var n=Date.now();n-e<=300&&t.preventDefault(),e=n},{passive:!1})}catch{}})()</script><a href=#content class="cba5854 c21e770 caffa6e cc5f604 cf2c31d cdd44dd c10dda9 c43876e c787e9b cddc2d2 cf55a7b c6dfb1e c9391e2">Skip to content</a>
<script>(function(){try{const e=localStorage.getItem("theme");e==="dark"&&document.documentElement.classList.add("dark");const t=document.querySelector('button[aria-label="Toggle theme"]');t&&t.setAttribute("aria-pressed",String(e==="dark"))}catch{}})();function toggleTheme(e){const s=document.documentElement,t=s.classList.toggle("dark");try{localStorage.setItem("theme",t?"dark":"light")}catch{}try{var n=e&&e.nodeType===1?e:document.querySelector('button[aria-label="Toggle theme"]');n&&n.setAttribute("aria-pressed",String(!!t))}catch{}}(function(){function e(){try{return document.documentElement.classList.contains("dark")?"dark":"light"}catch{return"light"}}function n(t){const n=document.getElementById("i98aca2"),s=document.getElementById("iad2af0"),o=document.getElementById("i975fb5");if(!n||!s||!o)return;try{n.style.transform="translateX(0)",n.style.transition||(n.style.transition="transform 200ms ease-out")}catch{}try{s.hidden=!1,s.style.display="block"}catch{}o.setAttribute("aria-expanded","true"),n.setAttribute("aria-hidden","false");try{document.body.classList.add("c150bbe")}catch{}const i=document.getElementById("i190984");i&&i.focus();try{window.umami&&typeof window.umami.track=="function"&&window.umami.track("mobile_menu_open",{page:location.pathname,theme:e(),source:t||"programmatic"})}catch{}}function t(t){const n=document.getElementById("i98aca2"),s=document.getElementById("iad2af0"),o=document.getElementById("i975fb5");if(!n||!s||!o)return;try{n.style.transform="translateX(100%)",n.style.transition||(n.style.transition="transform 200ms ease-out")}catch{}try{s.hidden=!0,s.style.display="none"}catch{}o.setAttribute("aria-expanded","false"),n.setAttribute("aria-hidden","true");try{document.body.classList.remove("c150bbe")}catch{}o.focus();try{window.umami&&typeof window.umami.track=="function"&&window.umami.track("mobile_menu_close",{page:location.pathname,theme:e(),source:t||"programmatic"})}catch{}}function s(e){e.key==="Escape"&&t("escape")}window.__openMobileMenu=n,window.__closeMobileMenu=t;try{window.addEventListener("keydown",s,!0)}catch{}})()</script><header class="cd019ba c98dfae cdd44dd cfdda01 c9ee25d ce2dc7a cd72dd7 cc0dc37" role=banner><div class="cfdda01 c6942b3 ccf47f4 c7c11d8"><a href=/ class="c87e2b0 c6942b3 c7c11d8 c1838fa cb594e4" aria-label=Home><img src=/static/assets/favicon/favicon.svg alt=Logo width=32 height=32 class="c3de71a c4d5191">
<span class="cf8f011 c4d1253 cbd72bc cd7e69e">Leonardo Benicio</span></a><div class="c6942b3 c85cbd4 c7c11d8 ca798da c1838fa c7a0580"><nav class="cc1689c cd9b445 c75065d c04bab1" aria-label=Main><a href=/ class="c4d1253 c9e4539 cbbda39 c01f421 c19ee42 c3ecea6">Home</a>
<a href=https://lbenicio.dev/about target=_blank rel="noopener noreferrer" class="c4d1253 c9e4539 cbbda39 c01f421 c19ee42 c3ecea6">About</a>
<a href=https://lbenicio.dev/timeline target=_blank rel="noopener noreferrer" class="c4d1253 c9e4539 cbbda39 c01f421 c19ee42 c3ecea6">Timeline</a>
<a href=https://lbenicio.dev/reading target=_blank rel="noopener noreferrer" class="c4d1253 c9e4539 cbbda39 c01f421 c19ee42 c3ecea6">Reading</a>
<a href=https://publications.lbenicio.dev target=_blank rel="noopener noreferrer" class="c4d1253 c9e4539 cbbda39 c01f421 c19ee42 c3ecea6">Publications</a>
<a href=https://lbenicio.dev/contact target=_blank rel="noopener noreferrer" class="c4d1253 c9e4539 cbbda39 c01f421 c19ee42 c3ecea6">Contact</a></nav><button id="i1d73d4" type=button class="c1d6c20 c81ac7c c6a899b c7c11d8 c1d0018 c10dda9 c8e184d c514027 c88daee c7a66a6 c097fa1 cfc01c7 c286dd7 c2bd687 cfdce1d cfef18f" onclick=toggleTheme(this) aria-label="Toggle theme" aria-pressed=false title="Toggle theme">
<svg class="cb26e41 c50ceea cb69a5c c4f45c8 c8c2c40" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" aria-hidden="true"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg>
<svg class="cb26e41 c8fca2b cb69a5c c4f45c8 cc1689c c9c27ff" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" aria-hidden="true"><circle cx="12" cy="12" r="4"/><path d="M12 2v4"/><path d="M12 18v4"/><path d="M2 12h4"/><path d="M18 12h4"/><path d="M4.93 4.93l2.83 2.83"/><path d="M16.24 16.24l2.83 2.83"/><path d="M6.34 17.66l2.83-2.83"/><path d="M14.83 9.17l2.83-2.83"/></svg>
<span class="cba5854">Toggle theme</span></button><div class="c658bcf c097fa1"><button id="i975fb5" type=button class="c1d6c20 c81ac7c c6a899b c7c11d8 c1d0018 c10dda9 c8e184d c514027 c88daee c7a66a6 cfc01c7 c286dd7 c2bd687 cfdce1d cfef18f" aria-label="Open menu" aria-controls="i98aca2" aria-expanded=false onclick='window.__openMobileMenu("button")' data-d38f920=mobile_menu_open_click>
<svg class="c20e4eb cb58471" fill="none" viewBox="0 0 24 24" stroke="currentColor"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M4 6h16M4 12h16M4 18h16"/></svg>
<span class="cba5854">Open menu</span></button></div></div></div></header><div id="iad2af0" class="caffa6e ce4b5f4 c14639a" style=background-color:hsl(var(--background)) hidden onclick='window.__closeMobileMenu("overlay")' data-d38f920=mobile_menu_overlay_click></div><aside id="i98aca2" class="caffa6e c9efbc5 c437fa9 c49e97e c6c6936 c7cacca c7b34a4 c787e9b c88daee cad071a c6942b3 c03620d" role=dialog aria-modal=true aria-hidden=true aria-label="Mobile navigation" style="transform:translateX(100%);transition:transform 200ms ease-out;will-change:transform"><div class="c6942b3 c7c11d8 c82c52d c5df473 ccf47f4 c9ee25d"><a href=/ class="c6942b3 c7c11d8 c1838fa" aria-label=Home><img src=/static/assets/favicon/favicon.svg alt=Logo width=24 height=24 class="c20e4eb cb58471">
<span class="c62aaf0 c7c1b66 cbd72bc">Leonardo Benicio</span>
</a><button id="i190984" type=button class="c1d6c20 c81ac7c c6a899b c7c11d8 c1d0018 c10dda9 c514027 c286dd7 c2bd687 cfdce1d" aria-label="Close menu" onclick='window.__closeMobileMenu("button")' data-d38f920=mobile_menu_close_click>
<svg class="c16e528 c61f467" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M18 6 6 18"/><path d="m6 6 12 12"/></svg>
<span class="cba5854">Close</span></button></div><nav class="c85cbd4 ca0eaa4 c5df473 c6689b9"><ul class="cd69733"><li><a href=/ class="c3b5299 c10dda9 cddc2d2 cf55a7b c7c1b66 cbbda39 c3ecea6 c19ee42 c514027" onclick=window.__closeMobileMenu()>Home</a></li><li><a href=https://lbenicio.dev/about target=_blank rel="noopener noreferrer" class="c3b5299 c10dda9 cddc2d2 cf55a7b c7c1b66 cbbda39 c3ecea6 c19ee42 c514027" onclick=window.__closeMobileMenu()>About</a></li><li><a href=https://lbenicio.dev/timeline target=_blank rel="noopener noreferrer" class="c3b5299 c10dda9 cddc2d2 cf55a7b c7c1b66 cbbda39 c3ecea6 c19ee42 c514027" onclick=window.__closeMobileMenu()>Timeline</a></li><li><a href=https://lbenicio.dev/reading target=_blank rel="noopener noreferrer" class="c3b5299 c10dda9 cddc2d2 cf55a7b c7c1b66 cbbda39 c3ecea6 c19ee42 c514027" onclick=window.__closeMobileMenu()>Reading</a></li><li><a href=https://publications.lbenicio.dev target=_blank rel="noopener noreferrer" class="c3b5299 c10dda9 cddc2d2 cf55a7b c7c1b66 cbbda39 c3ecea6 c19ee42 c514027" onclick=window.__closeMobileMenu()>Publications</a></li><li><a href=https://lbenicio.dev/contact target=_blank rel="noopener noreferrer" class="c3b5299 c10dda9 cddc2d2 cf55a7b c7c1b66 cbbda39 c3ecea6 c19ee42 c514027" onclick=window.__closeMobileMenu()>Contact</a></li></ul></nav><div class="c60a4cc ccdf0e8 c277478 c13044e"><p>&copy; 2026 Leonardo Benicio</p></div></aside><div class="caffa6e c437fa9 ce9aced c97bba6 c15da2a c975cba" role=complementary aria-label="GitHub repository"><div class="c9d056d c252f85 ca22532 ca88a1a c876315"><div class="c6942b3 c7c11d8 c1d0018 cd1fd22 c6066e4 c43876e ce3d5b6 caa20d2 c3ecea6 c0cd2e2 cddc2d2 c3ed5c9 cd4074c c876315"><a href=https://github.com/lbenicio/aboutme target=_blank rel="noopener noreferrer" class="c6942b3 c7c11d8 cd1fd22 c71bae8 cfac1ac c19ee42 c25dc7c cb40739 cbbda39 cf55a7b" aria-label="View source on GitHub"><svg width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="cb26e41 c41bcd4 cf17690 cfa4e34 c78d562" aria-hidden="true"><path d="M15 22v-4a4.8 4.8.0 00-1-3.5c3 0 6-2 6-5.5.08-1.25-.27-2.48-1-3.5.28-1.15.28-2.35.0-3.5.0.0-1 0-3 1.5-2.64-.5-5.36-.5-8 0C6 2 5 2 5 2c-.3 1.15-.3 2.35.0 3.5A5.403 5.403.0 004 9c0 3.5 3 5.5 6 5.5-.39.49-.68 1.05-.85 1.65-.17.6-.22 1.23-.15 1.85v4"/><path d="M9 18c-4.51 2-5-2-7-2"/></svg>
<span class="cb5c327 cd7e69e">Fork me</span></a></div></div></div><main id="i7eccc0" class="cfdda01 c5df473 c0eecc8 c85cbd4" role=main aria-label=Content><nav class="cb545ce c8d8ae4 c277478" aria-label=Breadcrumb><ol class="c6942b3 c3adaf2 c7c11d8 cd365ee c3ecea6"><li><a href=/ class="c19ee42 c71bae8 cfac1ac">Home</a></li><li class="c6942b3 c7c11d8 cd365ee"><span class="cb82ddd">/</span><a href=/ class="c19ee42 c71bae8 cfac1ac">Blog</a></li><li class="c6942b3 c7c11d8 cd365ee"><span class="cb82ddd">/</span><span class="c88daee">Write Ahead Logging the Unsung Hero of Database Durability</span></li></ol></nav><article class="c461ba0 c1c203f cfb6084 c995404 c6ca165"><nav class="cb545ce c8d8ae4 c277478" aria-label=Breadcrumb><ol class="c6942b3 c3adaf2 c7c11d8 cd365ee c3ecea6"><li><a href=/ class="c19ee42 c71bae8 cfac1ac">Home</a></li><li class="c6942b3 c7c11d8 cd365ee"><span class="cb82ddd">/</span><a href=/ class="c19ee42 c71bae8 cfac1ac">Blog</a></li><li class="c6942b3 c7c11d8 cd365ee"><span class="cb82ddd">/</span><span class="c88daee">Write Ahead Logging the Unsung Hero of Database Durability</span></li></ol></nav><header class="c8aedc7"><h1 class="cf304bc c6fb0fe cf8f011 cc484e1">Write-Ahead Logging: The Unsung Hero of Database Durability</h1><div class="c277478 c3ecea6 c8fb24a">2024-09-10
· Leonardo Benicio</div><div class="c1a1a3f c8124f2"><img src=/static/assets/images/blog/write-ahead-logging-durability-performance.png alt class="cfdda01 c524300 c677556"></div><p class="lead c3ecea6">Dive deep into write-ahead logging (WAL), the technique that lets databases promise durability without sacrificing performance. Learn how WAL works, why it matters, and how modern systems push its limits.</p></header><div class="content"><p>Every database makes a promise: your committed data will survive crashes, power failures, and hardware hiccups. This promise—durability—seems simple until you consider the physics. Disks are slow. Memory is volatile. Writes can be reordered. Yet somehow, databases deliver both durability and performance. The secret weapon? Write-ahead logging.</p><h2 id="1-the-durability-problem">1. The Durability Problem</h2><p>Consider what happens when you execute <code>UPDATE accounts SET balance = balance - 100 WHERE id="42</code">. The database must:</p><ol><li>Find the relevant page in memory (or load it from disk)</li><li>Modify the balance value</li><li>Eventually write the page back to disk</li></ol><p>The problem: step 3 is expensive. A single 4KB page write to an SSD takes 50-100 microseconds. To a spinning disk, it&rsquo;s 5-10 milliseconds. If we waited for every modification to reach disk before acknowledging the transaction, throughput would collapse.</p><p>But what if the system crashes between step 2 and step 3? The modification exists only in volatile memory—it&rsquo;s lost forever. Worse, what if the crash happens during step 3, leaving the page partially written and corrupted?</p><p>Databases need a way to guarantee durability without waiting for every page write. Write-ahead logging provides exactly that.</p><h2 id="2-the-write-ahead-logging-protocol">2. The Write-Ahead Logging Protocol</h2><p>Write-ahead logging (WAL) rests on a deceptively simple principle: before modifying any data page, first write a log record describing the change to a sequential log file, and ensure that log record reaches stable storage.</p><p>The protocol has three rules:</p><ol><li><p><strong>Log before data:</strong> A log record describing a modification must be written to stable storage before the modified data page is written.</p></li><li><p><strong>Commit record:</strong> When a transaction commits, a commit log record must be written to stable storage before the commit is acknowledged to the client.</p></li><li><p><strong>Sequential writes:</strong> Log records are appended sequentially, never modified in place.</p></li></ol><p>Why does this work? Sequential writes are fast—SSDs and HDDs both optimize for sequential access. A commit requires only one sequential write (the commit record), not random writes to potentially dozens of modified pages. The log captures enough information to reconstruct any changes, so even if data pages are lost, recovery can replay the log.</p><h3 id="21-anatomy-of-a-log-record">2.1 Anatomy of a Log Record</h3><p>A typical log record contains:</p><div class="highlight"><pre tabindex=0 style=color:#e6edf3;background-color:#0d1117;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-text-size-adjust:none><code class="language-text" data-lang=text><span style=display:flex><span>+--------+--------+--------+--------+----------------+--------+
</span></span><span style=display:flex><span>|  LSN   |  TxID  |  Type  | PageID |    Payload     |  CRC   |
</span></span><span style=display:flex><span>+--------+--------+--------+--------+----------------+--------+
</span></span></code></pre></div><ul><li><strong>LSN (Log Sequence Number):</strong> A monotonically increasing identifier for the record. Often the byte offset in the log file.</li><li><strong>TxID:</strong> The transaction that generated this record.</li><li><strong>Type:</strong> The operation type (INSERT, UPDATE, DELETE, COMMIT, ABORT, CHECKPOINT, etc.).</li><li><strong>PageID:</strong> The page affected by this operation.</li><li><strong>Payload:</strong> The actual change data. For an UPDATE, this might be the old and new values (or just the new value, depending on the logging strategy).</li><li><strong>CRC:</strong> Checksum for integrity verification.</li></ul><h3 id="22-logical-vs-physical-logging">2.2 Logical vs. Physical Logging</h3><p>There are two approaches to what goes in the payload:</p><p><strong>Physical logging</strong> records the exact bytes changed:</p><div class="highlight"><pre tabindex=0 style=color:#e6edf3;background-color:#0d1117;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-text-size-adjust:none><code class="language-text" data-lang=text><span style=display:flex><span>Page 42, offset 128: old=0x00000064 new=0x0000003C
</span></span></code></pre></div><p>This is simple and fast to apply during recovery, but generates large logs for operations that touch many bytes.</p><p><strong>Logical logging</strong> records the operation itself:</p><div class="highlight"><pre tabindex=0 style=color:#e6edf3;background-color:#0d1117;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-text-size-adjust:none><code class="language-text" data-lang=text><span style=display:flex><span>UPDATE accounts SET balance = balance - 100 WHERE id="42"
</span></span></code></pre></div><p>This is compact but requires re-executing the operation during recovery, which may be complex and must be deterministic.</p><p>Most systems use <strong>physiological logging</strong>—a hybrid. Log records are physical within a page (describing byte-level changes) but logical across pages (not recording the effects of page splits or other structural changes at the byte level).</p><h2 id="3-the-log-buffer-and-flushing">3. The Log Buffer and Flushing</h2><p>Writing every log record immediately to disk would defeat the purpose—we&rsquo;d trade random page writes for nearly as many sequential log writes. Instead, databases buffer log records in memory and flush them in batches.</p><h3 id="31-group-commit">3.1 Group Commit</h3><p>When multiple transactions commit around the same time, their commit records can be flushed together in a single I/O operation. This is <strong>group commit</strong>:</p><ol><li>Transaction A commits, queues its commit record, waits</li><li>Transaction B commits, queues its commit record, waits</li><li>Transaction C commits, queues its commit record, triggers flush</li><li>All three commit records write in one I/O</li><li>All three transactions are acknowledged</li></ol><p>Group commit amortizes the cost of fsync() across multiple transactions. Under high concurrency, this can improve throughput by 10-100×.</p><h3 id="32-flush-policies">3.2 Flush Policies</h3><p>When should the log buffer flush?</p><ul><li><strong>On commit:</strong> Always flush at least up to the transaction&rsquo;s commit record. This is required for durability.</li><li><strong>Buffer full:</strong> When the log buffer fills, flush to make room.</li><li><strong>Periodic:</strong> Flush every N milliseconds to bound how much work would be lost on crash.</li><li><strong>On checkpoint:</strong> Flush all dirty log records before writing a checkpoint.</li></ul><p>The tension is between latency (flush often = lower per-transaction latency but more I/O operations) and throughput (batch flushes = better I/O efficiency but higher latency for early arrivals in the batch).</p><h3 id="33-the-fsync-tax">3.3 The fsync() Tax</h3><p>Calling write() isn&rsquo;t enough—data may linger in the OS page cache. To guarantee durability, databases must call fsync() (or fdatasync(), or use O_DIRECT with O_SYNC). This forces data to stable storage but is expensive:</p><ul><li>SSD: 50-200 microseconds</li><li>HDD: 5-15 milliseconds (a full rotation)</li><li>Battery-backed RAID controller: may acknowledge immediately if write-back cache is trusted</li></ul><p>The fsync() cost dominates commit latency for durable transactions. Optimizations focus on reducing fsync() frequency (group commit) or hiding its latency (asynchronous commits with risk disclosure).</p><h2 id="4-checkpoints-bounding-recovery-time">4. Checkpoints: Bounding Recovery Time</h2><p>If the system crashes, recovery must replay the log from some starting point. But replaying the entire log since the database was created would take forever. Checkpoints bound recovery time by establishing points where all committed data is known to be on disk.</p><h3 id="41-checkpoint-types">4.1 Checkpoint Types</h3><p><strong>Sharp checkpoint (quiesce):</strong></p><ol><li>Stop accepting new transactions</li><li>Wait for all active transactions to complete</li><li>Flush all dirty pages to disk</li><li>Write a checkpoint record to the log</li><li>Resume normal operation</li></ol><p>This is simple but causes a pause—unacceptable for production systems.</p><p><strong>Fuzzy checkpoint:</strong></p><ol><li>Record the current LSN as the checkpoint start</li><li>Write out dirty pages gradually, without stopping transactions</li><li>Track which pages were dirty at checkpoint start</li><li>Write a checkpoint record when all those pages are flushed</li></ol><p>Fuzzy checkpoints allow continuous operation. Recovery starts from the checkpoint LSN and replays any log records after it.</p><p><strong>Incremental checkpoint:</strong></p><p>Track dirty pages continuously. Periodically flush a subset of dirty pages (e.g., the oldest or coldest). The checkpoint &ldquo;advances&rdquo; as pages are flushed. This spreads I/O evenly over time.</p><h3 id="42-checkpoint-contents">4.2 Checkpoint Contents</h3><p>A checkpoint record typically includes:</p><ul><li>The LSN at checkpoint start</li><li>A list of active transactions (for undo during recovery)</li><li>The dirty page table: which pages were modified and their first modifying LSN</li><li>Optionally, the transaction table: each active transaction&rsquo;s state</li></ul><p>This metadata enables efficient recovery—the system knows exactly which log records might need to be replayed and which transactions were in progress.</p><h2 id="5-recovery-aries-and-beyond">5. Recovery: ARIES and Beyond</h2><p>The canonical recovery algorithm is ARIES (Algorithms for Recovery and Isolation Exploiting Semantics), developed at IBM in the early 1990s. ARIES handles all the corner cases: crashes during recovery, nested transactions, and fine-grained locking.</p><h3 id="51-aries-recovery-phases">5.1 ARIES Recovery Phases</h3><p>Recovery proceeds in three phases:</p><h4 id="phase-1-analysis"><strong>Phase 1: Analysis</strong></h4><p>Scan the log forward from the last checkpoint. Reconstruct the dirty page table and transaction table as they were at crash time. Identify:</p><ul><li>Which pages might need redo (were dirty and might not have been flushed)</li><li>Which transactions were active (need undo)</li></ul><h5 id="phase-2-redo"><strong>Phase 2: Redo</strong></h5><p>Scan forward again, this time applying log records. For each record:</p><ul><li>If the page&rsquo;s LSN is less than the record&rsquo;s LSN, the change might be missing—reapply it</li><li>If the page&rsquo;s LSN is ≥ the record&rsquo;s LSN, the change already reached disk—skip it</li></ul><p>Redo is idempotent: applying a change multiple times has the same effect as applying it once. This is crucial—we don&rsquo;t know exactly which writes made it to disk.</p><h5 id="phase-3-undo"><strong>Phase 3: Undo</strong></h5><p>For each transaction that was active at crash time, roll back its changes. Process the log backward, undoing each operation for these transactions. Write <strong>Compensation Log Records (CLRs)</strong> for each undo operation—this ensures that if we crash during recovery, we don&rsquo;t undo the same operation twice.</p><h3 id="52-the-lsn-cornerstone-of-recovery">5.2 The LSN: Cornerstone of Recovery</h3><p>The Log Sequence Number appears everywhere:</p><ul><li>Each log record has an LSN</li><li>Each page stores the LSN of the last log record that modified it (the page LSN)</li><li>The dirty page table maps pages to their recovery LSN (first dirty modification)</li><li>CLRs reference the LSN of the record they&rsquo;re compensating</li></ul><p>During redo, comparing the page LSN to the log record&rsquo;s LSN determines whether to reapply. This simple comparison makes recovery correct even when we don&rsquo;t know exactly what reached disk.</p><h3 id="53-physiological-logging-and-page-splits">5.3 Physiological Logging and Page Splits</h3><p>Consider a B-tree insertion that causes a page split. Physical logging would need to record changes to:</p><ul><li>The original page (removed entries)</li><li>The new page (added entries, initialized headers)</li><li>The parent page (new pointer)</li><li>Possibly more pages if the split propagates</li></ul><p>This is complex and verbose. Physiological logging instead records:</p><ul><li>&ldquo;Split page P1 creating P2 with key K as separator&rdquo;</li></ul><p>During redo, the system re-executes the split logic. The page LSN check ensures we don&rsquo;t re-split an already-split page.</p><h2 id="6-wal-in-practice-postgresql">6. WAL in Practice: PostgreSQL</h2><p>PostgreSQL&rsquo;s WAL implementation illustrates real-world considerations.</p><h3 id="61-wal-segment-files">6.1 WAL Segment Files</h3><p>PostgreSQL organizes WAL into segment files, typically 16MB each. File names encode the timeline and segment number:</p><div class="highlight"><pre tabindex=0 style=color:#e6edf3;background-color:#0d1117;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-text-size-adjust:none><code class="language-text" data-lang=text><span style=display:flex><span>000000010000000000000001
</span></span><span style=display:flex><span>000000010000000000000002
</span></span><span style=display:flex><span>...
</span></span></code></pre></div><p>The timeline identifier supports point-in-time recovery (PITR) and replication branching.</p><h3 id="62-wal-buffers-and-background-writer">6.2 WAL Buffers and Background Writer</h3><p>The wal_buffers setting controls the shared memory WAL buffer size (default: ~16MB). Log records accumulate here until:</p><ul><li>A transaction commits (triggers flush up to its commit record)</li><li>The buffer fills</li><li>The background writer decides to flush</li></ul><p>The walwriter background process periodically flushes WAL buffers to reduce commit latency jitter.</p><h3 id="63-synchronous-vs-asynchronous-commit">6.3 Synchronous vs. Asynchronous Commit</h3><p>PostgreSQL&rsquo;s synchronous_commit setting offers trade-offs:</p><ul><li><strong>on (default):</strong> Commit waits for WAL flush to disk. Full durability.</li><li><strong>remote_write:</strong> In replication, commit waits for WAL to reach standby&rsquo;s memory. Durable if both nodes don&rsquo;t fail.</li><li><strong>local:</strong> Commit waits for local WAL flush only.</li><li><strong>off:</strong> Commit returns immediately. WAL flushes in background. Up to 3× wal_writer_delay of transactions could be lost on crash.</li></ul><p>Setting synchronous_commit = off is not &ldquo;turn off durability&rdquo;—it&rsquo;s &ldquo;accept a small window of potential loss for better latency.&rdquo;</p><h3 id="64-full-page-writes">6.4 Full Page Writes</h3><p>After a checkpoint, the first modification to a page writes the entire page image to WAL, not just the change. This handles torn pages: if the OS writes 4KB atomically but the database uses 8KB pages, a crash mid-write could corrupt the page. The full page image in WAL allows recovery to restore a consistent page.</p><p>This increases WAL volume significantly (full_page_writes = on is the default). Some file systems and hardware with atomic writes > page size can disable this.</p><h2 id="7-wal-in-practice-sqlite">7. WAL in Practice: SQLite</h2><p>SQLite, the embedded database, takes a different approach with its WAL mode.</p><h3 id="71-shadow-paging-vs-wal">7.1 Shadow Paging vs. WAL</h3><p>Originally, SQLite used shadow paging (rollback journal):</p><ol><li>Before modifying a page, copy the original to a rollback journal</li><li>Modify pages in place</li><li>On commit, delete the rollback journal</li><li>On crash, restore original pages from the journal</li></ol><p>This is simple but has drawbacks: writes are random (to both database and journal), and readers block writers.</p><p>WAL mode inverts this:</p><ol><li>Writes go to a separate WAL file</li><li>The main database file is not modified during transactions</li><li>Readers access the main database file plus relevant WAL entries</li><li>Periodically, WAL entries are &ldquo;checkpointed&rdquo; back to the main database</li></ol><h3 id="72-wal-mode-benefits">7.2 WAL Mode Benefits</h3><ul><li><strong>Readers don&rsquo;t block writers:</strong> The main database file is stable; readers access it directly while writers append to WAL.</li><li><strong>Writers don&rsquo;t block readers:</strong> Readers see a consistent snapshot from before the write started.</li><li><strong>Faster commits:</strong> Sequential WAL writes are faster than random database writes.</li><li><strong>Better concurrency:</strong> Multiple readers can proceed simultaneously with a writer.</li></ul><h3 id="73-wal-index-wal-index">7.3 WAL-Index (wal-index)</h3><p>SQLite maintains a WAL-index in shared memory mapping page numbers to WAL frame numbers. Readers consult this index to find the latest version of a page—either in WAL or the main database file.</p><p>The WAL-index uses a hash table for fast lookups and is rebuilt from the WAL file if corrupted or missing.</p><h2 id="8-wal-in-distributed-systems">8. WAL in Distributed Systems</h2><p>Distributed databases face additional challenges: coordinating logs across nodes, ensuring consistency, and handling network partitions.</p><h3 id="81-replicated-wal">8.1 Replicated WAL</h3><p>In primary-backup replication, the primary&rsquo;s WAL is shipped to standbys:</p><ol><li>Primary writes log records locally</li><li>Primary sends log records to standbys (synchronously or asynchronously)</li><li>Standbys apply log records to their copies</li><li>On primary failure, a standby with the most complete log takes over</li></ol><p>PostgreSQL streaming replication, MySQL binlog replication, and many others use this pattern.</p><h3 id="82-consensus-based-wal">8.2 Consensus-Based WAL</h3><p>Raft and Paxos turn the log into a replicated state machine:</p><ol><li>A leader proposes appending an entry to the log</li><li>Followers accept and persist the entry</li><li>Once a majority acknowledges, the entry is committed</li><li>All nodes apply committed entries in order</li></ol><p>The log is the source of truth; the database state is a derived projection of the log. This inverts the traditional model where the database is primary and the log is a recovery mechanism.</p><h3 id="83-log-structured-merge-trees-and-wal">8.3 Log-Structured Merge-Trees and WAL</h3><p>LSM-tree databases (LevelDB, RocksDB, Cassandra) blur the line between WAL and data storage:</p><ul><li>Writes go to an in-memory memtable and a WAL (for durability)</li><li>When the memtable fills, it&rsquo;s flushed to an SSTable (sorted string table) on disk</li><li>The WAL can be discarded once its contents are in SSTables</li></ul><p>The WAL here is purely for crash recovery of the memtable. The SSTables themselves form a log-structured store where data is written sequentially and compacted in the background.</p><h2 id="9-performance-optimization-techniques">9. Performance Optimization Techniques</h2><h3 id="91-reducing-fsync-frequency">9.1 Reducing fsync() Frequency</h3><p>The most impactful optimization is calling fsync() less often:</p><ul><li><strong>Group commit:</strong> Batch multiple commits into one fsync()</li><li><strong>Commit delay:</strong> Wait a few milliseconds before flushing to gather more commits (trade latency for throughput)</li><li><strong>Asynchronous commit:</strong> Return to client before fsync() completes (trade durability for latency)</li></ul><h3 id="92-parallel-log-io">9.2 Parallel Log I/O</h3><p>Some systems maintain multiple log files and stripe writes across them:</p><ul><li>Increases aggregate bandwidth</li><li>Requires careful coordination to maintain ordering guarantees</li><li>More complex recovery (merge multiple streams)</li></ul><h3 id="93-log-compression">9.3 Log Compression</h3><p>Compress log records before writing:</p><ul><li>Reduces I/O volume (log writes are often I/O-bound)</li><li>Adds CPU overhead (compression/decompression)</li><li>Must be careful about compression boundaries—don&rsquo;t want to decompress half a record</li></ul><p>LZ4 and Snappy offer good speed/ratio trade-offs for real-time compression.</p><h3 id="94-non-volatile-memory-nvm">9.4 Non-Volatile Memory (NVM)</h3><p>Intel Optane and similar persistent memory technologies change the game:</p><ul><li>Write latency: ~300 nanoseconds (vs. 50+ microseconds for SSD)</li><li>No fsync() needed—writes are immediately durable</li><li>Byte-addressable—no need for block-sized log records</li></ul><p>With NVM, WAL can become a persistent in-memory buffer. Some systems eliminate WAL entirely, directly persisting data structures to NVM with careful ordering.</p><h3 id="95-direct-io-and-io_uring">9.5 Direct I/O and io_uring</h3><p>Traditional I/O through the page cache adds overhead:</p><ul><li>Data copies from user space to kernel buffers</li><li>Page cache management</li><li>Potentially unnecessary read-ahead</li></ul><p>Direct I/O (O_DIRECT) bypasses the page cache, putting the database in full control. Combined with io_uring for asynchronous operations:</p><div class="highlight"><pre tabindex=0 style=color:#e6edf3;background-color:#0d1117;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-text-size-adjust:none><code class="language-c" data-lang=c><span style=display:flex><span><span style=color:#8b949e;font-style:italic>// Submit multiple log writes asynchronously
</span></span></span><span style=display:flex><span><span style=color:#d2a8ff;font-weight:700>io_uring_prep_write</span>(sqe, log_fd, buffer, size, offset);
</span></span><span style=display:flex><span><span style=color:#d2a8ff;font-weight:700>io_uring_submit</span>(ring);
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#8b949e;font-style:italic>// Later, reap completions
</span></span></span><span style=display:flex><span><span style=color:#d2a8ff;font-weight:700>io_uring_wait_cqe</span>(ring, <span style=color:#ff7b72;font-weight:700>&amp;</span>cqe);
</span></span></code></pre></div><p>This can significantly reduce latency and CPU overhead for log writes.</p><h2 id="10-wal-size-management">10. WAL Size Management</h2><p>The WAL grows indefinitely without intervention. Managing its size is essential.</p><h3 id="101-log-truncation">10.1 Log Truncation</h3><p>Old log records are no longer needed after:</p><ul><li>All transactions that generated them have committed or aborted</li><li>All dirty pages they modified have been checkpointed to disk</li><li>Any replicas have received and applied them</li></ul><p>The database tracks the &ldquo;oldest needed LSN&rdquo; and truncates the log up to that point.</p><h3 id="102-archive-and-point-in-time-recovery">10.2 Archive and Point-in-Time Recovery</h3><p>For disaster recovery, WAL segments can be archived before truncation:</p><ol><li>Copy filled WAL segments to archive storage (S3, HDFS, tape)</li><li>To recover: restore a base backup, then replay archived WAL segments</li></ol><p>This enables point-in-time recovery (PITR): restore to any moment by replaying WAL up to that LSN.</p><p>PostgreSQL&rsquo;s archive_command and restore_command configure this:</p><div class="highlight"><pre tabindex=0 style=color:#e6edf3;background-color:#0d1117;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-text-size-adjust:none><code class="language-text" data-lang=text><span style=display:flex><span>archive_command = &#39;cp %p /archive/%f&#39;
</span></span><span style=display:flex><span>restore_command = &#39;cp /archive/%f %p&#39;
</span></span></code></pre></div><h3 id="103-log-recycling">10.3 Log Recycling</h3><p>Instead of creating and deleting segment files, recycle them:</p><ol><li>Fill segment 001, mark ready for archive</li><li>Start writing to segment 002</li><li>When segment 001 is archived and no longer needed, rename it to 003</li><li>Reuse 003 after filling 002</li></ol><p>This reduces file system overhead and metadata updates.</p><h2 id="11-correctness-considerations">11. Correctness Considerations</h2><h3 id="111-torn-writes-and-checksums">11.1 Torn Writes and Checksums</h3><p>A power failure mid-write can leave partial data on disk. Defenses:</p><ul><li><strong>Checksums:</strong> Every log record includes a CRC. On recovery, validate checksums; discard corrupted records at the end.</li><li><strong>Double writes:</strong> Write to two locations; use the intact copy. (More common for data pages than log.)</li><li><strong>Atomic write units:</strong> Use hardware or file system features that guarantee atomic writes.</li></ul><h3 id="112-write-ordering">11.2 Write Ordering</h3><p>Databases assume certain write ordering guarantees:</p><ul><li>Log writes before data writes</li><li>Data writes before checkpoint records</li><li>fsync() before returning to client</li></ul><p>File systems and storage controllers may reorder writes. Use:</p><ul><li>fsync() / fdatasync() to force ordering</li><li>O_SYNC or O_DSYNC for immediate durability</li><li>Write barriers (deprecated on Linux, use explicit flushes)</li></ul><p>Beware of &ldquo;lying&rdquo; controllers that acknowledge writes before they reach stable storage. Battery-backed write caches are acceptable; volatile caches are not.</p><h3 id="113-testing-with-fault-injection">11.3 Testing with Fault Injection</h3><p>Databases should test crash recovery exhaustively:</p><ul><li><strong>Kill -9 tests:</strong> Stop the process abruptly and verify recovery</li><li><strong>Power failure simulation:</strong> Use tools like dm-flakey or libeatmydata to simulate crashes</li><li><strong>File system corruption:</strong> Inject bad blocks or truncate files</li><li><strong>Slow I/O:</strong> Delay writes to expose timing-related bugs</li></ul><p>Formal verification of WAL implementations (e.g., using TLA+) catches subtle bugs in the protocol logic.</p><h2 id="12-wal-alternatives-and-complements">12. WAL Alternatives and Complements</h2><h3 id="121-shadow-paging">12.1 Shadow Paging</h3><p>Instead of logging changes, maintain two copies of each page:</p><ul><li>Current: the live version</li><li>Shadow: the previous committed version</li></ul><p>On commit, atomically switch from shadow to current (update a single pointer). On abort or crash, the shadow version is intact.</p><p>Shadow paging has fallen out of favor:</p><ul><li>Fragmentation: pages scatter across disk</li><li>Pointer updates cascade (updating a leaf requires updating parent, etc.)</li><li>Concurrency is harder than with WAL</li></ul><p>LMDB uses a variant with copy-on-write: modified pages are written to new locations, and a root pointer update commits the transaction.</p><h3 id="122-command-logging">12.2 Command Logging</h3><p>Log the commands (SQL statements, operations) instead of their effects:</p><ul><li>Very compact logs</li><li>Requires deterministic execution for replay</li><li>Recovery replays commands, which may be slow</li></ul><p>VoltDB uses command logging, relying on deterministic execution within a partition.</p><h3 id="123-write-behind-logging-wbl">12.3 Write-Behind Logging (WBL)</h3><p>With NVM&rsquo;s fast writes, WAL&rsquo;s overhead becomes significant. Write-behind logging inverts the order:</p><ol><li>Write dirty data directly to NVM (fast)</li><li>Log which data was written (for recovery metadata)</li><li>On crash, analyze what might be inconsistent and recover</li></ol><p>This is speculative—WBL requires new data structures and recovery logic tailored to NVM&rsquo;s characteristics.</p><h2 id="13-case-study-wal-in-etcd">13. Case Study: WAL in etcd</h2><p>etcd, the distributed key-value store backing Kubernetes, uses WAL for durability within each Raft node.</p><h3 id="131-wal-structure">13.1 WAL Structure</h3><p>Each etcd node maintains a WAL directory:</p><div class="highlight"><pre tabindex=0 style=color:#e6edf3;background-color:#0d1117;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-text-size-adjust:none><code class="language-text" data-lang=text><span style=display:flex><span>wal/
</span></span><span style=display:flex><span>  0000000000000000-0000000000000000.wal
</span></span><span style=display:flex><span>  0000000000000000-0000000000001000.wal
</span></span><span style=display:flex><span>  ...
</span></span></code></pre></div><p>Segment files contain:</p><ul><li>Raft log entries (proposals, configuration changes)</li><li>Raft state (term, vote, commit index)</li><li>CRC records for integrity</li></ul><h3 id="132-snapshot-and-wal-compaction">13.2 Snapshot and WAL Compaction</h3><p>As the Raft log grows, etcd takes snapshots:</p><ol><li>Serialize the current key-value state to a snapshot file</li><li>Record the snapshot&rsquo;s index in the WAL</li><li>Truncate WAL entries before the snapshot index</li></ol><p>Recovery loads the latest snapshot, then replays WAL entries after it.</p><h3 id="133-fsync-on-every-entry">13.3 fsync() on Every Entry</h3><p>etcd fsync()s after every Raft entry by default—correctness requires durability before acknowledging. This limits single-node throughput but guarantees linearizability.</p><p>The &ndash;wal-fsync-interval flag allows batching for higher throughput with slightly weaker guarantees (suitable for some workloads).</p><h2 id="14-case-study-wal-in-apache-kafka">14. Case Study: WAL in Apache Kafka</h2><p>Kafka is fundamentally a distributed commit log. Each partition is an append-only log stored as segment files.</p><h3 id="141-log-segments">14.1 Log Segments</h3><p>Kafka partitions consist of segment files:</p><div class="highlight"><pre tabindex=0 style=color:#e6edf3;background-color:#0d1117;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-text-size-adjust:none><code class="language-text" data-lang=text><span style=display:flex><span>00000000000000000000.log
</span></span><span style=display:flex><span>00000000000000000000.index
</span></span><span style=display:flex><span>00000000000000012345.log
</span></span><span style=display:flex><span>00000000000000012345.index
</span></span><span style=display:flex><span>...
</span></span></code></pre></div><p>Each .log file contains messages; the .index file maps offsets to file positions for fast seeks.</p><h3 id="142-producer-acknowledgments">14.2 Producer Acknowledgments</h3><p>Kafka producers can choose durability level:</p><ul><li><strong>acks=0:</strong> Fire and forget. No durability guarantee.</li><li><strong>acks=1:</strong> Wait for leader to write to its log. Durable if leader doesn&rsquo;t fail.</li><li><strong>acks=all:</strong> Wait for all in-sync replicas to acknowledge. Full durability.</li></ul><p>The log.flush.interval.messages and log.flush.interval.ms settings control when Kafka calls fsync(). By default, Kafka relies on replication for durability, not fsync(), accepting small data loss windows for throughput.</p><h3 id="143-log-compaction">14.3 Log Compaction</h3><p>Kafka supports two retention policies:</p><ul><li><strong>Time/size-based:</strong> Delete segments older than N days or when log exceeds N bytes.</li><li><strong>Compaction:</strong> Retain only the latest value for each key. Useful for changelog topics.</li></ul><p>Compaction is a form of garbage collection for the log, preserving state while reducing storage.</p><h2 id="15-wal-and-storage-class-memory">15. WAL and Storage Class Memory</h2><p>Emerging storage class memory (SCM) like Intel Optane DC Persistent Memory blurs the line between memory and storage.</p><h3 id="151-dax-and-pmem">15.1 DAX and PMEM</h3><p>Direct Access (DAX) mode allows applications to mmap() persistent memory and access it directly:</p><div class="highlight"><pre tabindex=0 style=color:#e6edf3;background-color:#0d1117;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-text-size-adjust:none><code class="language-c" data-lang=c><span style=display:flex><span><span style=color:#ff7b72>void</span><span style=color:#ff7b72;font-weight:700>*</span> pmem <span style=color:#ff7b72;font-weight:700>=</span> <span style=color:#d2a8ff;font-weight:700>mmap</span>(NULL, size, PROT_READ <span style=color:#ff7b72;font-weight:700>|</span> PROT_WRITE,
</span></span><span style=display:flex><span>                  MAP_SHARED <span style=color:#ff7b72;font-weight:700>|</span> MAP_SYNC, fd, <span style=color:#a5d6ff>0</span>);
</span></span><span style=display:flex><span><span style=color:#8b949e;font-style:italic>// Writes to pmem are directly persistent (after cache flush)
</span></span></span><span style=display:flex><span><span style=color:#d2a8ff;font-weight:700>_mm_clflush</span>(pmem <span style=color:#ff7b72;font-weight:700>+</span> offset);
</span></span><span style=display:flex><span><span style=color:#d2a8ff;font-weight:700>_mm_sfence</span>();
</span></span></code></pre></div><p>No syscalls, no page cache—just load and store instructions.</p><h3 id="152-redesigning-wal-for-pmem">15.2 Redesigning WAL for PMEM</h3><p>With PMEM, traditional WAL has overhead:</p><ul><li>Copying data to log buffers (unnecessary with direct access)</li><li>fsync() calls (replaced by cache flush instructions)</li><li>Sequential-only access (PMEM supports random access efficiently)</li></ul><p>New designs:</p><ul><li><strong>Log directly to PMEM:</strong> Allocate log buffer in persistent memory. Writes are immediately durable.</li><li><strong>In-place updates with undo logging:</strong> Log the old value, update in place, flush. Simpler than redo logging.</li><li><strong>Hybrid:</strong> Keep hot path in PMEM, spill to SSD for capacity.</li></ul><h3 id="153-challenges">15.3 Challenges</h3><p>PMEM introduces new challenges:</p><ul><li><strong>Cache line granularity:</strong> CPU cache flushes are 64 bytes. Writes smaller than a cache line may require read-modify-write.</li><li><strong>Ordering:</strong> Memory fences and flush instructions have specific semantics. Getting ordering wrong corrupts data.</li><li><strong>Wear leveling:</strong> PMEM has limited write endurance. Avoid hot spots in the log.</li></ul><p>Libraries like PMDK (Persistent Memory Development Kit) provide safe abstractions for PMEM programming.</p><h2 id="16-debugging-and-observability">16. Debugging and Observability</h2><h3 id="161-log-analysis-tools">16.1 Log Analysis Tools</h3><p>Most databases provide tools to inspect WAL contents:</p><ul><li><strong>PostgreSQL:</strong> pg_waldump decodes WAL records</li><li><strong>MySQL:</strong> mysqlbinlog decodes the binary log</li><li><strong>SQLite:</strong> sqlite3 .dump with WAL mode shows effective state</li></ul><p>These tools help diagnose:</p><ul><li>What operations are generating the most WAL</li><li>Whether WAL growth is expected</li><li>What happened before a crash</li></ul><h3 id="162-metrics-to-monitor">16.2 Metrics to Monitor</h3><p>Key WAL metrics:</p><ul><li><strong>WAL write rate:</strong> Bytes per second written to WAL. High rates may indicate heavy write workloads or inefficient operations.</li><li><strong>WAL fsync latency:</strong> Time per fsync() call. Spikes indicate storage issues.</li><li><strong>WAL buffer wait time:</strong> How long transactions wait for buffer space. High waits suggest increasing wal_buffers.</li><li><strong>Checkpoint frequency and duration:</strong> Frequent or long checkpoints indicate I/O or configuration issues.</li><li><strong>WAL size / segment count:</strong> Growth indicates archiving or replication lag.</li></ul><h3 id="163-tracing">16.3 Tracing</h3><p>Tracing frameworks (DTrace, BPF, perf) can instrument WAL operations:</p><div class="highlight"><pre tabindex=0 style=color:#e6edf3;background-color:#0d1117;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-text-size-adjust:none><code class="language-bash" data-lang=bash><span style=display:flex><span><span style=color:#8b949e;font-style:italic># Trace PostgreSQL WAL writes</span>
</span></span><span style=display:flex><span>bpftrace -e <span style=color:#a5d6ff>&#39;tracepoint:syscalls:sys_enter_write
</span></span></span><span style=display:flex><span><span style=color:#a5d6ff>  /comm == &#34;postgres&#34; &amp;&amp; args-&gt;fd == $WAL_FD/
</span></span></span><span style=display:flex><span><span style=color:#a5d6ff>  { @bytes = hist(args-&gt;count); }&#39;</span>
</span></span></code></pre></div><p>This reveals I/O patterns: are writes batched efficiently? Is there unexpected synchronization?</p><h2 id="17-common-pitfalls">17. Common Pitfalls</h2><h3 id="171-ignoring-disk-flush-semantics">17.1 Ignoring Disk Flush Semantics</h3><p>Assuming write() implies durability is a classic mistake. Always:</p><ul><li>Call fsync() or fdatasync() for durability</li><li>Test with actual power loss (not just kill -9)</li><li>Verify storage controller write cache settings</li></ul><h3 id="172-log-contention">17.2 Log Contention</h3><p>A single log can become a bottleneck:</p><ul><li>All transactions serialize on log buffer allocation</li><li>A single fsync() blocks all waiters</li></ul><p>Mitigations:</p><ul><li>Group commit reduces fsync() frequency</li><li>Multiple log files (if supported)</li><li>Asynchronous commit for suitable workloads</li></ul><h3 id="173-unbounded-log-growth">17.3 Unbounded Log Growth</h3><p>Forgetting to checkpoint or archive leads to disk exhaustion. Configure:</p><ul><li>Regular checkpoints (checkpoint_timeout, checkpoint_completion_target in PostgreSQL)</li><li>WAL archiving or streaming replication</li><li>Monitoring and alerts on WAL size</li></ul><h3 id="174-recovery-testing-neglect">17.4 Recovery Testing Neglect</h3><p>Many systems never test recovery until production disaster strikes. Regularly:</p><ul><li>Practice recovery procedures</li><li>Verify backups are restorable</li><li>Test point-in-time recovery to specific LSNs</li></ul><h2 id="18-future-directions">18. Future Directions</h2><h3 id="181-hardware-acceleration">18.1 Hardware Acceleration</h3><p>Emerging hardware offers log acceleration:</p><ul><li><strong>Computational storage:</strong> Push log compression/checksum to the drive</li><li><strong>SmartNICs:</strong> Offload log replication to network hardware</li><li><strong>Custom ASICs:</strong> Specialized log processing units</li></ul><h3 id="182-tiered-storage">18.2 Tiered Storage</h3><p>Logs don&rsquo;t need to live on one tier:</p><ul><li>Hot (newest) log records on fast storage (NVM, SSD)</li><li>Warm (recent) records on standard SSD</li><li>Cold (archive) records on HDD or object storage</li></ul><p>Automated tiering based on age or access patterns.</p><h3 id="183-log-as-the-database">18.3 Log as the Database</h3><p>The logical conclusion of log-centric design: the log is the database, and everything else is a materialized view. Kafka&rsquo;s evolution toward this model, systems like Materialize, and event sourcing architectures embrace this philosophy.</p><p>Benefits:</p><ul><li>Simpler consistency model</li><li>Natural audit trail</li><li>Easy to derive multiple views</li></ul><p>Challenges:</p><ul><li>Log growth management</li><li>Query efficiency on append-only structures</li><li>Integration with existing ecosystems</li></ul><h2 id="19-implementation-checklist">19. Implementation Checklist</h2><p>When implementing or configuring WAL:</p><ol><li><p><strong>Understand durability requirements:</strong> What data loss is acceptable? Zero? A few seconds? This determines flush policy.</p></li><li><p><strong>Size buffers appropriately:</strong> Too small = excessive flushing. Too large = wasted memory and longer recovery.</p></li><li><p><strong>Configure checkpoints:</strong> Balance recovery time (frequent checkpoints = fast recovery) against I/O overhead.</p></li><li><p><strong>Test crash recovery:</strong> Regularly crash the system and verify recovery correctness.</p></li><li><p><strong>Monitor WAL metrics:</strong> Track write rates, fsync latency, and log size. Alert on anomalies.</p></li><li><p><strong>Plan for growth:</strong> Archive or replicate WAL before truncation. Test restore procedures.</p></li><li><p><strong>Understand your storage:</strong> Know your disk&rsquo;s fsync behavior, write cache policy, and failure modes.</p></li><li><p><strong>Consider replication:</strong> WAL shipping provides durability beyond a single node.</p></li></ol><h2 id="20-the-mathematics-of-wal-performance">20. The Mathematics of WAL Performance</h2><h3 id="201-throughput-modeling">20.1 Throughput Modeling</h3><p>Let&rsquo;s model WAL throughput under group commit. Assume:</p><ul><li>t_write: Time to write a batch of log records</li><li>t_fsync: Time to fsync (typically dominates)</li><li>n: Average transactions per batch</li></ul><p>Throughput = n / (t_write + t_fsync)</p><p>With t_fsync ≈ 100 microseconds (SSD) and efficient batching (n = 100):
Throughput ≈ 100 / 0.0001 = 1,000,000 transactions per second</p><p>This theoretical limit explains why group commit is so effective—amortizing the fixed fsync cost over many transactions.</p><h3 id="202-recovery-time-estimation">20.2 Recovery Time Estimation</h3><p>Recovery time depends on:</p><ul><li>L: Log length since last checkpoint (in bytes)</li><li>R: Redo processing rate (bytes per second)</li><li>U: Number of uncommitted transactions (for undo)</li><li>T_undo: Average undo time per transaction</li></ul><p>Total recovery time ≈ L/R + U × T_undo</p><p>For a system with 1GB of log since checkpoint, 100MB/s redo rate, and 1000 uncommitted transactions at 1ms each:
Recovery ≈ 10 seconds + 1 second = 11 seconds</p><p>This guides checkpoint frequency decisions: more frequent checkpoints mean smaller L and faster recovery.</p><h3 id="203-wal-space-amplification">20.3 WAL Space Amplification</h3><p>WAL introduces space overhead:</p><ul><li>Each write appears twice: once in the log, once in the data file</li><li>Full page writes (after checkpoints) add more overhead</li><li>Log retention for replication or PITR extends this</li></ul><p>Space amplification = (Log size + Data size) / Effective data size</p><p>With aggressive log retention, amplification can reach 3-5×. This trade-off between durability and space is fundamental.</p><h2 id="21-summary">21. Summary</h2><p>Write-ahead logging is the foundation of database durability. By writing a sequential log before modifying data pages, databases achieve both crash safety and high performance. The key principles:</p><ul><li><strong>Log before data:</strong> Never modify a page without first logging the change</li><li><strong>Sequential writes win:</strong> Logs are append-only, turning random writes into sequential ones</li><li><strong>Group commit amortizes fsync():</strong> Batch commits together to reduce I/O overhead</li><li><strong>Checkpoints bound recovery:</strong> Periodic checkpoints limit how much log must be replayed</li><li><strong>ARIES provides a complete framework:</strong> Analysis, redo, undo phases handle all recovery scenarios</li></ul><p>Modern systems push these foundations further with replicated logs, consensus protocols, and emerging persistent memory. But the core insight remains: by constraining how we write, we gain the freedom to recover from anything.</p><p>Whether you&rsquo;re building a new database, configuring an existing one, or designing a distributed system, understanding WAL is essential. It&rsquo;s the unsung hero working quietly behind every committed transaction, ensuring your data survives whatever the world throws at it.</p></div><footer class="ce1a612 c6dfb1e c3ecea6"><div class="c364589">Categories:
<a href=/categories/systems/>systems</a>, <a href=/categories/databases/>databases</a></div><div>Tags:
<a href=/tags/databases/>#databases</a>, <a href=/tags/durability/>#durability</a>, <a href=/tags/wal/>#wal</a>, <a href=/tags/logging/>#logging</a>, <a href=/tags/recovery/>#recovery</a>, <a href=/tags/storage/>#storage</a>, <a href=/tags/transactions/>#transactions</a></div></footer></article></main><footer class="ccdf0e8" role=contentinfo aria-label=Footer><div class="cfdda01 c133889 c5df473 c0eecc8 c69618a c6942b3 c03620d c2a9f27 c7c11d8 c82c52d c14527b"><div class="c6dfb1e c3ecea6 c39ef11 c88ae6f">&copy; 2026 Leonardo Benicio. All rights
reserved.</div><div class="c6942b3 c7c11d8 cd1fd22"><a href=https://github.com/lbenicio target=_blank rel="noopener noreferrer" aria-label=GitHub class="c1d6c20 c7c11d8 c1d0018 cd1fd22 cb5c327 c10dda9 c6dfb1e cbbda39 cfc01c7 c01f421 c286dd7 c2bd687 cfdce1d cfef18f c000b66 cf55a7b c514027"><svg width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M15 22v-4a4.8 4.8.0 00-1-3.5c3 0 6-2 6-5.5.08-1.25-.27-2.48-1-3.5.28-1.15.28-2.35.0-3.5.0.0-1 0-3 1.5-2.64-.5-5.36-.5-8 0C6 2 5 2 5 2c-.3 1.15-.3 2.35.0 3.5A5.403 5.403.0 004 9c0 3.5 3 5.5 6 5.5-.39.5-.67 1.08-.82 1.7s-.2 1.27-.18 1.9V22"/></svg>
<span class="cba5854">GitHub</span>
</a><a href=https://www.linkedin.com/in/leonardo-benicio target=_blank rel="noopener noreferrer" aria-label=LinkedIn class="c1d6c20 c7c11d8 c1d0018 cd1fd22 cb5c327 c10dda9 c6dfb1e cbbda39 cfc01c7 c01f421 c286dd7 c2bd687 cfdce1d cfef18f c000b66 cf55a7b c514027"><svg width="20" height="20" viewBox="0 0 24 24" fill="currentColor"><path d="M20.447 20.452H17.21V14.86c0-1.333-.027-3.046-1.858-3.046-1.86.0-2.145 1.45-2.145 2.948v5.69H9.069V9h3.112v1.561h.044c.434-.82 1.494-1.686 3.074-1.686 3.29.0 3.897 2.165 3.897 4.983v6.594zM5.337 7.433a1.805 1.805.0 11-.002-3.61 1.805 1.805.0 01.002 3.61zM6.763 20.452H3.911V9h2.852v11.452z"/></svg>
<span class="cba5854">LinkedIn</span>
</a><a href=https://twitter.com/lbenicio_ target=_blank rel="noopener noreferrer" aria-label=Twitter class="c1d6c20 c7c11d8 c1d0018 cd1fd22 cb5c327 c10dda9 c6dfb1e cbbda39 cfc01c7 c01f421 c286dd7 c2bd687 cfdce1d cfef18f c000b66 cf55a7b c514027"><svg width="20" height="20" viewBox="0 0 24 24" fill="currentColor"><path d="M19.633 7.997c.013.177.013.354.013.53.0 5.386-4.099 11.599-11.6 11.599-2.31.0-4.457-.676-6.265-1.842.324.038.636.05.972.05 1.91.0 3.67-.65 5.07-1.755a4.099 4.099.0 01-3.827-2.84c.25.039.5.064.763.064.363.0.726-.051 1.065-.139A4.091 4.091.0 012.542 9.649v-.051c.538.3 1.162.482 1.824.507A4.082 4.082.0 012.54 6.7c0-.751.2-1.435.551-2.034a11.63 11.63.0 008.44 4.281 4.615 4.615.0 01-.101-.938 4.091 4.091.0 017.078-2.799 8.1 8.1.0 002.595-.988 4.112 4.112.0 01-1.8 2.261 8.2 8.2.0 002.357-.638A8.824 8.824.0 0119.613 7.96z"/></svg>
<span class="cba5854">Twitter</span></a></div></div></footer></body></html>